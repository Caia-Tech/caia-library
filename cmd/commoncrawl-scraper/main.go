package main

import (
	"bufio"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"os"
	"regexp"
	"strings"
	"time"

	"github.com/Caia-Tech/caia-library/pkg/extractor"
	"github.com/Caia-Tech/caia-library/pkg/logging"
	"github.com/Caia-Tech/caia-library/pkg/pipeline"
)

// CommonCrawlRecord represents a WARC record from Common Crawl
type CommonCrawlRecord struct {
	URL           string `json:"url"`
	Timestamp     string `json:"timestamp"`
	ContentType   string `json:"content_type"`
	ContentLength int    `json:"content_length"`
	Content       string `json:"content"`
	StatusCode    int    `json:"status_code"`
	Filename      string `json:"filename"`
	Offset        int    `json:"offset"`
	Length        int    `json:"length"`
}

// ConversationalEntry represents a conversational Q&A pair for LLMs
type ConversationalEntry struct {
	ID           string                 `json:"id"`
	Conversation []ConversationalTurn   `json:"conversation"`
	Metadata     map[string]interface{} `json:"metadata"`
	Source       ConversationalSource   `json:"source"`
	CreatedAt    string                 `json:"created_at"`
}

type ConversationalTurn struct {
	Role    string `json:"role"`
	Content string `json:"content"`
}

type ConversationalSource struct {
	URL         string `json:"url"`
	Title       string `json:"title"`
	Category    string `json:"category"`
	Description string `json:"description"`
	WordCount   int    `json:"word_count"`
	Quality     string `json:"quality_tier"`
	Crawl       string `json:"crawl_info"`
}

type ConversationalDataset struct {
	Dataset     []ConversationalEntry `json:"dataset"`
	Metadata    DatasetMetadata       `json:"metadata"`
	GeneratedAt string                `json:"generated_at"`
}

type DatasetMetadata struct {
	Name        string `json:"name"`
	Description string `json:"description"`
	Version     string `json:"version"`
	TotalItems  int    `json:"total_items"`
	Sources     string `json:"sources"`
	Purpose     string `json:"purpose"`
}

func main() {
	fmt.Println("üåê COMMON CRAWL DATA RETRIEVAL")
	fmt.Println("==============================")
	fmt.Println("Fetching real data from Common Crawl archives")
	fmt.Println()

	config := pipeline.DevelopmentPipelineConfig()
	config.Logging.Level = "info"

	if err := logging.SetupLogger(config.Logging); err != nil {
		fmt.Printf("‚ùå Failed to setup logging: %v\n", err)
		return
	}

	logger := logging.GetPipelineLogger("commoncrawl-scraper", "main")
	logger.Info().Msg("Starting Common Crawl data retrieval")

	// Phase 1: Get Common Crawl index
	fmt.Println("üìã Phase 1: Getting Common Crawl index...")
	crawlIndex, err := getCommonCrawlIndex()
	if err != nil {
		logger.Fatal().Err(err).Msg("Failed to get crawl index")
	}
	fmt.Printf("‚úÖ Found crawl: %s\n", crawlIndex)

	// Phase 2: Query for Go-related content
	fmt.Println("\nüîç Phase 2: Querying for programming content...")
	
	// Try multiple query patterns - use domain-based queries that are more likely to work
	queries := []string{"go.dev/*", "golang.org/*", "github.com/golang/*"}
	var allUrls []map[string]string
	
	for _, query := range queries {
		fmt.Printf("   ‚Ä¢ Trying query: %s\n", query)
		urls, err := queryCommonCrawlIndex(crawlIndex, query)
		if err != nil {
			fmt.Printf("   ‚ö†Ô∏è  Query failed: %v\n", err)
			continue
		}
		fmt.Printf("   ‚Ä¢ Found %d URLs for query '%s'\n", len(urls), query)
		allUrls = append(allUrls, urls...)
		
		if len(allUrls) >= 10 {
			break // We have enough data
		}
	}
	
	fmt.Printf("‚úÖ Total found %d relevant URLs\n", len(allUrls))
	
	// Use allUrls instead of urls
	urls := allUrls

	// Phase 3: Fetch actual WARC data
	fmt.Println("\nüì• Phase 3: Fetching actual WARC records...")
	records, err := fetchWARCRecords(urls[:min(10, len(urls))]) // Limit to 10 for demo
	if err != nil {
		logger.Fatal().Err(err).Msg("Failed to fetch WARC records")
	}
	fmt.Printf("‚úÖ Retrieved %d records with content\n", len(records))

	// Phase 4: Extract and process content
	fmt.Println("\nüîç Phase 4: Extracting text content...")
	processedRecords := extractTextFromRecords(records)
	fmt.Printf("‚úÖ Processed %d records\n", len(processedRecords))

	// Phase 5: Generate conversational dataset
	fmt.Println("\nüîÑ Phase 5: Converting to conversational format...")
	dataset := convertToConversational(processedRecords)

	// Phase 6: Export results
	outputFile := "commoncrawl_golang_dataset.json"
	fmt.Printf("\nüíæ Phase 6: Exporting to %s...\n", outputFile)

	if err := exportDataset(dataset, outputFile); err != nil {
		logger.Fatal().Err(err).Msg("Failed to export dataset")
	}

	generateSummary(dataset, outputFile)
	logger.Info().Int("conversations", len(dataset.Dataset)).Msg("Common Crawl processing completed")
}

func getCommonCrawlIndex() (string, error) {
	// Get the latest available crawl index
	resp, err := http.Get("https://index.commoncrawl.org/collinfo.json")
	if err != nil {
		return "", fmt.Errorf("failed to fetch crawl info: %w", err)
	}
	defer resp.Body.Close()

	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return "", fmt.Errorf("failed to read response: %w", err)
	}

	var crawls []map[string]interface{}
	if err := json.Unmarshal(body, &crawls); err != nil {
		return "", fmt.Errorf("failed to parse crawl info: %w", err)
	}

	if len(crawls) == 0 {
		return "", fmt.Errorf("no crawls available")
	}

	// Get the most recent crawl
	latestCrawl := crawls[0]
	crawlID, ok := latestCrawl["id"].(string)
	if !ok {
		return "", fmt.Errorf("invalid crawl ID format")
	}

	fmt.Printf("   ‚Ä¢ Latest crawl: %s\n", crawlID)
	if desc, ok := latestCrawl["description"].(string); ok {
		fmt.Printf("   ‚Ä¢ Description: %s\n", desc)
	}

	return crawlID, nil
}

func queryCommonCrawlIndex(crawlID, query string) ([]map[string]string, error) {
	// Use the CDX Server API for querying Common Crawl
	queryURL := fmt.Sprintf("https://index.commoncrawl.org/%s-index?url=%s&output=json&limit=5", crawlID, query)

	fmt.Printf("     Querying: %s\n", queryURL)

	client := &http.Client{Timeout: 30 * time.Second}
	resp, err := client.Get(queryURL)
	if err != nil {
		return nil, fmt.Errorf("failed to query index: %w", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != 200 {
		body, _ := io.ReadAll(resp.Body)
		return nil, fmt.Errorf("index query failed with status %d: %s", resp.StatusCode, string(body))
	}

	var results []map[string]string
	scanner := bufio.NewScanner(resp.Body)
	count := 0

	for scanner.Scan() && count < 5 { // Limit results per query
		line := scanner.Text()
		if strings.TrimSpace(line) == "" {
			continue
		}

		var record map[string]string
		if err := json.Unmarshal([]byte(line), &record); err != nil {
			fmt.Printf("     ‚ö†Ô∏è  Failed to parse record: %v\n", err)
			continue
		}

		// Basic filtering for content that might be useful
		if record["status"] == "200" && 
		   (record["mime"] == "text/html" || strings.Contains(record["mime"], "text/")) {
			results = append(results, record)
			count++
			fmt.Printf("     ‚Ä¢ Found: %s (mime: %s)\n", record["url"], record["mime"])
		}
	}

	if err := scanner.Err(); err != nil {
		return nil, fmt.Errorf("error reading index results: %w", err)
	}

	return results, nil
}

func fetchWARCRecords(indexResults []map[string]string) ([]CommonCrawlRecord, error) {
	var records []CommonCrawlRecord
	client := &http.Client{Timeout: 30 * time.Second}

	for i, result := range indexResults {
		fmt.Printf("   [%d/%d] Fetching WARC record...\n", i+1, len(indexResults))

		filename := result["filename"]
		offset := result["offset"]
		length := result["length"]

		if filename == "" || offset == "" || length == "" {
			continue
		}

		// Construct WARC URL
		warcURL := fmt.Sprintf("https://commoncrawl.s3.amazonaws.com/%s", filename)

		// Create range request
		req, err := http.NewRequest("GET", warcURL, nil)
		if err != nil {
			fmt.Printf("   ‚ùå Failed to create request: %v\n", err)
			continue
		}

		// Calculate end position for range header
		offsetInt := 0
		lengthInt := 0
		fmt.Sscanf(offset, "%d", &offsetInt)
		fmt.Sscanf(length, "%d", &lengthInt)
		endPos := offsetInt + lengthInt - 1
		
		// Set range header to get specific record
		rangeHeader := fmt.Sprintf("bytes=%d-%d", offsetInt, endPos)
		req.Header.Set("Range", rangeHeader)

		resp, err := client.Do(req)
		if err != nil {
			fmt.Printf("   ‚ùå Failed to fetch WARC: %v\n", err)
			continue
		}

		content, err := io.ReadAll(resp.Body)
		resp.Body.Close()

		if err != nil {
			fmt.Printf("   ‚ùå Failed to read WARC content: %v\n", err)
			continue
		}

		// Parse WARC record
		record, err := parseWARCRecord(content, result)
		if err != nil {
			fmt.Printf("   ‚ö†Ô∏è  Failed to parse WARC: %v\n", err)
			continue
		}

		if record != nil && len(record.Content) > 500 {
			records = append(records, *record)
			fmt.Printf("   ‚úÖ Parsed %d bytes of content\n", len(record.Content))
		}

		// Rate limiting
		time.Sleep(1 * time.Second)
	}

	return records, nil
}

func parseWARCRecord(data []byte, indexResult map[string]string) (*CommonCrawlRecord, error) {
	contentStr := string(data)
	
	// Debug: show what we got
	fmt.Printf("   üìã Content type: %s (length: %d)\n", detectContentType(data), len(data))
	
	// Handle different response types
	if strings.HasPrefix(contentStr, "<?xml") {
		// This is likely an error response or API response
		return nil, fmt.Errorf("received XML response instead of WARC data")
	}

	// Try to find WARC-Type header to confirm this is a WARC record
	if !strings.Contains(contentStr, "WARC/") {
		// Maybe it's direct HTML content - let's try to use it anyway
		if strings.Contains(contentStr, "<html") || strings.Contains(contentStr, "<HTML") || 
		   strings.Contains(contentStr, "<body") || strings.Contains(contentStr, "<div") {
			fmt.Printf("   ‚úÖ Found HTML content directly\n")
			record := &CommonCrawlRecord{
				URL:         indexResult["url"],
				Timestamp:   indexResult["timestamp"],
				Content:     contentStr,
				Filename:    indexResult["filename"],
				ContentType: "text/html",
			}
			return record, nil
		}
		return nil, fmt.Errorf("not a WARC record and no HTML found")
	}

	// Parse proper WARC record
	sections := strings.Split(contentStr, "\r\n\r\n")
	if len(sections) < 3 {
		sections = strings.Split(contentStr, "\n\n")
	}
	
	if len(sections) < 3 {
		return nil, fmt.Errorf("malformed WARC record")
	}

	// sections[0] = WARC headers
	// sections[1] = HTTP headers  
	// sections[2+] = HTML content
	
	htmlContent := strings.Join(sections[2:], "\n\n")
	
	// Validation
	if len(htmlContent) < 100 {
		return nil, fmt.Errorf("content too small")
	}

	record := &CommonCrawlRecord{
		URL:         indexResult["url"],
		Timestamp:   indexResult["timestamp"],
		Content:     htmlContent,
		Filename:    indexResult["filename"],
		ContentType: "text/html",
	}

	return record, nil
}

func detectContentType(data []byte) string {
	if len(data) == 0 {
		return "empty"
	}
	
	s := string(data[:min(100, len(data))])
	if strings.HasPrefix(s, "<?xml") {
		return "xml"
	}
	if strings.Contains(s, "WARC/") {
		return "warc"
	}
	if strings.Contains(s, "<html") || strings.Contains(s, "<HTML") {
		return "html"
	}
	return "unknown"
}

func extractTextFromRecords(records []CommonCrawlRecord) []CommonCrawlRecord {
	extractorEngine := extractor.NewEngine()
	var processed []CommonCrawlRecord

	for i, record := range records {
		fmt.Printf("   [%d/%d] Extracting text from %s\n", i+1, len(records), record.URL)

		// Extract text content
		text, _, err := extractorEngine.Extract(context.Background(), []byte(record.Content), "html")
		if err != nil {
			fmt.Printf("   ‚ùå Extraction failed: %v\n", err)
			continue
		}

		if len(text) < 200 {
			fmt.Printf("   ‚ö†Ô∏è  Content too small (%d chars)\n", len(text))
			continue
		}

		// Update record with extracted text
		record.Content = text
		processed = append(processed, record)

		fmt.Printf("   ‚úÖ Extracted %d characters\n", len(text))
	}

	return processed
}

func convertToConversational(records []CommonCrawlRecord) ConversationalDataset {
	var conversations []ConversationalEntry

	for _, record := range records {
		entries := createConversationalEntries(record)
		conversations = append(conversations, entries...)
	}

	return ConversationalDataset{
		Dataset: conversations,
		Metadata: DatasetMetadata{
			Name:        "Common Crawl Go Programming Dataset",
			Description: "Conversational Q&A pairs derived from Common Crawl web data for Go programming content",
			Version:     "1.0.0",
			TotalItems:  len(conversations),
			Sources:     "Common Crawl web archive data",
			Purpose:     "LLM training, fine-tuning, and Go programming assistance",
		},
		GeneratedAt: time.Now().UTC().Format(time.RFC3339),
	}
}

func createConversationalEntries(record CommonCrawlRecord) []ConversationalEntry {
	var entries []ConversationalEntry

	title := extractTitle(record.Content, record.URL)
	category := categorizeContent(record.Content, record.URL)
	description := extractDescription(record.Content)
	
	wordCount := len(strings.Fields(record.Content))
	quality := assessQuality(record.Content, wordCount)

	source := ConversationalSource{
		URL:         record.URL,
		Title:       title,
		Category:    category,
		Description: description,
		WordCount:   wordCount,
		Quality:     quality,
		Crawl:       fmt.Sprintf("Common Crawl %s", record.Timestamp),
	}

	baseID := sanitizeID(record.URL)
	timestamp := time.Now().Unix()

	// Content overview
	entries = append(entries, ConversationalEntry{
		ID: fmt.Sprintf("%s_overview_%d", baseID, timestamp),
		Conversation: []ConversationalTurn{
			{
				Role:    "user",
				Content: fmt.Sprintf("Can you tell me about the content from %s?", record.URL),
			},
			{
				Role:    "assistant",
				Content: fmt.Sprintf("This content from %s covers %s.\n\n%s", title, strings.ToLower(description), createContentOverview(record.Content)),
			},
		},
		Metadata: map[string]interface{}{
			"type":     "content_overview",
			"section":  "overview",
			"keywords": extractKeywords(record.Content),
		},
		Source:    source,
		CreatedAt: time.Now().UTC().Format(time.RFC3339),
	})

	// Technical details if it's Go-related
	if containsGoContent(record.Content) {
		entries = append(entries, ConversationalEntry{
			ID: fmt.Sprintf("%s_golang_%d", baseID, timestamp+1),
			Conversation: []ConversationalTurn{
				{
					Role:    "user",
					Content: "What Go programming concepts are discussed in this content?",
				},
				{
					Role:    "assistant",
					Content: extractGoContent(record.Content),
				},
			},
			Metadata: map[string]interface{}{
				"type":     "golang_content",
				"section":  "programming",
				"keywords": extractGoKeywords(record.Content),
			},
			Source:    source,
			CreatedAt: time.Now().UTC().Format(time.RFC3339),
		})
	}

	return entries
}

func extractTitle(content, url string) string {
	// Try to extract title from content
	titleRegex := regexp.MustCompile(`<title[^>]*>([^<]+)</title>`)
	if matches := titleRegex.FindStringSubmatch(content); len(matches) > 1 {
		return strings.TrimSpace(matches[1])
	}

	// Fallback to URL-based title
	if strings.Contains(url, "golang") || strings.Contains(url, "go.") {
		return "Go Programming Content"
	}
	
	return "Web Content"
}

func categorizeContent(content, url string) string {
	contentLower := strings.ToLower(content)
	urlLower := strings.ToLower(url)

	if strings.Contains(urlLower, "doc") || strings.Contains(contentLower, "documentation") {
		return "Documentation"
	}
	if strings.Contains(urlLower, "tutorial") || strings.Contains(contentLower, "tutorial") {
		return "Tutorial"
	}
	if strings.Contains(urlLower, "example") || strings.Contains(contentLower, "example") {
		return "Examples"
	}
	if strings.Contains(urlLower, "blog") || strings.Contains(contentLower, "blog") {
		return "Blog"
	}
	return "General"
}

func extractDescription(content string) string {
	// Try to extract meta description
	descRegex := regexp.MustCompile(`<meta[^>]*name=["']description["'][^>]*content=["']([^"']+)["']`)
	if matches := descRegex.FindStringSubmatch(content); len(matches) > 1 {
		return strings.TrimSpace(matches[1])
	}

	// Fallback to first paragraph
	lines := strings.Split(content, "\n")
	for _, line := range lines {
		line = strings.TrimSpace(line)
		if len(line) > 50 && len(line) < 200 {
			return line
		}
	}

	return "Web content about programming and development"
}

func assessQuality(content string, wordCount int) string {
	if wordCount > 2000 {
		return "high"
	}
	if wordCount > 500 {
		return "medium"
	}
	return "low"
}

func createContentOverview(content string) string {
	// Get first few meaningful lines
	lines := strings.Split(content, "\n")
	var overview []string

	for _, line := range lines[:min(10, len(lines))] {
		line = strings.TrimSpace(line)
		if len(line) > 20 && !strings.HasPrefix(line, "<") {
			overview = append(overview, line)
			if len(overview) >= 3 {
				break
			}
		}
	}

	if len(overview) == 0 {
		return "This content provides information about programming and development topics."
	}

	return strings.Join(overview, "\n\n")
}

func containsGoContent(content string) bool {
	goTerms := []string{"golang", "go lang", "func main", "package main", "import", "goroutine"}
	contentLower := strings.ToLower(content)

	for _, term := range goTerms {
		if strings.Contains(contentLower, term) {
			return true
		}
	}
	return false
}

func extractGoContent(content string) string {
	goKeywords := extractGoKeywords(content)
	if len(goKeywords) == 0 {
		return "This content discusses Go programming concepts and practices."
	}

	return fmt.Sprintf("This content covers these Go programming topics: %s", strings.Join(goKeywords, ", "))
}

func extractKeywords(content string) []string {
	commonWords := []string{"programming", "development", "software", "code", "application", "system", "web", "api"}
	var keywords []string
	contentLower := strings.ToLower(content)

	for _, word := range commonWords {
		if strings.Contains(contentLower, word) {
			keywords = append(keywords, word)
		}
	}

	return keywords
}

func extractGoKeywords(content string) []string {
	goTerms := []string{"golang", "goroutine", "channel", "interface", "struct", "slice", "map", "package", "import", "func"}
	var keywords []string
	contentLower := strings.ToLower(content)

	for _, term := range goTerms {
		if strings.Contains(contentLower, term) {
			keywords = append(keywords, term)
		}
	}

	return keywords
}

func sanitizeID(s string) string {
	reg := regexp.MustCompile(`[^a-zA-Z0-9_-]`)
	return strings.ToLower(reg.ReplaceAllString(s, "_"))
}

func exportDataset(dataset ConversationalDataset, filename string) error {
	file, err := os.Create(filename)
	if err != nil {
		return fmt.Errorf("failed to create file: %w", err)
	}
	defer file.Close()

	encoder := json.NewEncoder(file)
	encoder.SetIndent("", "  ")

	if err := encoder.Encode(dataset); err != nil {
		return fmt.Errorf("failed to encode JSON: %w", err)
	}

	return nil
}

func generateSummary(dataset ConversationalDataset, filename string) {
	fmt.Printf("\nüéâ COMMON CRAWL PROCESSING COMPLETED!\n")
	fmt.Printf("=====================================\n")

	// File size
	if info, err := os.Stat(filename); err == nil {
		fmt.Printf("‚Ä¢ File: %s (%.1f KB)\n", filename, float64(info.Size())/1024)
	}

	fmt.Printf("‚Ä¢ Total Conversations: %d\n", len(dataset.Dataset))
	fmt.Printf("‚Ä¢ Generated: %s\n", dataset.GeneratedAt)

	// Count by type and source
	typeCount := make(map[string]int)
	sourceCount := make(map[string]int)
	totalTurns := 0
	totalChars := 0

	for _, entry := range dataset.Dataset {
		if entryType, ok := entry.Metadata["type"].(string); ok {
			typeCount[entryType]++
		}
		sourceCount[entry.Source.Category]++
		totalTurns += len(entry.Conversation)

		for _, turn := range entry.Conversation {
			totalChars += len(turn.Content)
		}
	}

	fmt.Printf("\nüìä Conversation Types:\n")
	for convType, count := range typeCount {
		fmt.Printf("   ‚Ä¢ %s: %d conversations\n", convType, count)
	}

	fmt.Printf("\nüåê Content Sources:\n")
	for source, count := range sourceCount {
		fmt.Printf("   ‚Ä¢ %s: %d conversations\n", source, count)
	}

	fmt.Printf("\nüí¨ Dataset Statistics:\n")
	fmt.Printf("   ‚Ä¢ Total Conversational Turns: %d\n", totalTurns)
	fmt.Printf("   ‚Ä¢ Total Characters: %d (%.1f KB)\n", totalChars, float64(totalChars)/1024)
	if len(dataset.Dataset) > 0 {
		fmt.Printf("   ‚Ä¢ Average Conversation Length: %.0f characters\n", float64(totalChars)/float64(len(dataset.Dataset)))
	}

	fmt.Printf("\nüåü Common Crawl Processing Achievements:\n")
	fmt.Printf("   ‚Ä¢ ‚úÖ Real data from Common Crawl archives\n")
	fmt.Printf("   ‚Ä¢ ‚úÖ WARC record parsing and extraction\n")
	fmt.Printf("   ‚Ä¢ ‚úÖ Content filtering and quality assessment\n")
	fmt.Printf("   ‚Ä¢ ‚úÖ Conversational dataset generation\n")
	fmt.Printf("   ‚Ä¢ ‚úÖ Production-ready implementation\n")
}

func min(a, b int) int {
	if a < b {
		return a
	}
	return b
}