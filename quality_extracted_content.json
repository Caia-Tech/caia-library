{
  "sources": [
    {
      "url": "https://go.dev/doc/effective_go",
      "title": "Effective Go",
      "content": "Documentation Effective Go\n\nEffective Go\n\nIntroduction\n\nGo is a new language. Although it borrows ideas from\n\nexisting languages,\n\nit has unusual properties that make effective Go programs\n\ndifferent in character from programs written in its relatives.\n\nA straightforward translation of a C++ or Java program into Go\n\nis unlikely to produce a satisfactory result—Java programs\n\nare written in Java, not Go.\n\nOn the other hand, thinking about the problem from a Go\n\nperspective could produce a successful but quite different\n\nprogram.\n\nIn other words,\n\nto write Go well, it's important to understand its properties\n\nand idioms.\n\nIt's also important to know the established conventions for\n\nprogramming in Go, such as naming, formatting, program\n\nconstruction, and so on, so that programs you write\n\nwill be easy for other Go programmers to understand.\n\nThis document gives tips for writing clear, idiomatic Go code.\n\nIt augments the\n\nlanguage specification\n\n,\n\nthe\n\nTour of Go\n\n,\n\nand\n\nHow to Write Go Code\n\n,\n\nall of which you\n\nshould read first.\n\nNote added January, 2022:\n\nThis document was written for Go's\n\nrelease in 2009, and has not been updated significantly since.\n\nAlthough it is a good guide to understand how to use the language\n\nitself, thanks to the stability of the language, it says little\n\nabout the libraries and nothing about significant changes to the\n\nGo ecosystem since it was written, such as the build system, testing,\n\nmodules, and polymorphism.\n\nThere are no plans to update it, as so much has happened and a large\n\nand growing set of documents, blogs, and books do a fine job of\n\ndescribing modern Go usage.\n\nEffective Go continues to be useful, but the reader should\n\nunderstand it is far from a complete guide.\n\nSee\n\nissue\n\n28782\n\nfor context.\n\nExamples\n\nThe\n\nGo package sources\n\nare intended to serve not\n\nonly as the core library but also as examples of how to\n\nuse the language.\n\nMoreover, many of the packages contain working, self-contained\n\nexecutable examples you can run directly from the\n\ngo.dev\n\nweb site, such as\n\nthis one\n\n(if\n\nnecessary, click on the word \"Example\" to open it up).\n\nIf you have a question about how to approach a problem or how something\n\nmight be implemented, the documentation, code and examples in the\n\nlibrary can provide answers, ideas and\n\nbackground.\n\nFormatting\n\nFormatting issues are the most contentious\n\nbut the least consequential.\n\nPeople can adapt to different formatting styles\n\nbut it's better if they don't have to, and\n\nless time is devoted to the topic\n\nif everyone adheres to the same style.\n\nThe problem is how to approach this Utopia without a long\n\nprescriptive style guide.\n\nWith Go we take an unusual\n\napproach and let the machine\n\ntake care of most formatting issues.\n\nThe\n\ngofmt\n\nprogram\n\n(also available as\n\ngo fmt\n\n, which\n\noperates at the package level rather than source file level)\n\nreads a Go program\n\nand emits the source in a standard style of indentation\n\nand vertical alignment, retaining and if necessary\n\nreformatting comments.\n\nIf you want to know how to handle some new layout\n\nsituation, run\n\ngofmt\n\n; if the answer doesn't\n\nseem right, rearrange your program (or file a bug about\n\ngofmt\n\n),\n\ndon't work around it.\n\nAs an example, there's no need to spend time lining up\n\nthe comments on the fields of a structure.\n\nGofmt\n\nwill do that for you. Given the\n\ndeclaration\n\ntype T struct {\n\nname string // name of the object\n\nvalue int // its value\n\n}\n\ngofmt\n\nwill line up the columns:\n\ntype T struct {\n\nname string // name of the object\n\nvalue int // its value\n\n}\n\nAll Go code in the standard packages has been formatted with\n\ngofmt\n\n.\n\nSome formatting details remain. Very briefly:\n\nIndentation\n\nWe use tabs for indentation and\n\ngofmt\n\nemits them by default.\n\nUse spaces only if you must.\n\nLine length\n\nGo has no line length limit. Don't worry about overflowing a punched card.\n\nIf a line feels too long, wrap it and indent with an extra tab.\n\nParentheses\n\nGo needs fewer parentheses than C and Java: control structures (\n\nif\n\n,\n\nfor\n\n,\n\nswitch\n\n) do not have parentheses in\n\ntheir syntax.\n\nAlso, the operator precedence hierarchy is shorter and clearer, so\n\nx\u003c\u003c8 + y\u003c\u003c16\n\nmeans what the spacing implies, unlike in the other languages.\n\nCommentary\n\nGo provides C-style\n\n/* */\n\nblock comments\n\nand C++-style\n\n//\n\nline comments.\n\nLine comments are the norm;\n\nblock comments appear mostly as package comments, but\n\nare useful within an expression or to disable large swaths of code.\n\nComments that appear before top-level declarations, with no intervening newlines,\n\nare considered to document the declaration itself.\n\nThese “doc comments” are the primary documentation for a given Go package or command.\n\nFor more about doc comments, see “\n\nGo Doc Comments\n\n”.\n\nNames\n\nNames are as important in Go as in any other language.\n\nThey even have semantic effect:\n\nthe visibility of a name outside a package is determined by whether its\n\nfirst character is upper case.\n\nIt's therefore worth spending a little time talking about naming conventions\n\nin Go programs.\n\nPackage names\n\nWhen a package is imported, the package name becomes an accessor for the\n\ncontents. After\n\nimport \"bytes\"\n\nthe importing package can talk about\n\nbytes.Buffer\n\n. It's\n\nhelpful if everyone using the package can use the same name to refer to\n\nits contents, which implies that the package name should be good:\n\nshort, concise, evocative. By convention, packages are given\n\nlower case, single-word names; there should be no need for underscores\n\nor mixedCaps.\n\nErr on the side of brevity, since everyone using your\n\npackage will be typing that name.\n\nAnd don't worry about collisions\n\na priori\n\n.\n\nThe package name is only the default name for imports; it need not be unique\n\nacross all source code, and in the rare case of a collision the\n\nimporting package can choose a different name to use locally.\n\nIn any case, confusion is rare because the file name in the import\n\ndetermines just which package is being used.\n\nAnother convention is that the package name is the base name of\n\nits source directory;\n\nthe package in\n\nsrc/encoding/base64\n\nis imported as\n\n\"encoding/base64\"\n\nbut has name\n\nbase64\n\n,\n\nnot\n\nencoding_base64\n\nand not\n\nencodingBase64\n\n.\n\nThe importer of a package will use the name to refer to its contents,\n\nso exported names in the package can use that fact\n\nto avoid repetition.\n\n(Don't use the\n\nimport .\n\nnotation, which can simplify\n\ntests that must run outside the package they are testing, but should otherwise be avoided.)\n\nFor instance, the buffered reader type in the\n\nbufio\n\npackage is called\n\nReader\n\n,\n\nnot\n\nBufReader\n\n, because users see it as\n\nbufio.Reader\n\n,\n\nwhich is a clear, concise name.\n\nMoreover,\n\nbecause imported entities are always addressed with their package name,\n\nbufio.Reader\n\ndoes not conflict with\n\nio.Reader\n\n.\n\nSimilarly, the function to make new instances of\n\nring.Ring\n\n—which\n\nis the definition of a\n\nconstructor\n\nin Go—would\n\nnormally be called\n\nNewRing\n\n, but since\n\nRing\n\nis the only type exported by the package, and since the\n\npackage is called\n\nring\n\n, it's called just\n\nNew\n\n,\n\nwhich clients of the package see as\n\nring.New\n\n.\n\nUse the package structure to help you choose good names.\n\nAnother short example is\n\nonce.Do\n\n;\n\nonce.Do(setup)\n\nreads well and would not be improved by\n\nwriting\n\nonce.DoOrWaitUntilDone(setup)\n\n.\n\nLong names don't automatically make things more readable.\n\nA helpful doc comment can often be more valuable than an extra long name.\n\nGetters\n\nGo doesn't provide automatic support for getters and setters.\n\nThere's nothing wrong with providing getters and setters yourself,\n\nand it's often appropriate to do so, but it's neither idiomatic nor necessary\n\nto put\n\nGet\n\ninto the getter's name. If you have a field called\n\nowner\n\n(lower case, unexported), the getter method should be\n\ncalled\n\nOwner\n\n(upper case, exported), not\n\nGetOwner\n\n.\n\nThe use of upper-case names for export provides the hook to discriminate\n\nthe field from the method.\n\nA setter function, if needed, will likely be called\n\nSetOwner\n\n.\n\nBoth names read well in practice:\n\nowner := obj.Owner()\n\nif owner != user {\n\nobj.SetOwner(user)\n\n}\n\nInterface names\n\nBy convention, one-method interfaces are named by\n\nthe method name plus an -er suffix or similar modification\n\nto construct an agent noun:\n\nReader\n\n,\n\nWriter\n\n,\n\nFormatter\n\n,\n\nCloseNotifier\n\netc.\n\nThere are a number of such names and it's productive to honor them and the function\n\nnames they capture.\n\nRead\n\n,\n\nWrite\n\n,\n\nClose\n\n,\n\nFlush\n\n,\n\nString\n\nand so on have\n\ncanonical signatures and meanings. To avoid confusion,\n\ndon't give your method one of those names unless it\n\nhas the same signature and meaning.\n\nConversely, if your type implements a method with the\n\nsame meaning as a method on a well-known type,\n\ngive it the same name and signature;\n\ncall your string-converter method\n\nString\n\nnot\n\nToString\n\n.\n\nMixedCaps\n\nFinally, the convention in Go is to use\n\nMixedCaps\n\nor\n\nmixedCaps\n\nrather than underscores to write\n\nmultiword names.\n\nSemicolons\n\nLike C, Go's formal grammar uses semicolons to terminate statements,\n\nbut unlike in C, those semicolons do not appear in the source.\n\nInstead the lexer uses a simple rule to insert semicolons automatically\n\nas it scans, so the input text is mostly free of them.\n\nThe rule is this. If the last token before a newline is an identifier\n\n(which includes words like\n\nint\n\nand\n\nfloat64\n\n),\n\na basic literal such as a number or string constant, or one of the\n\ntokens\n\nbreak continue fallthrough return ++ -- ) }\n\nthe lexer always inserts a semicolon after the token.\n\nThis could be summarized as, “if the newline comes\n\nafter a token that could end a statement, insert a semicolon”.\n\nA semicolon can also be omitted immediately before a closing brace,\n\nso a statement such as\n\ngo func() { for { dst \u003c- \u003c-src } }()\n\nneeds no semicolons.\n\nIdiomatic Go programs have semicolons only in places such as\n\nfor\n\nloop clauses, to separate the initializer, condition, and\n\ncontinuation elements. They are also necessary to separate multiple\n\nstatements on a line, should you write code that way.\n\nOne consequence of the semicolon insertion rules\n\nis that you cannot put the opening brace of a\n\ncontrol structure (\n\nif\n\n,\n\nfor\n\n,\n\nswitch\n\n,\n\nor\n\nselect\n\n) on the next line. If you do, a semicolon\n\nwill be inserted before the brace, which could cause unwanted\n\neffects. Write them like this\n\nif i \u003c f() {\n\ng()\n\n}\n\nnot like this\n\nif i \u003c f() // wrong!\n\n{ // wrong!\n\ng()\n\n}\n\nControl structures\n\nThe control structures of Go are related to those of C but differ\n\nin important ways.\n\nThere is no\n\ndo\n\nor\n\nwhile\n\nloop, only a\n\nslightly generalized\n\nfor\n\n;\n\nswitch\n\nis more flexible;\n\nif\n\nand\n\nswitch\n\naccept an optional\n\ninitialization statement like that of\n\nfor\n\n;\n\nbreak\n\nand\n\ncontinue\n\nstatements\n\ntake an optional label to identify what to break or continue;\n\nand there are new control structures including a type switch and a\n\nmultiway communications multiplexer,\n\nselect\n\n.\n\nThe syntax is also slightly different:\n\nthere are no parentheses\n\nand the bodies must always be brace-delimited.\n\nIf\n\nIn Go a simple\n\nif\n\nlooks like this:\n\nif x \u003e 0 {\n\nreturn y\n\n}\n\nMandatory braces encourage writing simple\n\nif\n\nstatements\n\non multiple lines. It's good style to do so anyway,\n\nespecially when the body contains a control statement such as a\n\nreturn\n\nor\n\nbreak\n\n.\n\nSince\n\nif\n\nand\n\nswitch\n\naccept an initialization\n\nstatement, it's common to see one used to set up a local variable.\n\nif err := file.Chmod(0664); err != nil {\n\nlog.Print(err)\n\nreturn err\n\n}\n\nIn the Go libraries, you'll find that\n\nwhen an\n\nif\n\nstatement doesn't flow into the next statement—that is,\n\nthe body ends in\n\nbreak\n\n,\n\ncontinue\n\n,\n\ngoto\n\n, or\n\nreturn\n\n—the unnecessary\n\nelse\n\nis omitted.\n\nf, err := os.Open(name)\n\nif err != nil {\n\nreturn err\n\n}\n\ncodeUsing(f)\n\nThis is an example of a common situation where code must guard against a\n\nsequence of error conditions. The code reads well if the\n\nsuccessful flow of control runs down the page, eliminating error cases\n\nas they arise. Since error cases tend to end in\n\nreturn\n\nstatements, the resulting code needs no\n\nelse\n\nstatements.\n\nf, err := os.Open(name)\n\nif err != nil {\n\nreturn err\n\n}\n\nd, err := f.Stat()\n\nif err != nil {\n\nf.Close()\n\nreturn err\n\n}\n\ncodeUsing(f, d)\n\nRedeclaration and reassignment\n\nAn aside: The last example in the previous section demonstrates a detail of how the\n\n:=\n\nshort declaration form works.\n\nThe declaration that calls\n\nos.Open\n\nreads,\n\nf, err := os.Open(name)\n\nThis statement declares two variables,\n\nf\n\nand\n\nerr\n\n.\n\nA few lines later, the call to\n\nf.Stat\n\nreads,\n\nd, err := f.Stat()\n\nwhich looks as if it declares\n\nd\n\nand\n\nerr\n\n.\n\nNotice, though, that\n\nerr\n\nappears in both statements.\n\nThis duplication is legal:\n\nerr\n\nis declared by the first statement,\n\nbut only\n\nre-assigned\n\nin the second.\n\nThis means that the call to\n\nf.Stat\n\nuses the existing\n\nerr\n\nvariable declared above, and just gives it a new value.\n\nIn a\n\n:=\n\ndeclaration a variable\n\nv\n\nmay appear even\n\nif it has already been declared, provided:\n\nthis declaration is in the same scope as the existing declaration of\n\nv\n\n(if\n\nv\n\nis already declared in an outer scope, the declaration will create a new variable §),\n\nthe corresponding value in the initialization is assignable to\n\nv\n\n, and\n\nthere is at least one other variable that is created by the declaration.\n\nThis unusual property is pure pragmatism,\n\nmaking it easy to use a single\n\nerr\n\nvalue, for example,\n\nin a long\n\nif-else\n\nchain.\n\nYou'll see it used often.\n\n§ It's worth noting here that in Go the scope of function parameters and return values\n\nis the same as the function body, even though they appear lexically outside the braces\n\nthat enclose the body.\n\nFor\n\nThe Go\n\nfor\n\nloop is similar to—but not the same as—C's.\n\nIt unifies\n\nfor\n\nand\n\nwhile\n\nand there is no\n\ndo-while\n\n.\n\nThere are three forms, only one of which has semicolons.\n\n// Like a C for\n\nfor init; condition; post { }\n\n// Like a C while\n\nfor condition { }\n\n// Like a C for(;;)\n\nfor { }\n\nShort declarations make it easy to declare the index variable right in the loop.\n\nsum := 0\n\nfor i := 0; i \u003c 10; i++ {\n\nsum += i\n\n}\n\nIf you're looping over an array, slice, string, or map,\n\nor reading from a channel, a\n\nrange\n\nclause can\n\nmanage the loop.\n\nfor key, value := range oldMap {\n\nnewMap[key] = value\n\n}\n\nIf you only need the first item in the range (the key or index), drop the second:\n\nfor key := range m {\n\nif key.expired() {\n\ndelete(m, key)\n\n}\n\n}\n\nIf you only need the second item in the range (the value), use the\n\nblank identifier\n\n, an underscore, to discard the first:\n\nsum := 0\n\nfor _, value := range array {\n\nsum += value\n\n}\n\nThe blank identifier has many uses, as described in\n\na later section\n\n.\n\nFor strings, the\n\nrange\n\ndoes more work for you, breaking out individual\n\nUnicode code points by parsing the UTF-8.\n\nErroneous encodings consume one byte and produce the\n\nreplacement rune U+FFFD.\n\n(The name (with associated builtin type)\n\nrune\n\nis Go terminology for a\n\nsingle Unicode code point.\n\nSee\n\nthe language specification\n\nfor details.)\n\nThe loop\n\nfor pos, char := range \"日本\\x80語\" { // \\x80 is an illegal UTF-8 encoding\n\nfmt.Printf(\"character %#U starts at byte position %d\\n\", char, pos)\n\n}\n\nprints\n\ncharacter U+65E5 '日' starts at byte position 0\n\ncharacter U+672C '本' starts at byte position 3\n\ncharacter U+FFFD '�' starts at byte position 6\n\ncharacter U+8A9E '語' starts at byte position 7\n\nFinally, Go has no comma operator and\n\n++\n\nand\n\n--\n\nare statements not expressions.\n\nThus if you want to run multiple variables in a\n\nfor\n\nyou should use parallel assignment (although that precludes\n\n++\n\nand\n\n--\n\n).\n\n// Reverse a\n\nfor i, j := 0, len(a)-1; i \u003c j; i, j = i+1, j-1 {\n\na[i], a[j] = a[j], a[i]\n\n}\n\nSwitch\n\nGo's\n\nswitch\n\nis more general than C's.\n\nThe expressions need not be constants or even integers,\n\nthe cases are evaluated top to bottom until a match is found,\n\nand if the\n\nswitch\n\nhas no expression it switches on\n\ntrue\n\n.\n\nIt's therefore possible—and idiomatic—to write an\n\nif\n\n-\n\nelse\n\n-\n\nif\n\n-\n\nelse\n\nchain as a\n\nswitch\n\n.\n\nfunc unhex(c byte) byte {\n\nswitch {\n\ncase '0' \u003c= c \u0026\u0026 c \u003c= '9':\n\nreturn c - '0'\n\ncase 'a' \u003c= c \u0026\u0026 c \u003c= 'f':\n\nreturn c - 'a' + 10\n\ncase 'A' \u003c= c \u0026\u0026 c \u003c= 'F':\n\nreturn c - 'A' + 10\n\n}\n\nreturn 0\n\n}\n\nThere is no automatic fall through, but cases can be presented\n\nin comma-separated lists.\n\nfunc shouldEscape(c byte) bool {\n\nswitch c {\n\ncase ' ', '?', '\u0026', '=', '#', '+', '%':\n\nreturn true\n\n}\n\nreturn false\n\n}\n\nAlthough they are not nearly as common in Go as some other C-like\n\nlanguages,\n\nbreak\n\nstatements can be used to terminate\n\na\n\nswitch\n\nearly.\n\nSometimes, though, it's necessary to break out of a surrounding loop,\n\nnot the switch, and in Go that can be accomplished by putting a label\n\non the loop and \"breaking\" to that label.\n\nThis example shows both uses.\n\nLoop:\n\nfor n := 0; n \u003c len(src); n += size {\n\nswitch {\n\ncase src[n] \u003c sizeOne:\n\nif validateOnly {\n\nbreak\n\n}\n\nsize = 1\n\nupdate(src[n])\n\ncase src[n] \u003c sizeTwo:\n\nif n+1 \u003e= len(src) {\n\nerr = errShortInput\n\nbreak Loop\n\n}\n\nif validateOnly {\n\nbreak\n\n}\n\nsize = 2\n\nupdate(src[n] + src[n+1]\u003c\u003cshift)\n\n}\n\n}\n\nOf course, the\n\ncontinue\n\nstatement also accepts an optional label\n\nbut it applies only to loops.\n\nTo close this section, here's a comparison routine for byte slices that uses two\n\nswitch\n\nstatements:\n\n// Compare returns an integer comparing the two byte slices,\n\n// lexicographically.\n\n// The result will be 0 if a == b, -1 if a \u003c b, and +1 if a \u003e b\n\nfunc Compare(a, b []byte) int {\n\nfor i := 0; i \u003c len(a) \u0026\u0026 i \u003c len(b); i++ {\n\nswitch {\n\ncase a[i] \u003e b[i]:\n\nreturn 1\n\ncase a[i] \u003c b[i]:\n\nreturn -1\n\n}\n\n}\n\nswitch {\n\ncase len(a) \u003e len(b):\n\nreturn 1\n\ncase len(a) \u003c len(b):\n\nreturn -1\n\n}\n\nreturn 0\n\n}\n\nType switch\n\nA switch can also be used to discover the dynamic type of an interface\n\nvariable. Such a\n\ntype switch\n\nuses the syntax of a type\n\nassertion with the keyword\n\ntype\n\ninside the parentheses.\n\nIf the switch declares a variable in the expression, the variable will\n\nhave the corresponding type in each clause.\n\nIt's also idiomatic to reuse the name in such cases, in effect declaring\n\na new variable with the same name but a different type in each case.\n\nvar t interface{}\n\nt = functionOfSomeType()\n\nswitch t := t.(type) {\n\ndefault:\n\nfmt.Printf(\"unexpected type %T\\n\", t) // %T prints whatever type t has\n\ncase bool:\n\nfmt.Printf(\"boolean %t\\n\", t) // t has type bool\n\ncase int:\n\nfmt.Printf(\"integer %d\\n\", t) // t has type int\n\ncase *bool:\n\nfmt.Printf(\"pointer to boolean %t\\n\", *t) // t has type *bool\n\ncase *int:\n\nfmt.Printf(\"pointer to integer %d\\n\", *t) // t has type *int\n\n}\n\nFunctions\n\nMultiple return values\n\nOne of Go's unusual features is that functions and methods\n\ncan return multiple values. This form can be used to\n\nimprove on a couple of clumsy idioms in C programs: in-band\n\nerror returns such as\n\n-1\n\nfor\n\nEOF\n\nand modifying an argument passed by address.\n\nIn C, a write error is signaled by a negative count with the\n\nerror code secreted away in a volatile location.\n\nIn Go,\n\nWrite\n\ncan return a count\n\nand\n\nan error: “Yes, you wrote some\n\nbytes but not all of them because you filled the device”.\n\nThe signature of the\n\nWrite\n\nmethod on files from\n\npackage\n\nos\n\nis:\n\nfunc (file *File) Write(b []byte) (n int, err error)\n\nand as the documentation says, it returns the number of bytes\n\nwritten and a non-nil\n\nerror\n\nwhen\n\nn != len(b)\n\n.\n\nThis is a common style; see the section on error handling for more examples.\n\nA similar approach obviates the need to pass a pointer to a return\n\nvalue to simulate a reference parameter.\n\nHere's a simple-minded function to\n\ngrab a number from a position in a byte slice, returning the number\n\nand the next position.\n\nfunc nextInt(b []byte, i int) (int, int) {\n\nfor ; i \u003c len(b) \u0026\u0026 !isDigit(b[i]); i++ {\n\n}\n\nx := 0\n\nfor ; i \u003c len(b) \u0026\u0026 isDigit(b[i]); i++ {\n\nx = x*10 + int(b[i]) - '0'\n\n}\n\nreturn x, i\n\n}\n\nYou could use it to scan the numbers in an input slice\n\nb\n\nlike this:\n\nfor i := 0; i \u003c len(b); {\n\nx, i = nextInt(b, i)\n\nfmt.Println(x)\n\n}\n\nNamed result parameters\n\nThe return or result \"parameters\" of a Go function can be given names and\n\nused as regular variables, just like the incoming parameters.\n\nWhen named, they are initialized to the zero values for their types when\n\nthe function begins; if the function executes a\n\nreturn\n\nstatement\n\nwith no arguments, the current values of the result parameters are\n\nused as the returned values.\n\nThe names are not mandatory but they can make code shorter and clearer:\n\nthey're documentation.\n\nIf we name the results of\n\nnextInt\n\nit becomes\n\nobvious which returned\n\nint\n\nis which.\n\nfunc nextInt(b []byte, pos int) (value, nextPos int) {\n\nBecause named results are initialized and tied to an unadorned return, they can simplify\n\nas well as clarify. Here's a version\n\nof\n\nio.ReadFull\n\nthat uses them well:\n\nfunc ReadFull(r Reader, buf []byte) (n int, err error) {\n\nfor len(buf) \u003e 0 \u0026\u0026 err == nil {\n\nvar nr int\n\nnr, err = r.Read(buf)\n\nn += nr\n\nbuf = buf[nr:]\n\n}\n\nreturn\n\n}\n\nDefer\n\nGo's\n\ndefer\n\nstatement schedules a function call (the\n\ndeferred\n\nfunction) to be run immediately before the function\n\nexecuting the\n\ndefer\n\nreturns. It's an unusual but\n\neffective way to deal with situations such as resources that must be\n\nreleased regardless of which path a function takes to return. The\n\ncanonical examples are unlocking a mutex or closing a file.\n\n// Contents returns the file's contents as a string.\n\nfunc Contents(filename string) (string, error) {\n\nf, err := os.Open(filename)\n\nif err != nil {\n\nreturn \"\", err\n\n}\n\ndefer f.Close() // f.Close will run when we're finished.\n\nvar result []byte\n\nbuf := make([]byte, 100)\n\nfor {\n\nn, err := f.Read(buf[0:])\n\nresult = append(result, buf[0:n]...) // append is discussed later.\n\nif err != nil {\n\nif err == io.EOF {\n\nbreak\n\n}\n\nreturn \"\", err // f will be closed if we return here.\n\n}\n\n}\n\nreturn string(result), nil // f will be closed if we return here.\n\n}\n\nDeferring a call to a function such as\n\nClose\n\nhas two advantages. First, it\n\nguarantees that you will never forget to close the file, a mistake\n\nthat's easy to make if you later edit the function to add a new return\n\npath. Second, it means that the close sits near the open,\n\nwhich is much clearer than placing it at the end of the function.\n\nThe arguments to the deferred function (which include the receiver if\n\nthe function is a method) are evaluated when the\n\ndefer\n\nexecutes, not when the\n\ncall\n\nexecutes. Besides avoiding worries\n\nabout variables changing values as the function executes, this means\n\nthat a single deferred call site can defer multiple function\n\nexecutions. Here's a silly example.\n\nfor i := 0; i \u003c 5; i++ {\n\ndefer fmt.Printf(\"%d \", i)\n\n}\n\nDeferred functions are executed in LIFO order, so this code will cause\n\n4 3 2 1 0\n\nto be printed when the function returns. A\n\nmore plausible example is a simple way to trace function execution\n\nthrough the program. We could write a couple of simple tracing\n\nroutines like this:\n\nfunc trace(s string) { fmt.Println(\"entering:\", s) }\n\nfunc untrace(s string) { fmt.Println(\"leaving:\", s) }\n\n// Use them like this:\n\nfunc a() {\n\ntrace(\"a\")\n\ndefer untrace(\"a\")\n\n// do something....\n\n}\n\nWe can do better by exploiting the fact that arguments to deferred\n\nfunctions are evaluated when the\n\ndefer\n\nexecutes. The\n\ntracing routine can set up the argument to the untracing routine.\n\nThis example:\n\nfunc trace(s string) string {\n\nfmt.Println(\"entering:\", s)\n\nreturn s\n\n}\n\nfunc un(s string) {\n\nfmt.Println(\"leaving:\", s)\n\n}\n\nfunc a() {\n\ndefer un(trace(\"a\"))\n\nfmt.Println(\"in a\")\n\n}\n\nfunc b() {\n\ndefer un(trace(\"b\"))\n\nfmt.Println(\"in b\")\n\na()\n\n}\n\nfunc main() {\n\nb()\n\n}\n\nprints\n\nentering: b\n\nin b\n\nentering: a\n\nin a\n\nleaving: a\n\nleaving: b\n\nFor programmers accustomed to block-level resource management from\n\nother languages,\n\ndefer\n\nmay seem peculiar, but its most\n\ninteresting and powerful applications come precisely from the fact\n\nthat it's not block-based but function-based. In the section on\n\npanic\n\nand\n\nrecover\n\nwe'll see another\n\nexample of its possibilities.\n\nData\n\nAllocation with\n\nnew\n\nGo has two allocation primitives, the built-in functions\n\nnew\n\nand\n\nmake\n\n.\n\nThey do different things and apply to different types, which can be confusing,\n\nbut the rules are simple.\n\nLet's talk about\n\nnew\n\nfirst.\n\nIt's a built-in function that allocates memory, but unlike its namesakes\n\nin some other languages it does not\n\ninitialize\n\nthe memory,\n\nit only\n\nzeros\n\nit.\n\nThat is,\n\nnew(T)\n\nallocates zeroed storage for a new item of type\n\nT\n\nand returns its address, a value of type\n\n*T\n\n.\n\nIn Go terminology, it returns a pointer to a newly allocated zero value of type\n\nT\n\n.\n\nSince the memory returned by\n\nnew\n\nis zeroed, it's helpful to arrange\n\nwhen designing your data structures that the\n\nzero value of each type can be used without further initialization. This means a user of\n\nthe data structure can create one with\n\nnew\n\nand get right to\n\nwork.\n\nFor example, the documentation for\n\nbytes.Buffer\n\nstates that\n\n\"the zero value for\n\nBuffer\n\nis an empty buffer ready to use.\"\n\nSimilarly,\n\nsync.Mutex\n\ndoes not\n\nhave an explicit constructor or\n\nInit\n\nmethod.\n\nInstead, the zero value for a\n\nsync.Mutex\n\nis defined to be an unlocked mutex.\n\nThe zero-value-is-useful property works transitively. Consider this type declaration.\n\ntype SyncedBuffer struct {\n\nlock sync.Mutex\n\nbuffer bytes.Buffer\n\n}\n\nValues of type\n\nSyncedBuffer\n\nare also ready to use immediately upon allocation\n\nor just declaration. In the next snippet, both\n\np\n\nand\n\nv\n\nwill work\n\ncorrectly without further arrangement.\n\np := new(SyncedBuffer) // type *SyncedBuffer\n\nvar v SyncedBuffer // type SyncedBuffer\n\nConstructors and composite literals\n\nSometimes the zero value isn't good enough and an initializing\n\nconstructor is necessary, as in this example derived from\n\npackage\n\nos\n\n.\n\nfunc NewFile(fd int, name string) *File {\n\nif fd \u003c 0 {\n\nreturn nil\n\n}\n\nf := new(File)\n\nf.fd = fd\n\nf.name = name\n\nf.dirinfo = nil\n\nf.nepipe = 0\n\nreturn f\n\n}\n\nThere's a lot of boilerplate in there. We can simplify it\n\nusing a\n\ncomposite literal\n\n, which is\n\nan expression that creates a\n\nnew instance each time it is evaluated.\n\nfunc NewFile(fd int, name string) *File {\n\nif fd \u003c 0 {\n\nreturn nil\n\n}\n\nf := File{fd, name, nil, 0}\n\nreturn \u0026f\n\n}\n\nNote that, unlike in C, it's perfectly OK to return the address of a local variable;\n\nthe storage associated with the variable survives after the function\n\nreturns.\n\nIn fact, taking the address of a composite literal\n\nallocates a fresh instance each time it is evaluated,\n\nso we can combine these last two lines.\n\nreturn \u0026File{fd, name, nil, 0}\n\nThe fields of a composite literal are laid out in order and must all be present.\n\nHowever, by labeling the elements explicitly as\n\nfield : value\n\npairs, the initializers can appear in any\n\norder, with the missing ones left as their respective zero values. Thus we could say\n\nreturn \u0026File{fd: fd, name: name}\n\nAs a limiting case, if a composite literal contains no fields at all, it creates\n\na zero value for the type. The expressions\n\nnew(File)\n\nand\n\n\u0026File{}\n\nare equivalent.\n\nComposite literals can also be created for arrays, slices, and maps,\n\nwith the field labels being indices or map keys as appropriate.\n\nIn these examples, the initializations work regardless of the values of\n\nEnone\n\n,\n\nEio\n\n, and\n\nEinval\n\n, as long as they are distinct.\n\na := [...]string {Enone: \"no error\", Eio: \"Eio\", Einval: \"invalid argument\"}\n\ns := []string {Enone: \"no error\", Eio: \"Eio\", Einval: \"invalid argument\"}\n\nm := map[int]string{Enone: \"no error\", Eio: \"Eio\", Einval: \"invalid argument\"}\n\nAllocation with\n\nmake\n\nBack to allocation.\n\nThe built-in function\n\nmake(T, args )\n\nserves\n\na purpose different from\n\nnew(T)\n\n.\n\nIt creates slices, maps, and channels only, and it returns an\n\ninitialized\n\n(not\n\nzeroed\n\n)\n\nvalue of type\n\nT\n\n(not\n\n*T\n\n).\n\nThe reason for the distinction\n\nis that these three types represent, under the covers, references to data structures that\n\nmust be initialized before use.\n\nA slice, for example, is a three-item descriptor\n\ncontaining a pointer to the data (inside an array), the length, and the\n\ncapacity, and until those items are initialized, the slice is\n\nnil\n\n.\n\nFor slices, maps, and channels,\n\nmake\n\ninitializes the internal data structure and prepares\n\nthe value for use.\n\nFor instance,\n\nmake([]int, 10, 100)\n\nallocates an array of 100 ints and then creates a slice\n\nstructure with length 10 and a capacity of 100 pointing at the first\n\n10 elements of the array.\n\n(When making a slice, the capacity can be omitted; see the section on slices\n\nfor more information.)\n\nIn contrast,\n\nnew([]int)\n\nreturns a pointer to a newly allocated, zeroed slice\n\nstructure, that is, a pointer to a\n\nnil\n\nslice value.\n\nThese examples illustrate the difference between\n\nnew\n\nand\n\nmake\n\n.\n\nvar p *[]int = new([]int) // allocates slice structure; *p == nil; rarely useful\n\nvar v []int = make([]int, 100) // the slice v now refers to a new array of 100 ints\n\n// Unnecessarily complex:\n\nvar p *[]int = new([]int)\n\n*p = make([]int, 100, 100)\n\n// Idiomatic:\n\nv := make([]int, 100)\n\nRemember that\n\nmake\n\napplies only to maps, slices and channels\n\nand does not return a pointer.\n\nTo obtain an explicit pointer allocate with\n\nnew\n\nor take the address\n\nof a variable explicitly.\n\nArrays\n\nArrays are useful when planning the detailed layout of memory and sometimes\n\ncan help avoid allocation, but primarily\n\nthey are a building block for slices, the subject of the next section.\n\nTo lay the foundation for that topic, here are a few words about arrays.\n\nThere are major differences between the ways arrays work in Go and C.\n\nIn Go,\n\nArrays are values. Assigning one array to another copies all the elements.\n\nIn particular, if you pass an array to a function, it\n\nwill receive a\n\ncopy\n\nof the array, not a pointer to it.\n\nThe size of an array is part of its type. The types\n\n[10]int\n\nand\n\n[20]int\n\nare distinct.\n\nThe value property can be useful but also expensive; if you want C-like behavior and efficiency,\n\nyou can pass a pointer to the array.\n\nfunc Sum(a *[3]float64) (sum float64) {\n\nfor _, v := range *a {\n\nsum += v\n\n}\n\nreturn\n\n}\n\narray := [...]float64{7.0, 8.5, 9.1}\n\nx := Sum(\u0026array) // Note the explicit address-of operator\n\nBut even this style isn't idiomatic Go.\n\nUse slices instead.\n\nSlices\n\nSlices wrap arrays to give a more general, powerful, and convenient\n\ninterface to sequences of data. Except for items with explicit\n\ndimension such as transformation matrices, most array programming in\n\nGo is done with slices rather than simple arrays.\n\nSlices hold references to an underlying array, and if you assign one\n\nslice to another, both refer to the same array.\n\nIf a function takes a slice argument, changes it makes to\n\nthe elements of the slice will be visible to the caller, analogous to\n\npassing a pointer to the underlying array. A\n\nRead\n\nfunction can therefore accept a slice argument rather than a pointer\n\nand a count; the length within the slice sets an upper\n\nlimit of how much data to read. Here is the signature of the\n\nRead\n\nmethod of the\n\nFile\n\ntype in package\n\nos\n\n:\n\nfunc (f *File) Read(buf []byte) (n int, err error)\n\nThe method returns the number of bytes read and an error value, if\n\nany.\n\nTo read into the first 32 bytes of a larger buffer\n\nbuf\n\n,\n\nslice\n\n(here used as a verb) the buffer.\n\nn, err := f.Read(buf[0:32])\n\nSuch slicing is common and efficient. In fact, leaving efficiency aside for\n\nthe moment, the following snippet would also read the first 32 bytes of the buffer.\n\nvar n int\n\nvar err error\n\nfor i := 0; i \u003c 32; i++ {\n\nnbytes, e := f.Read(buf[i:i+1]) // Read one byte.\n\nn += nbytes\n\nif nbytes == 0 || e != nil {\n\nerr = e\n\nbreak\n\n}\n\n}\n\nThe length of a slice may be changed as long as it still fits within\n\nthe limits of the underlying array; just assign it to a slice of\n\nitself. The\n\ncapacity\n\nof a slice, accessible by the built-in\n\nfunction\n\ncap\n\n, reports the maximum length the slice may\n\nassume. Here is a function to append data to a slice. If the data\n\nexceeds the capacity, the slice is reallocated. The\n\nresulting slice is returned. The function uses the fact that\n\nlen\n\nand\n\ncap\n\nare legal when applied to the\n\nnil\n\nslice, and return 0.\n\nfunc Append(slice, data []byte) []byte {\n\nl := len(slice)\n\nif l + len(data) \u003e cap(slice) { // reallocate\n\n// Allocate double what's needed, for future growth.\n\nnewSlice := make([]byte, (l+len(data))*2)\n\n// The copy function is predeclared and works for any slice type.\n\ncopy(newSlice, slice)\n\nslice = newSlice\n\n}\n\nslice = slice[0:l+len(data)]\n\ncopy(slice[l:], data)\n\nreturn slice\n\n}\n\nWe must return the slice afterwards because, although\n\nAppend\n\ncan modify the elements of\n\nslice\n\n, the slice itself (the run-time data\n\nstructure holding the pointer, length, and capacity) is passed by value.\n\nThe idea of appending to a slice is so useful it's captured by the\n\nappend\n\nbuilt-in function. To understand that function's\n\ndesign, though, we need a little more information, so we'll return\n\nto it later.\n\nTwo-dimensional slices\n\nGo's arrays and slices are one-dimensional.\n\nTo create the equivalent of a 2D array or slice, it is necessary to define an array-of-arrays\n\nor slice-of-slices, like this:\n\ntype Transform [3][3]float64 // A 3x3 array, really an array of arrays.\n\ntype LinesOfText [][]byte // A slice of byte slices.\n\nBecause slices are variable-length, it is possible to have each inner\n\nslice be a different length.\n\nThat can be a common situation, as in our\n\nLinesOfText\n\nexample: each line has an independent length.\n\ntext := LinesOfText{\n\n[]byte(\"Now is the time\"),\n\n[]byte(\"for all good gophers\"),\n\n[]byte(\"to bring some fun to the party.\"),\n\n}\n\nSometimes it's necessary to allocate a 2D slice, a situation that can arise when\n\nprocessing scan lines of pixels, for instance.\n\nThere are two ways to achieve this.\n\nOne is to allocate each slice independently; the other\n\nis to allocate a single array and point the individual slices into it.\n\nWhich to use depends on your application.\n\nIf the slices might grow or shrink, they should be allocated independently\n\nto avoid overwriting the next line; if not, it can be more efficient to construct\n\nthe object with a single allocation.\n\nFor reference, here are sketches of the two methods.\n\nFirst, a line at a time:\n\n// Allocate the top-level slice.\n\npicture := make([][]uint8, YSize) // One row per unit of y.\n\n// Loop over the rows, allocating the slice for each row.\n\nfor i := range picture {\n\npicture[i] = make([]uint8, XSize)\n\n}\n\nAnd now as one allocation, sliced into lines:\n\n// Allocate the top-level slice, the same as before.\n\npicture := make([][]uint8, YSize) // One row per unit of y.\n\n// Allocate one large slice to hold all the pixels.\n\npixels := make([]uint8, XSize*YSize) // Has type []uint8 even though picture is [][]uint8.\n\n// Loop over the rows, slicing each row from the front of the remaining pixels slice.\n\nfor i := range picture {\n\npicture[i], pixels = pixels[:XSize], pixels[XSize:]\n\n}\n\nMaps\n\nMaps are a convenient and powerful built-in data structure that associate\n\nvalues of one type (the\n\nkey\n\n) with values of another type\n\n(the\n\nelement\n\nor\n\nvalue\n\n).\n\nThe key can be of any type for which the equality operator is defined,\n\nsuch as integers,\n\nfloating point and complex numbers,\n\nstrings, pointers, interfaces (as long as the dynamic type\n\nsupports equality), structs and arrays.\n\nSlices cannot be used as map keys,\n\nbecause equality is not defined on them.\n\nLike slices, maps hold references to an underlying data structure.\n\nIf you pass a map to a function\n\nthat changes the contents of the map, the changes will be visible\n\nin the caller.\n\nMaps can be constructed using the usual composite literal syntax\n\nwith colon-separated key-value pairs,\n\nso it's easy to build them during initialization.\n\nvar timeZone = map[string]int{\n\n\"UTC\": 0*60*60,\n\n\"EST\": -5*60*60,\n\n\"CST\": -6*60*60,\n\n\"MST\": -7*60*60,\n\n\"PST\": -8*60*60,\n\n}\n\nAssigning and fetching map values looks syntactically just like\n\ndoing the same for arrays and slices except that the index doesn't\n\nneed to be an integer.\n\noffset := timeZone[\"EST\"]\n\nAn attempt to fetch a map value with a key that\n\nis not present in the map will return the zero value for the type\n\nof the entries\n\nin the map. For instance, if the map contains integers, looking\n\nup a non-existent key will return\n\n0\n\n.\n\nA set can be implemented as a map with value type\n\nbool\n\n.\n\nSet the map entry to\n\ntrue\n\nto put the value in the set, and then\n\ntest it by simple indexing.\n\nattended := map[string]bool{\n\n\"Ann\": true,\n\n\"Joe\": true,\n\n...\n\n}\n\nif attended[person] { // will be false if person is not in the map\n\nfmt.Println(person, \"was at the meeting\")\n\n}\n\nSometimes you need to distinguish a missing entry from\n\na zero value. Is there an entry for\n\n\"UTC\"\n\nor is that 0 because it's not in the map at all?\n\nYou can discriminate with a form of multiple assignment.\n\nvar seconds int\n\nvar ok bool\n\nseconds, ok = timeZone[tz]\n\nFor obvious reasons this is called the “comma ok” idiom.\n\nIn this example, if\n\ntz\n\nis present,\n\nseconds\n\nwill be set appropriately and\n\nok\n\nwill be true; if not,\n\nseconds\n\nwill be set to zero and\n\nok\n\nwill\n\nbe false.\n\nHere's a function that puts it together with a nice error report:\n\nfunc offset(tz string) int {\n\nif seconds, ok := timeZone[tz]; ok {\n\nreturn seconds\n\n}\n\nlog.Println(\"unknown time zone:\", tz)\n\nreturn 0\n\n}\n\nTo test for presence in the map without worrying about the actual value,\n\nyou can use the\n\nblank identifier\n\n(\n\n_\n\n)\n\nin place of the usual variable for the value.\n\n_, present := timeZone[tz]\n\nTo delete a map entry, use the\n\ndelete\n\nbuilt-in function, whose arguments are the map and the key to be deleted.\n\nIt's safe to do this even if the key is already absent\n\nfrom the map.\n\ndelete(timeZone, \"PDT\") // Now on Standard Time\n\nPrinting\n\nFormatted printing in Go uses a style similar to C's\n\nprintf\n\nfamily but is richer and more general. The functions live in the\n\nfmt\n\npackage and have capitalized names:\n\nfmt.Printf\n\n,\n\nfmt.Fprintf\n\n,\n\nfmt.Sprintf\n\nand so on. The string functions (\n\nSprintf\n\netc.)\n\nreturn a string rather than filling in a provided buffer.\n\nYou don't need to provide a format string. For each of\n\nPrintf\n\n,\n\nFprintf\n\nand\n\nSprintf\n\nthere is another pair\n\nof functions, for instance\n\nPrint\n\nand\n\nPrintln\n\n.\n\nThese functions do not take a format string but instead generate a default\n\nformat for each argument. The\n\nPrintln\n\nversions also insert a blank\n\nbetween arguments and append a newline to the output while\n\nthe\n\nPrint\n\nversions add blanks only if the operand on neither side is a string.\n\nIn this example each line produces the same output.\n\nfmt.Printf(\"Hello %d\\n\", 23)\n\nfmt.Fprint(os.Stdout, \"Hello \", 23, \"\\n\")\n\nfmt.Println(\"Hello\", 23)\n\nfmt.Println(fmt.Sprint(\"Hello \", 23))\n\nThe formatted print functions\n\nfmt.Fprint\n\nand friends take as a first argument any object\n\nthat implements the\n\nio.Writer\n\ninterface; the variables\n\nos.Stdout\n\nand\n\nos.Stderr\n\nare familiar instances.\n\nHere things start to diverge from C. First, the numeric formats such as\n\n%d\n\ndo not take flags for signedness or size; instead, the printing routines use the\n\ntype of the argument to decide these properties.\n\nvar x uint64 = 1\u003c\u003c64 - 1\n\nfmt.Printf(\"%d %x; %d %x\\n\", x, x, int64(x), int64(x))\n\nprints\n\n18446744073709551615 ffffffffffffffff; -1 -1\n\nIf you just want the default conversion, such as decimal for integers, you can use\n\nthe catchall format\n\n%v\n\n(for “value”); the result is exactly\n\nwhat\n\nPrint\n\nand\n\nPrintln\n\nwould produce.\n\nMoreover, that format can print\n\nany\n\nvalue, even arrays, slices, structs, and\n\nmaps. Here is a print statement for the time zone map defined in the previous section.\n\nfmt.Printf(\"%v\\n\", timeZone) // or just fmt.Println(timeZone)\n\nwhich gives output:\n\nmap[CST:-21600 EST:-18000 MST:-25200 PST:-28800 UTC:0]\n\nFor maps,\n\nPrintf\n\nand friends sort the output lexicographically by key.\n\nWhen printing a struct, the modified format\n\n%+v\n\nannotates the\n\nfields of the structure with their names, and for any value the alternate\n\nformat\n\n%#v\n\nprints the value in full Go syntax.\n\ntype T struct {\n\na int\n\nb float64\n\nc string\n\n}\n\nt := \u0026T{ 7, -2.35, \"abc\\tdef\" }\n\nfmt.Printf(\"%v\\n\", t)\n\nfmt.Printf(\"%+v\\n\", t)\n\nfmt.Printf(\"%#v\\n\", t)\n\nfmt.Printf(\"%#v\\n\", timeZone)\n\nprints\n\n\u0026{7 -2.35 abc def}\n\n\u0026{a:7 b:-2.35 c:abc def}\n\n\u0026main.T{a:7, b:-2.35, c:\"abc\\tdef\"}\n\nmap[string]int{\"CST\":-21600, \"EST\":-18000, \"MST\":-25200, \"PST\":-28800, \"UTC\":0}\n\n(Note the ampersands.)\n\nThat quoted string format is also available through\n\n%q\n\nwhen\n\napplied to a value of type\n\nstring\n\nor\n\n[]byte\n\n.\n\nThe alternate format\n\n%#q\n\nwill use backquotes instead if possible.\n\n(The\n\n%q\n\nformat also applies to integers and runes, producing a\n\nsingle-quoted rune constant.)\n\nAlso,\n\n%x\n\nworks on strings, byte arrays and byte slices as well as\n\non integers, generating a long hexadecimal string, and with\n\na space in the format (\n\n% x\n\n) it puts spaces between the bytes.\n\nAnother handy format is\n\n%T\n\n, which prints the\n\ntype\n\nof a value.\n\nfmt.Printf(\"%T\\n\", timeZone)\n\nprints\n\nmap[string]int\n\nIf you want to control the default format for a custom type, all that's required is to define\n\na method with the signature\n\nString() string\n\non the type.\n\nFor our simple type\n\nT\n\n, that might look like this.\n\nfunc (t *T) String() string {\n\nreturn fmt.Sprintf(\"%d/%g/%q\", t.a, t.b, t.c)\n\n}\n\nfmt.Printf(\"%v\\n\", t)\n\nto print in the format\n\n7/-2.35/\"abc\\tdef\"\n\n(If you need to print\n\nvalues\n\nof type\n\nT\n\nas well as pointers to\n\nT\n\n,\n\nthe receiver for\n\nString\n\nmust be of value type; this example used a pointer because\n\nthat's more efficient and idiomatic for struct types.\n\nSee the section below on\n\npointers vs. value receivers\n\nfor more information.)\n\nOur\n\nString\n\nmethod is able to call\n\nSprintf\n\nbecause the\n\nprint routines are fully reentrant and can be wrapped this way.\n\nThere is one important detail to understand about this approach,\n\nhowever: don't construct a\n\nString\n\nmethod by calling\n\nSprintf\n\nin a way that will recur into your\n\nString\n\nmethod indefinitely. This can happen if the\n\nSprintf\n\ncall attempts to print the receiver directly as a string, which in\n\nturn will invoke the method again. It's a common and easy mistake\n\nto make, as this example shows.\n\ntype MyString string\n\nfunc (m MyString) String() string {\n\nreturn fmt.Sprintf(\"MyString=%s\", m) // Error: will recur forever.\n\n}\n\nIt's also easy to fix: convert the argument to the basic string type, which does not have the\n\nmethod.\n\ntype MyString string\n\nfunc (m MyString) String() string {\n\nreturn fmt.Sprintf(\"MyString=%s\", string(m)) // OK: note conversion.\n\n}\n\nIn the\n\ninitialization section\n\nwe'll see another technique that avoids this recursion.\n\nAnother printing technique is to pass a print routine's arguments directly to another such routine.\n\nThe signature of\n\nPrintf\n\nuses the type\n\n...interface{}\n\nfor its final argument to specify that an arbitrary number of parameters (of arbitrary type)\n\ncan appear after the format.\n\nfunc Printf(format string, v ...interface{}) (n int, err error) {\n\nWithin the function\n\nPrintf\n\n,\n\nv\n\nacts like a variable of type\n\n[]interface{}\n\nbut if it is passed to another variadic function, it acts like\n\na regular list of arguments.\n\nHere is the implementation of the\n\nfunction\n\nlog.Println\n\nwe used above. It passes its arguments directly to\n\nfmt.Sprintln\n\nfor the actual formatting.\n\n// Println prints to the standard logger in the manner of fmt.Println.\n\nfunc Println(v ...interface{}) {\n\nstd.Output(2, fmt.Sprintln(v...)) // Output takes parameters (int, string)\n\n}\n\nWe write\n\n...\n\nafter\n\nv\n\nin the nested call to\n\nSprintln\n\nto tell the\n\ncompiler to treat\n\nv\n\nas a list of arguments; otherwise it would just pass\n\nv\n\nas a single slice argument.\n\nThere's even more to printing than we've covered here. See the\n\ngodoc\n\ndocumentation\n\nfor package\n\nfmt\n\nfor the details.\n\nBy the way, a\n\n...\n\nparameter can be of a specific type, for instance\n\n...int\n\nfor a min function that chooses the least of a list of integers:\n\nfunc Min(a ...int) int {\n\nmin := int(^uint(0) \u003e\u003e 1) // largest int\n\nfor _, i := range a {\n\nif i \u003c min {\n\nmin = i\n\n}\n\n}\n\nreturn min\n\n}\n\nAppend\n\nNow we have the missing piece we needed to explain the design of\n\nthe\n\nappend\n\nbuilt-in function. The signature of\n\nappend\n\nis different from our custom\n\nAppend\n\nfunction above.\n\nSchematically, it's like this:\n\nfunc append(slice []\n\nT\n\n, elements ...\n\nT\n\n) []\n\nT\n\nwhere\n\nT\n\nis a placeholder for any given type. You can't\n\nactually write a function in Go where the type\n\nT\n\nis determined by the caller.\n\nThat's why\n\nappend\n\nis built in: it needs support from the\n\ncompiler.\n\nWhat\n\nappend\n\ndoes is append the elements to the end of\n\nthe slice and return the result. The result needs to be returned\n\nbecause, as with our hand-written\n\nAppend\n\n, the underlying\n\narray may change. This simple example\n\nx := []int{1,2,3}\n\nx = append(x, 4, 5, 6)\n\nfmt.Println(x)\n\nprints\n\n[1 2 3 4 5 6]\n\n. So\n\nappend\n\nworks a\n\nlittle like\n\nPrintf\n\n, collecting an arbitrary number of\n\narguments.\n\nBut what if we wanted to do what our\n\nAppend\n\ndoes and\n\nappend a slice to a slice? Easy: use\n\n...\n\nat the call\n\nsite, just as we did in the call to\n\nOutput\n\nabove. This\n\nsnippet produces identical output to the one above.\n\nx := []int{1,2,3}\n\ny := []int{4,5,6}\n\nx = append(x, y...)\n\nfmt.Println(x)\n\nWithout that\n\n...\n\n, it wouldn't compile because the types\n\nwould be wrong;\n\ny\n\nis not of type\n\nint\n\n.\n\nInitialization\n\nAlthough it doesn't look superficially very different from\n\ninitialization in C or C++, initialization in Go is more powerful.\n\nComplex structures can be built during initialization and the ordering\n\nissues among initialized objects, even among different packages, are handled\n\ncorrectly.\n\nConstants\n\nConstants in Go are just that—constant.\n\nThey are created at compile time, even when defined as\n\nlocals in functions,\n\nand can only be numbers, characters (runes), strings or booleans.\n\nBecause of the compile-time restriction, the expressions\n\nthat define them must be constant expressions,\n\nevaluatable by the compiler. For instance,\n\n1\u003c\u003c3\n\nis a constant expression, while\n\nmath.Sin(math.Pi/4)\n\nis not because\n\nthe function call to\n\nmath.Sin\n\nneeds\n\nto happen at run time.\n\nIn Go, enumerated constants are created using the\n\niota\n\nenumerator. Since\n\niota\n\ncan be part of an expression and\n\nexpressions can be implicitly repeated, it is easy to build intricate\n\nsets of values.\n\ntype ByteSize float64\n\nconst (\n\n_ = iota\n\n// ignore first value by assigning to blank identifier\n\nKB ByteSize = 1 \u003c\u003c (10 * iota)\n\nMB\n\nGB\n\nTB\n\nPB\n\nEB\n\nZB\n\nYB\n\n)\n\nThe ability to attach a method such as\n\nString\n\nto any\n\nuser-defined type makes it possible for arbitrary values to format themselves\n\nautomatically for printing.\n\nAlthough you'll see it most often applied to structs, this technique is also useful for\n\nscalar types such as floating-point types like\n\nByteSize\n\n.\n\nfunc (b ByteSize) String() string {\n\nswitch {\n\ncase b \u003e= YB:\n\nreturn fmt.Sprintf(\"%.2fYB\", b/YB)\n\ncase b \u003e= ZB:\n\nreturn fmt.Sprintf(\"%.2fZB\", b/ZB)\n\ncase b \u003e= EB:\n\nreturn fmt.Sprintf(\"%.2fEB\", b/EB)\n\ncase b \u003e= PB:\n\nreturn fmt.Sprintf(\"%.2fPB\", b/PB)\n\ncase b \u003e= TB:\n\nreturn fmt.Sprintf(\"%.2fTB\", b/TB)\n\ncase b \u003e= GB:\n\nreturn fmt.Sprintf(\"%.2fGB\", b/GB)\n\ncase b \u003e= MB:\n\nreturn fmt.Sprintf(\"%.2fMB\", b/MB)\n\ncase b \u003e= KB:\n\nreturn fmt.Sprintf(\"%.2fKB\", b/KB)\n\n}\n\nreturn fmt.Sprintf(\"%.2fB\", b)\n\n}\n\nThe expression\n\nYB\n\nprints as\n\n1.00YB\n\n,\n\nwhile\n\nByteSize(1e13)\n\nprints as\n\n9.09TB\n\n.\n\nThe use here of\n\nSprintf\n\nto implement\n\nByteSize\n\n's\n\nString\n\nmethod is safe\n\n(avoids recurring indefinitely) not because of a conversion but\n\nbecause it calls\n\nSprintf\n\nwith\n\n%f\n\n,\n\nwhich is not a string format:\n\nSprintf\n\nwill only call\n\nthe\n\nString\n\nmethod when it wants a string, and\n\n%f\n\nwants a floating-point value.\n\nVariables\n\nVariables can be initialized just like constants but the\n\ninitializer can be a general expression computed at run time.\n\nvar (\n\nhome = os.Getenv(\"HOME\")\n\nuser = os.Getenv(\"USER\")\n\ngopath = os.Getenv(\"GOPATH\")\n\n)\n\nThe init function\n\nFinally, each source file can define its own niladic\n\ninit\n\nfunction to\n\nset up whatever state is required. (Actually each file can have multiple\n\ninit\n\nfunctions.)\n\nAnd finally means finally:\n\ninit\n\nis called after all the\n\nvariable declarations in the package have evaluated their initializers,\n\nand those are evaluated only after all the imported packages have been\n\ninitialized.\n\nBesides initializations that cannot be expressed as declarations,\n\na common use of\n\ninit\n\nfunctions is to verify or repair\n\ncorrectness of the program state before real execution begins.\n\nfunc init() {\n\nif user == \"\" {\n\nlog.Fatal(\"$USER not set\")\n\n}\n\nif home == \"\" {\n\nhome = \"/home/\" + user\n\n}\n\nif gopath == \"\" {\n\ngopath = home + \"/go\"\n\n}\n\n// gopath may be overridden by --gopath flag on command line.\n\nflag.StringVar(\u0026gopath, \"gopath\", gopath, \"override default GOPATH\")\n\n}\n\nMethods\n\nPointers vs. Values\n\nAs we saw with\n\nByteSize\n\n,\n\nmethods can be defined for any named type (except a pointer or an interface);\n\nthe receiver does not have to be a struct.\n\nIn the discussion of slices above, we wrote an\n\nAppend\n\nfunction. We can define it as a method on slices instead. To do\n\nthis, we first declare a named type to which we can bind the method, and\n\nthen make the receiver for the method a value of that type.\n\ntype ByteSlice []byte\n\nfunc (slice ByteSlice) Append(data []byte) []byte {\n\n// Body exactly the same as the Append function defined above.\n\n}\n\nThis still requires the method to return the updated slice. We can\n\neliminate that clumsiness by redefining the method to take a\n\npointer\n\nto a\n\nByteSlice\n\nas its receiver, so the\n\nmethod can overwrite the caller's slice.\n\nfunc (p *ByteSlice) Append(data []byte) {\n\nslice := *p\n\n// Body as above, without the return.\n\n*p = slice\n\n}\n\nIn fact, we can do even better. If we modify our function so it looks\n\nlike a standard\n\nWrite\n\nmethod, like this,\n\nfunc (p *ByteSlice) Write(data []byte) (n int, err error) {\n\nslice := *p\n\n// Again as above.\n\n*p = slice\n\nreturn len(data), nil\n\n}\n\nthen the type\n\n*ByteSlice\n\nsatisfies the standard interface\n\nio.Writer\n\n, which is handy. For instance, we can\n\nprint into one.\n\nvar b ByteSlice\n\nfmt.Fprintf(\u0026b, \"This hour has %d days\\n\", 7)\n\nWe pass the address of a\n\nByteSlice\n\nbecause only\n\n*ByteSlice\n\nsatisfies\n\nio.Writer\n\n.\n\nThe rule about pointers vs. values for receivers is that value methods\n\ncan be invoked on pointers and values, but pointer methods can only be\n\ninvoked on pointers.\n\nThis rule arises because pointer methods can modify the receiver; invoking\n\nthem on a value would cause the method to receive a copy of the value, so\n\nany modifications would be discarded.\n\nThe language therefore disallows this mistake.\n\nThere is a handy exception, though. When the value is addressable, the\n\nlanguage takes care of the common case of invoking a pointer method on a\n\nvalue by inserting the address operator automatically.\n\nIn our example, the variable\n\nb\n\nis addressable, so we can call\n\nits\n\nWrite\n\nmethod with just\n\nb.Write\n\n. The compiler\n\nwill rewrite that to\n\n(\u0026b).Write\n\nfor us.\n\nBy the way, the idea of using\n\nWrite\n\non a slice of bytes\n\nis central to the implementation of\n\nbytes.Buffer\n\n.\n\nInterfaces and other types\n\nInterfaces\n\nInterfaces in Go provide a way to specify the behavior of an\n\nobject: if something can do\n\nthis\n\n, then it can be used\n\nhere\n\n. We've seen a couple of simple examples already;\n\ncustom printers can be implemented by a\n\nString\n\nmethod\n\nwhile\n\nFprintf\n\ncan generate output to anything\n\nwith a\n\nWrite\n\nmethod.\n\nInterfaces with only one or two methods are common in Go code, and are\n\nusually given a name derived from the method, such as\n\nio.Writer\n\nfor something that implements\n\nWrite\n\n.\n\nA type can implement multiple interfaces.\n\nFor instance, a collection can be sorted\n\nby the routines in package\n\nsort\n\nif it implements\n\nsort.Interface\n\n, which contains\n\nLen()\n\n,\n\nLess(i, j int) bool\n\n, and\n\nSwap(i, j int)\n\n,\n\nand it could also have a custom formatter.\n\nIn this contrived example\n\nSequence\n\nsatisfies both.\n\ntype Sequence []int\n\n// Methods required by sort.Interface.\n\nfunc (s Sequence) Len() int {\n\nreturn len(s)\n\n}\n\nfunc (s Sequence) Less(i, j int) bool {\n\nreturn s[i] \u003c s[j]\n\n}\n\nfunc (s Sequence) Swap(i, j int) {\n\ns[i], s[j] = s[j], s[i]\n\n}\n\n// Copy returns a copy of the Sequence.\n\nfunc (s Sequence) Copy() Sequence {\n\ncopy := make(Sequence, 0, len(s))\n\nreturn append(copy, s...)\n\n}\n\n// Method for printing - sorts the elements before printing.\n\nfunc (s Sequence) String() string {\n\ns = s.Copy()\n\n// Make a copy; don't overwrite argument.\n\nsort.Sort(s)\n\nstr := \"[\"\n\nfor i, elem := range s {\n\n// Loop is O(N²); will fix that in next example.\n\nif i \u003e 0 {\n\nstr += \" \"\n\n}\n\nstr += fmt.Sprint(elem)\n\n}\n\nreturn str + \"]\"\n\n}\n\nConversions\n\nThe\n\nString\n\nmethod of\n\nSequence\n\nis recreating the\n\nwork that\n\nSprint\n\nalready does for slices.\n\n(It also has complexity O(N²), which is poor.) We can share the\n\neffort (and also speed it up) if we convert the\n\nSequence\n\nto a plain\n\n[]int\n\nbefore calling\n\nSprint\n\n.\n\nfunc (s Sequence) String() string {\n\ns = s.Copy()\n\nsort.Sort(s)\n\nreturn fmt.Sprint([]int(s))\n\n}\n\nThis method is another example of the conversion technique for calling\n\nSprintf\n\nsafely from a\n\nString\n\nmethod.\n\nBecause the two types (\n\nSequence\n\nand\n\n[]int\n\n)\n\nare the same if we ignore the type name, it's legal to convert between them.\n\nThe conversion doesn't create a new value, it just temporarily acts\n\nas though the existing value has a new type.\n\n(There are other legal conversions, such as from integer to floating point, that\n\ndo create a new value.)\n\nIt's an idiom in Go programs to convert the\n\ntype of an expression to access a different\n\nset of methods. As an example, we could use the existing\n\ntype\n\nsort.IntSlice\n\nto reduce the entire example\n\nto this:\n\ntype Sequence []int\n\n// Method for printing - sorts the elements before printing\n\nfunc (s Sequence) String() string {\n\ns = s.Copy()\n\nsort.IntSlice(s).Sort()\n\nreturn fmt.Sprint([]int(s))\n\n}\n\nNow, instead of having\n\nSequence\n\nimplement multiple\n\ninterfaces (sorting and printing), we're using the ability of a data item to be\n\nconverted to multiple types (\n\nSequence\n\n,\n\nsort.IntSlice\n\nand\n\n[]int\n\n), each of which does some part of the job.\n\nThat's more unusual in practice but can be effective.\n\nInterface conversions and type assertions\n\nType switches\n\nare a form of conversion: they take an interface and, for\n\neach case in the switch, in a sense convert it to the type of that case.\n\nHere's a simplified version of how the code under\n\nfmt.Printf\n\nturns a value into\n\na string using a type switch.\n\nIf it's already a string, we want the actual string value held by the interface, while if it has a\n\nString\n\nmethod we want the result of calling the method.\n\ntype Stringer interface {\n\nString() string\n\n}\n\nvar value interface{} // Value provided by caller.\n\nswitch str := value.(type) {\n\ncase string:\n\nreturn str\n\ncase Stringer:\n\nreturn str.String()\n\n}\n\nThe first case finds a concrete value; the second converts the interface into another interface.\n\nIt's perfectly fine to mix types this way.\n\nWhat if there's only one type we care about? If we know the value holds a\n\nstring\n\nand we just want to extract it?\n\nA one-case type switch would do, but so would a\n\ntype assertion\n\n.\n\nA type assertion takes an interface value and extracts from it a value of the specified explicit type.\n\nThe syntax borrows from the clause opening a type switch, but with an explicit\n\ntype rather than the\n\ntype\n\nkeyword:\n\nvalue.(typeName)\n\nand the result is a new value with the static type\n\ntypeName\n\n.\n\nThat type must either be the concrete type held by the interface, or a second interface\n\ntype that the value can be converted to.\n\nTo extract the string we know is in the value, we could write:\n\nstr := value.(string)\n\nBut if it turns out that the value does not contain a string, the program will crash with a run-time error.\n\nTo guard against that, use the \"comma, ok\" idiom to test, safely, whether the value is a string:\n\nstr, ok := value.(string)\n\nif ok {\n\nfmt.Printf(\"string value is: %q\\n\", str)\n\n} else {\n\nfmt.Printf(\"value is not a string\\n\")\n\n}\n\nIf the type assertion fails,\n\nstr\n\nwill still exist and be of type string, but it will have\n\nthe zero value, an empty string.\n\nAs an illustration of the capability, here's an\n\nif\n\n-\n\nelse\n\nstatement that's equivalent to the type switch that opened this section.\n\nif str, ok := value.(string); ok {\n\nreturn str\n\n} else if str, ok := value.(Stringer); ok {\n\nreturn str.String()\n\n}\n\nGenerality\n\nIf a type exists only to implement an interface and will\n\nnever have exported methods beyond that interface, there is\n\nno need to export the type itself.\n\nExporting just the interface makes it clear the value has no\n\ninteresting behavior beyond what is described in the\n\ninterface.\n\nIt also avoids the need to repeat the documentation\n\non every instance of a common method.\n\nIn such cases, the constructor should return an interface value\n\nrather than the implementing type.\n\nAs an example, in the hash libraries\n\nboth\n\ncrc32.NewIEEE\n\nand\n\nadler32.New\n\nreturn the interface type\n\nhash.Hash32\n\n.\n\nSubstituting the CRC-32 algorithm for Adler-32 in a Go program\n\nrequires only changing the constructor call;\n\nthe rest of the code is unaffected by the change of algorithm.\n\nA similar approach allows the streaming cipher algorithms\n\nin the various\n\ncrypto\n\npackages to be\n\nseparated from the block ciphers they chain together.\n\nThe\n\nBlock\n\ninterface\n\nin the\n\ncrypto/cipher\n\npackage specifies the\n\nbehavior of a block cipher, which provides encryption\n\nof a single block of data.\n\nThen, by analogy with the\n\nbufio\n\npackage,\n\ncipher packages that implement this interface\n\ncan be used to construct streaming ciphers, represented\n\nby the\n\nStream\n\ninterface, without\n\nknowing the details of the block encryption.\n\nThe\n\ncrypto/cipher\n\ninterfaces look like this:\n\ntype Block interface {\n\nBlockSize() int\n\nEncrypt(dst, src []byte)\n\nDecrypt(dst, src []byte)\n\n}\n\ntype Stream interface {\n\nXORKeyStream(dst, src []byte)\n\n}\n\nHere's the definition of the counter mode (CTR) stream,\n\nwhich turns a block cipher into a streaming cipher; notice\n\nthat the block cipher's details are abstracted away:\n\n// NewCTR returns a Stream that encrypts/decrypts using the given Block in\n\n// counter mode. The length of iv must be the same as the Block's block size.\n\nfunc NewCTR(block Block, iv []byte) Stream\n\nNewCTR\n\napplies not\n\njust to one specific encryption algorithm and data source but to any\n\nimplementation of the\n\nBlock\n\ninterface and any\n\nStream\n\n. Because they return\n\ninterface values, replacing CTR\n\nencryption with other encryption modes is a localized change. The constructor\n\ncalls must be edited, but because the surrounding code must treat the result only\n\nas a\n\nStream\n\n, it won't notice the difference.\n\nInterfaces and methods\n\nSince almost anything can have methods attached, almost anything can\n\nsatisfy an interface. One illustrative example is in the\n\nhttp\n\npackage, which defines the\n\nHandler\n\ninterface. Any object\n\nthat implements\n\nHandler\n\ncan serve HTTP requests.\n\ntype Handler interface {\n\nServeHTTP(ResponseWriter, *Request)\n\n}\n\nResponseWriter\n\nis itself an interface that provides access\n\nto the methods needed to return the response to the client.\n\nThose methods include the standard\n\nWrite\n\nmethod, so an\n\nhttp.ResponseWriter\n\ncan be used wherever an\n\nio.Writer\n\ncan be used.\n\nRequest\n\nis a struct containing a parsed representation\n\nof the request from the client.\n\nFor brevity, let's ignore POSTs and assume HTTP requests are always\n\nGETs; that simplification does not affect the way the handlers are set up.\n\nHere's a trivial implementation of a handler to count the number of times\n\nthe page is visited.\n\n// Simple counter server.\n\ntype Counter struct {\n\nn int\n\n}\n\nfunc (ctr *Counter) ServeHTTP(w http.ResponseWriter, req *http.Request) {\n\nctr.n++\n\nfmt.Fprintf(w, \"counter = %d\\n\", ctr.n)\n\n}\n\n(Keeping with our theme, note how\n\nFprintf\n\ncan print to an\n\nhttp.ResponseWriter\n\n.)\n\nIn a real server, access to\n\nctr.n\n\nwould need protection from\n\nconcurrent access.\n\nSee the\n\nsync\n\nand\n\natomic\n\npackages for suggestions.\n\nFor reference, here's how to attach such a server to a node on the URL tree.\n\nimport \"net/http\"\n\n...\n\nctr := new(Counter)\n\nhttp.Handle(\"/counter\", ctr)\n\nBut why make\n\nCounter\n\na struct? An integer is all that's needed.\n\n(The receiver needs to be a pointer so the increment is visible to the caller.)\n\n// Simpler counter server.\n\ntype Counter int\n\nfunc (ctr *Counter) ServeHTTP(w http.ResponseWriter, req *http.Request) {\n\n*ctr++\n\nfmt.Fprintf(w, \"counter = %d\\n\", *ctr)\n\n}\n\nWhat if your program has some internal state that needs to be notified that a page\n\nhas been visited? Tie a channel to the web page.\n\n// A channel that sends a notification on each visit.\n\n// (Probably want the channel to be buffered.)\n\ntype Chan chan *http.Request\n\nfunc (ch Chan) ServeHTTP(w http.ResponseWriter, req *http.Request) {\n\nch \u003c- req\n\nfmt.Fprint(w, \"notification sent\")\n\n}\n\nFinally, let's say we wanted to present on\n\n/args\n\nthe arguments\n\nused when invoking the server binary.\n\nIt's easy to write a function to print the arguments.\n\nfunc ArgServer() {\n\nfmt.Println(os.Args)\n\n}\n\nHow do we turn that into an HTTP server? We could make\n\nArgServer\n\na method of some type whose value we ignore, but there's a cleaner way.\n\nSince we can define a method for any type except pointers and interfaces,\n\nwe can write a method for a function.\n\nThe\n\nhttp\n\npackage contains this code:\n\n// The HandlerFunc type is an adapter to allow the use of\n\n// ordinary functions as HTTP handlers. If f is a function\n\n// with the appropriate signature, HandlerFunc(f) is a\n\n// Handler object that calls f.\n\ntype HandlerFunc func(ResponseWriter, *Request)\n\n// ServeHTTP calls f(w, req).\n\nfunc (f HandlerFunc) ServeHTTP(w ResponseWriter, req *Request) {\n\nf(w, req)\n\n}\n\nHandlerFunc\n\nis a type with a method,\n\nServeHTTP\n\n,\n\nso values of that type can serve HTTP requests. Look at the implementation\n\nof the method: the receiver is a function,\n\nf\n\n, and the method\n\ncalls\n\nf\n\n. That may seem odd but it's not that different from, say,\n\nthe receiver being a channel and the method sending on the channel.\n\nTo make\n\nArgServer\n\ninto an HTTP server, we first modify it\n\nto have the right signature.\n\n// Argument server.\n\nfunc ArgServer(w http.ResponseWriter, req *http.Request) {\n\nfmt.Fprintln(w, os.Args)\n\n}\n\nArgServer\n\nnow has the same signature as\n\nHandlerFunc\n\n,\n\nso it can be converted to that type to access its methods,\n\njust as we converted\n\nSequence\n\nto\n\nIntSlice\n\nto access\n\nIntSlice.Sort\n\n.\n\nThe code to set it up is concise:\n\nhttp.Handle(\"/args\", http.HandlerFunc(ArgServer))\n\nWhen someone visits the page\n\n/args\n\n,\n\nthe handler installed at that page has value\n\nArgServer\n\nand type\n\nHandlerFunc\n\n.\n\nThe HTTP server will invoke the method\n\nServeHTTP\n\nof that type, with\n\nArgServer\n\nas the receiver, which will in turn call\n\nArgServer\n\n(via the invocation\n\nf(w, req)\n\ninside\n\nHandlerFunc.ServeHTTP\n\n).\n\nThe arguments will then be displayed.\n\nIn this section we have made an HTTP server from a struct, an integer,\n\na channel, and a function, all because interfaces are just sets of\n\nmethods, which can be defined for (almost) any type.\n\nThe blank identifier\n\nWe've mentioned the blank identifier a couple of times now, in the context of\n\nfor range loops\n\nand\n\nmaps\n\n.\n\nThe blank identifier can be assigned or declared with any value of any type, with the\n\nvalue discarded harmlessly.\n\nIt's a bit like writing to the Unix\n\n/dev/null\n\nfile:\n\nit represents a write-only value\n\nto be used as a place-holder\n\nwhere a variable is needed but the actual value is irrelevant.\n\nIt has uses beyond those we've seen already.\n\nThe blank identifier in multiple assignment\n\nThe use of a blank identifier in a\n\nfor range\n\nloop is a\n\nspecial case of a general situation: multiple assignment.\n\nIf an assignment requires multiple values on the left side,\n\nbut one of the values will not be used by the program,\n\na blank identifier on the left-hand-side of\n\nthe assignment avoids the need\n\nto create a dummy variable and makes it clear that the\n\nvalue is to be discarded.\n\nFor instance, when calling a function that returns\n\na value and an error, but only the error is important,\n\nuse the blank identifier to discard the irrelevant value.\n\nif _, err := os.Stat(path); os.IsNotExist(err) {\n\nfmt.Printf(\"%s does not exist\\n\", path)\n\n}\n\nOccasionally you'll see code that discards the error value in order\n\nto ignore the error; this is terrible practice. Always check error returns;\n\nthey're provided for a reason.\n\n// Bad! This code will crash if path does not exist.\n\nfi, _ := os.Stat(path)\n\nif fi.IsDir() {\n\nfmt.Printf(\"%s is a directory\\n\", path)\n\n}\n\nUnused imports and variables\n\nIt is an error to import a package or to declare a variable without using it.\n\nUnused imports bloat the program and slow compilation,\n\nwhile a variable that is initialized but not used is at least\n\na wasted computation and perhaps indicative of a\n\nlarger bug.\n\nWhen a program is under active development, however,\n\nunused imports and variables often arise and it can\n\nbe annoying to delete them just to have the compilation proceed,\n\nonly to have them be needed again later.\n\nThe blank identifier provides a workaround.\n\nThis half-written program has two unused imports\n\n(\n\nfmt\n\nand\n\nio\n\n)\n\nand an unused variable (\n\nfd\n\n),\n\nso it will not compile, but it would be nice to see if the\n\ncode so far is correct.\n\npackage main\n\nimport (\n\n\"fmt\"\n\n\"io\"\n\n\"log\"\n\n\"os\"\n\n)\n\nfunc main() {\n\nfd, err := os.Open(\"test.go\")\n\nif err != nil {\n\nlog.Fatal(err)\n\n}\n\n// TODO: use fd.\n\n}\n\nTo silence complaints about the unused imports, use a\n\nblank identifier to refer to a symbol from the imported package.\n\nSimilarly, assigning the unused variable\n\nfd\n\nto the blank identifier will silence the unused variable error.\n\nThis version of the program does compile.\n\npackage main\n\nimport (\n\n\"fmt\"\n\n\"io\"\n\n\"log\"\n\n\"os\"\n\n)\n\nvar _ = fmt.Printf\n\n// For debugging; delete when done.\n\nvar _ io.Reader\n\n// For debugging; delete when done.\n\nfunc main() {\n\nfd, err := os.Open(\"test.go\")\n\nif err != nil {\n\nlog.Fatal(err)\n\n}\n\n// TODO: use fd.\n\n_ = fd\n\n}\n\nBy convention, the global declarations to silence import errors\n\nshould come right after the imports and be commented,\n\nboth to make them easy to find and as a reminder to clean things up later.\n\nImport for side effect\n\nAn unused import like\n\nfmt\n\nor\n\nio\n\nin the\n\nprevious example should eventually be used or removed:\n\nblank assignments identify code as a work in progress.\n\nBut sometimes it is useful to import a package only for its\n\nside effects, without any explicit use.\n\nFor example, during its\n\ninit\n\nfunction,\n\nthe\n\nnet/http/pprof\n\npackage registers HTTP handlers that provide\n\ndebugging information. It has an exported API, but\n\nmost clients need only the handler registration and\n\naccess the data through a web page.\n\nTo import the package only for its side effects, rename the package\n\nto the blank identifier:\n\nimport _ \"net/http/pprof\"\n\nThis form of import makes clear that the package is being\n\nimported for its side effects, because there is no other possible\n\nuse of the package: in this file, it doesn't have a name.\n\n(If it did, and we didn't use that name, the compiler would reject the program.)\n\nInterface checks\n\nAs we saw in the discussion of\n\ninterfaces\n\nabove,\n\na type need not declare explicitly that it implements an interface.\n\nInstead, a type implements the interface just by implementing the interface's methods.\n\nIn practice, most interface conversions are static and therefore checked at compile time.\n\nFor example, passing an\n\n*os.File\n\nto a function\n\nexpecting an\n\nio.Reader\n\nwill not compile unless\n\n*os.File\n\nimplements the\n\nio.Reader\n\ninterface.\n\nSome interface checks do happen at run-time, though.\n\nOne instance is in the\n\nencoding/json\n\npackage, which defines a\n\nMarshaler\n\ninterface. When the JSON encoder receives a value that implements that interface,\n\nthe encoder invokes the value's marshaling method to convert it to JSON\n\ninstead of doing the standard conversion.\n\nThe encoder checks this property at run time with a\n\ntype assertion\n\nlike:\n\nm, ok := val.(json.Marshaler)\n\nIf it's necessary only to ask whether a type implements an interface, without\n\nactually using the interface itself, perhaps as part of an error check, use the blank\n\nidentifier to ignore the type-asserted value:\n\nif _, ok := val.(json.Marshaler); ok {\n\nfmt.Printf(\"value %v of type %T implements json.Marshaler\\n\", val, val)\n\n}\n\nOne place this situation arises is when it is necessary to guarantee within the package implementing the type that\n\nit actually satisfies the interface.\n\nIf a type—for example,\n\njson.RawMessage\n\n—needs\n\na custom JSON representation, it should implement\n\njson.Marshaler\n\n, but there are no static conversions that would\n\ncause the compiler to verify this automatically.\n\nIf the type inadvertently fails to satisfy the interface, the JSON encoder will still work,\n\nbut will not use the custom implementation.\n\nTo guarantee that the implementation is correct,\n\na global declaration using the blank identifier can be used in the package:\n\nvar _ json.Marshaler = (*RawMessage)(nil)\n\nIn this declaration, the assignment involving a conversion of a\n\n*RawMessage\n\nto a\n\nMarshaler\n\nrequires that\n\n*RawMessage\n\nimplements\n\nMarshaler\n\n,\n\nand that property will be checked at compile time.\n\nShould the\n\njson.Marshaler\n\ninterface change, this package\n\nwill no longer compile and we will be on notice that it needs to be updated.\n\nThe appearance of the blank identifier in this construct indicates that\n\nthe declaration exists only for the type checking,\n\nnot to create a variable.\n\nDon't do this for every type that satisfies an interface, though.\n\nBy convention, such declarations are only used\n\nwhen there are no static conversions already present in the code,\n\nwhich is a rare event.\n\nEmbedding\n\nGo does not provide the typical, type-driven notion of subclassing,\n\nbut it does have the ability to “borrow” pieces of an\n\nimplementation by\n\nembedding\n\ntypes within a struct or\n\ninterface.\n\nInterface embedding is very simple.\n\nWe've mentioned the\n\nio.Reader\n\nand\n\nio.Writer\n\ninterfaces before;\n\nhere are their definitions.\n\ntype Reader interface {\n\nRead(p []byte) (n int, err error)\n\n}\n\ntype Writer interface {\n\nWrite(p []byte) (n int, err error)\n\n}\n\nThe\n\nio\n\npackage also exports several other interfaces\n\nthat specify objects that can implement several such methods.\n\nFor instance, there is\n\nio.ReadWriter\n\n, an interface\n\ncontaining both\n\nRead\n\nand\n\nWrite\n\n.\n\nWe could specify\n\nio.ReadWriter\n\nby listing the\n\ntwo methods explicitly, but it's easier and more evocative\n\nto embed the two interfaces to form the new one, like this:\n\n// ReadWriter is the interface that combines the Reader and Writer interfaces.\n\ntype ReadWriter interface {\n\nReader\n\nWriter\n\n}\n\nThis says just what it looks like: A\n\nReadWriter\n\ncan do\n\nwhat a\n\nReader\n\ndoes\n\nand\n\nwhat a\n\nWriter\n\ndoes; it is a union of the embedded interfaces.\n\nOnly interfaces can be embedded within interfaces.\n\nThe same basic idea applies to structs, but with more far-reaching\n\nimplications. The\n\nbufio\n\npackage has two struct types,\n\nbufio.Reader\n\nand\n\nbufio.Writer\n\n, each of\n\nwhich of course implements the analogous interfaces from package\n\nio\n\n.\n\nAnd\n\nbufio\n\nalso implements a buffered reader/writer,\n\nwhich it does by combining a reader and a writer into one struct\n\nusing embedding: it lists the types within the struct\n\nbut does not give them field names.\n\n// ReadWriter stores pointers to a Reader and a Writer.\n\n// It implements io.ReadWriter.\n\ntype ReadWriter struct {\n\n*Reader // *bufio.Reader\n\n*Writer // *bufio.Writer\n\n}\n\nThe embedded elements are pointers to structs and of course\n\nmust be initialized to point to valid structs before they\n\ncan be used.\n\nThe\n\nReadWriter\n\nstruct could be written as\n\ntype ReadWriter struct {\n\nreader *Reader\n\nwriter *Writer\n\n}\n\nbut then to promote the methods of the fields and to\n\nsatisfy the\n\nio\n\ninterfaces, we would also need\n\nto provide forwarding methods, like this:\n\nfunc (rw *ReadWriter) Read(p []byte) (n int, err error) {\n\nreturn rw.reader.Read(p)\n\n}\n\nBy embedding the structs directly, we avoid this bookkeeping.\n\nThe methods of embedded types come along for free, which means that\n\nbufio.ReadWriter\n\nnot only has the methods of\n\nbufio.Reader\n\nand\n\nbufio.Writer\n\n,\n\nit also satisfies all three interfaces:\n\nio.Reader\n\n,\n\nio.Writer\n\n, and\n\nio.ReadWriter\n\n.\n\nThere's an important way in which embedding differs from subclassing. When we embed a type,\n\nthe methods of that type become methods of the outer type,\n\nbut when they are invoked the receiver of the method is the inner type, not the outer one.\n\nIn our example, when the\n\nRead\n\nmethod of a\n\nbufio.ReadWriter\n\nis\n\ninvoked, it has exactly the same effect as the forwarding method written out above;\n\nthe receiver is the\n\nreader\n\nfield of the\n\nReadWriter\n\n, not the\n\nReadWriter\n\nitself.\n\nEmbedding can also be a simple convenience.\n\nThis example shows an embedded field alongside a regular, named field.\n\ntype Job struct {\n\nCommand string\n\n*log.Logger\n\n}\n\nThe\n\nJob\n\ntype now has the\n\nPrint\n\n,\n\nPrintf\n\n,\n\nPrintln\n\nand other\n\nmethods of\n\n*log.Logger\n\n. We could have given the\n\nLogger\n\na field name, of course, but it's not necessary to do so. And now, once\n\ninitialized, we can\n\nlog to the\n\nJob\n\n:\n\njob.Println(\"starting now...\")\n\nThe\n\nLogger\n\nis a regular field of the\n\nJob\n\nstruct,\n\nso we can initialize it in the usual way inside the constructor for\n\nJob\n\n, like this,\n\nfunc NewJob(command string, logger *log.Logger) *Job {\n\nreturn \u0026Job{command, logger}\n\n}\n\nor with a composite literal,\n\njob := \u0026Job{command, log.New(os.Stderr, \"Job: \", log.Ldate)}\n\nIf we need to refer to an embedded field directly, the type name of the field,\n\nignoring the package qualifier, serves as a field name, as it did\n\nin the\n\nRead\n\nmethod of our\n\nReadWriter\n\nstruct.\n\nHere, if we needed to access the\n\n*log.Logger\n\nof a\n\nJob\n\nvariable\n\njob\n\n,\n\nwe would write\n\njob.Logger\n\n,\n\nwhich would be useful if we wanted to refine the methods of\n\nLogger\n\n.\n\nfunc (job *Job) Printf(format string, args ...interface{}) {\n\njob.Logger.Printf(\"%q: %s\", job.Command, fmt.Sprintf(format, args...))\n\n}\n\nEmbedding types introduces the problem of name conflicts but the rules to resolve\n\nthem are simple.\n\nFirst, a field or method\n\nX\n\nhides any other item\n\nX\n\nin a more deeply\n\nnested part of the type.\n\nIf\n\nlog.Logger\n\ncontained a field or method called\n\nCommand\n\n, the\n\nCommand\n\nfield\n\nof\n\nJob\n\nwould dominate it.\n\nSecond, if the same name appears at the same nesting level, it is usually an error;\n\nit would be erroneous to embed\n\nlog.Logger\n\nif the\n\nJob\n\nstruct\n\ncontained another field or method called\n\nLogger\n\n.\n\nHowever, if the duplicate name is never mentioned in the program outside the type definition, it is OK.\n\nThis qualification provides some protection against changes made to types embedded from outside; there\n\nis no problem if a field is added that conflicts with another field in another subtype if neither field\n\nis ever used.\n\nConcurrency\n\nShare by communicating\n\nConcurrent programming is a large topic and there is space only for some\n\nGo-specific highlights here.\n\nConcurrent programming in many environments is made difficult by the\n\nsubtleties required to implement correct access to shared variables. Go encourages\n\na different approach in which shared values are passed around on channels\n\nand, in fact, never actively shared by separate threads of execution.\n\nOnly one goroutine has access to the value at any given time.\n\nData races cannot occur, by design.\n\nTo encourage this way of thinking we have reduced it to a slogan:\n\nDo not communicate by sharing memory;\n\ninstead, share memory by communicating.\n\nThis approach can be taken too far. Reference counts may be best done\n\nby putting a mutex around an integer variable, for instance. But as a\n\nhigh-level approach, using channels to control access makes it easier\n\nto write clear, correct programs.\n\nOne way to think about this model is to consider a typical single-threaded\n\nprogram running on one CPU. It has no need for synchronization primitives.\n\nNow run another such instance; it too needs no synchronization. Now let those\n\ntwo communicate; if the communication is the synchronizer, there's still no need\n\nfor other synchronization. Unix pipelines, for example, fit this model\n\nperfectly. Although Go's approach to concurrency originates in Hoare's\n\nCommunicating Sequential Processes (CSP),\n\nit can also be seen as a type-safe generalization of Unix pipes.\n\nGoroutines\n\nThey're called\n\ngoroutines\n\nbecause the existing\n\nterms—threads, coroutines, processes, and so on—convey\n\ninaccurate connotations. A goroutine has a simple model: it is a\n\nfunction executing concurrently with other goroutines in the same\n\naddress space. It is lightweight, costing little more than the\n\nallocation of stack space.\n\nAnd the stacks start small, so they are cheap, and grow\n\nby allocating (and freeing) heap storage as required.\n\nGoroutines are multiplexed onto multiple OS threads so if one should\n\nblock, such as while waiting for I/O, others continue to run. Their\n\ndesign hides many of the complexities of thread creation and\n\nmanagement.\n\nPrefix a function or method call with the\n\ngo\n\nkeyword to run the call in a new goroutine.\n\nWhen the call completes, the goroutine\n\nexits, silently. (The effect is similar to the Unix shell's\n\n\u0026\n\nnotation for running a command in the\n\nbackground.)\n\ngo list.Sort() // run list.Sort concurrently; don't wait for it.\n\nA function literal can be handy in a goroutine invocation.\n\nfunc Announce(message string, delay time.Duration) {\n\ngo func() {\n\ntime.Sleep(delay)\n\nfmt.Println(message)\n\n}() // Note the parentheses - must call the function.\n\n}\n\nIn Go, function literals are closures: the implementation makes\n\nsure the variables referred to by the function survive as long as they are active.\n\nThese examples aren't too practical because the functions have no way of signaling\n\ncompletion. For that, we need channels.\n\nChannels\n\nLike maps, channels are allocated with\n\nmake\n\n, and\n\nthe resulting value acts as a reference to an underlying data structure.\n\nIf an optional integer parameter is provided, it sets the buffer size for the channel.\n\nThe default is zero, for an unbuffered or synchronous channel.\n\nci := make(chan int) // unbuffered channel of integers\n\ncj := make(chan int, 0) // unbuffered channel of integers\n\ncs := make(chan *os.File, 100) // buffered channel of pointers to Files\n\nUnbuffered channels combine communication—the exchange of a value—with\n\nsynchronization—guaranteeing that two calculations (goroutines) are in\n\na known state.\n\nThere are lots of nice idioms using channels. Here's one to get us started.\n\nIn the previous section we launched a sort in the background. A channel\n\ncan allow the launching goroutine to wait for the sort to complete.\n\nc := make(chan int) // Allocate a channel.\n\n// Start the sort in a goroutine; when it completes, signal on the channel.\n\ngo func() {\n\nlist.Sort()\n\nc \u003c- 1 // Send a signal; value does not matter.\n\n}()\n\ndoSomethingForAWhile()\n\n\u003c-c // Wait for sort to finish; discard sent value.\n\nReceivers always block until there is data to receive.\n\nIf the channel is unbuffered, the sender blocks until the receiver has\n\nreceived the value.\n\nIf the channel has a buffer, the sender blocks only until the\n\nvalue has been copied to the buffer; if the buffer is full, this\n\nmeans waiting until some receiver has retrieved a value.\n\nA buffered channel can be used like a semaphore, for instance to\n\nlimit throughput. In this example, incoming requests are passed\n\nto\n\nhandle\n\n, which sends a value into the channel, processes\n\nthe request, and then receives a value from the channel\n\nto ready the “semaphore” for the next consumer.\n\nThe capacity of the channel buffer limits the number of\n\nsimultaneous calls to\n\nprocess\n\n.\n\nvar sem = make(chan int, MaxOutstanding)\n\nfunc handle(r *Request) {\n\nsem \u003c- 1 // Wait for active queue to drain.\n\nprocess(r) // May take a long time.\n\n\u003c-sem // Done; enable next request to run.\n\n}\n\nfunc Serve(queue chan *Request) {\n\nfor {\n\nreq := \u003c-queue\n\ngo handle(req) // Don't wait for handle to finish.\n\n}\n\n}\n\nOnce\n\nMaxOutstanding\n\nhandlers are executing\n\nprocess\n\n,\n\nany more will block trying to send into the filled channel buffer,\n\nuntil one of the existing handlers finishes and receives from the buffer.\n\nThis design has a problem, though:\n\nServe\n\ncreates a new goroutine for\n\nevery incoming request, even though only\n\nMaxOutstanding\n\nof them can run at any moment.\n\nAs a result, the program can consume unlimited resources if the requests come in too fast.\n\nWe can address that deficiency by changing\n\nServe\n\nto\n\ngate the creation of the goroutines:\n\nfunc Serve(queue chan *Request) {\n\nfor req := range queue {\n\nsem \u003c- 1\n\ngo func() {\n\nprocess(req)\n\n\u003c-sem\n\n}()\n\n}\n\n}\n\n(Note that in Go versions before 1.22 this code has a bug: the loop\n\nvariable is shared across all goroutines.\n\nSee the\n\nGo wiki\n\nfor details.)\n\nAnother approach that manages resources well is to start a fixed\n\nnumber of\n\nhandle\n\ngoroutines all reading from the request\n\nchannel.\n\nThe number of goroutines limits the number of simultaneous\n\ncalls to\n\nprocess\n\n.\n\nThis\n\nServe\n\nfunction also accepts a channel on which\n\nit will be told to exit; after launching the goroutines it blocks\n\nreceiving from that channel.\n\nfunc handle(queue chan *Request) {\n\nfor r := range queue {\n\nprocess(r)\n\n}\n\n}\n\nfunc Serve(clientRequests chan *Request, quit chan bool) {\n\n// Start handlers\n\nfor i := 0; i \u003c MaxOutstanding; i++ {\n\ngo handle(clientRequests)\n\n}\n\n\u003c-quit // Wait to be told to exit.\n\n}\n\nChannels of channels\n\nOne of the most important properties of Go is that\n\na channel is a first-class value that can be allocated and passed\n\naround like any other. A common use of this property is\n\nto implement safe, parallel demultiplexing.\n\nIn the example in the previous section,\n\nhandle\n\nwas\n\nan idealized handler for a request but we didn't define the\n\ntype it was handling. If that type includes a channel on which\n\nto reply, each client can provide its own path for the answer.\n\nHere's a schematic definition of type\n\nRequest\n\n.\n\ntype Request struct {\n\nargs []int\n\nf func([]int) int\n\nresultChan chan int\n\n}\n\nThe client provides a function and its arguments, as well as\n\na channel inside the request object on which to receive the answer.\n\nfunc sum(a []int) (s int) {\n\nfor _, v := range a {\n\ns += v\n\n}\n\nreturn\n\n}\n\nrequest := \u0026Request{[]int{3, 4, 5}, sum, make(chan int)}\n\n// Send request\n\nclientRequests \u003c- request\n\n// Wait for response.\n\nfmt.Printf(\"answer: %d\\n\", \u003c-request.resultChan)\n\nOn the server side, the handler function is the only thing that changes.\n\nfunc handle(queue chan *Request) {\n\nfor req := range queue {\n\nreq.resultChan \u003c- req.f(req.args)\n\n}\n\n}\n\nThere's clearly a lot more to do to make it realistic, but this\n\ncode is a framework for a rate-limited, parallel, non-blocking RPC\n\nsystem, and there's not a mutex in sight.\n\nParallelization\n\nAnother application of these ideas is to parallelize a calculation\n\nacross multiple CPU cores. If the calculation can be broken into\n\nseparate pieces that can execute independently, it can be parallelized,\n\nwith a channel to signal when each piece completes.\n\nLet's say we have an expensive operation to perform on a vector of items,\n\nand that the value of the operation on each item is independent,\n\nas in this idealized example.\n\ntype Vector []float64\n\n// Apply the operation to v[i], v[i+1] ... up to v[n-1].\n\nfunc (v Vector) DoSome(i, n int, u Vector, c chan int) {\n\nfor ; i \u003c n; i++ {\n\nv[i] += u.Op(v[i])\n\n}\n\nc \u003c- 1 // signal that this piece is done\n\n}\n\nWe launch the pieces independently in a loop, one per CPU.\n\nThey can complete in any order but it doesn't matter; we just\n\ncount the completion signals by draining the channel after\n\nlaunching all the goroutines.\n\nconst numCPU = 4 // number of CPU cores\n\nfunc (v Vector) DoAll(u Vector) {\n\nc := make(chan int, numCPU) // Buffering optional but sensible.\n\nfor i := 0; i \u003c numCPU; i++ {\n\ngo v.DoSome(i*len(v)/numCPU, (i+1)*len(v)/numCPU, u, c)\n\n}\n\n// Drain the channel.\n\nfor i := 0; i \u003c numCPU; i++ {\n\n\u003c-c // wait for one task to complete\n\n}\n\n// All done.\n\n}\n\nRather than create a constant value for numCPU, we can ask the runtime what\n\nvalue is appropriate.\n\nThe function\n\nruntime.NumCPU\n\nreturns the number of hardware CPU cores in the machine, so we could write\n\nvar numCPU = runtime.NumCPU()\n\nThere is also a function\n\nruntime.GOMAXPROCS\n\n,\n\nwhich reports (or sets)\n\nthe user-specified number of cores that a Go program can have running\n\nsimultaneously.\n\nIt defaults to the value of\n\nruntime.NumCPU\n\nbut can be\n\noverridden by setting the similarly named shell environment variable\n\nor by calling the function with a positive number. Calling it with\n\nzero just queries the value.\n\nTherefore if we want to honor the user's resource request, we should write\n\nvar numCPU = runtime.GOMAXPROCS(0)\n\nBe sure not to confuse the ideas of concurrency—structuring a program\n\nas independently executing components—and parallelism—executing\n\ncalculations in parallel for efficiency on multiple CPUs.\n\nAlthough the concurrency features of Go can make some problems easy\n\nto structure as parallel computations, Go is a concurrent language,\n\nnot a parallel one, and not all parallelization problems fit Go's model.\n\nFor a discussion of the distinction, see the talk cited in\n\nthis\n\nblog post\n\n.\n\nA leaky buffer\n\nThe tools of concurrent programming can even make non-concurrent\n\nideas easier to express. Here's an example abstracted from an RPC\n\npackage. The client goroutine loops receiving data from some source,\n\nperhaps a network. To avoid allocating and freeing buffers, it keeps\n\na free list, and uses a buffered channel to represent it. If the\n\nchannel is empty, a new buffer gets allocated.\n\nOnce the message buffer is ready, it's sent to the server on\n\nserverChan\n\n.\n\nvar freeList = make(chan *Buffer, 100)\n\nvar serverChan = make(chan *Buffer)\n\nfunc client() {\n\nfor {\n\nvar b *Buffer\n\n// Grab a buffer if available; allocate if not.\n\nselect {\n\ncase b = \u003c-freeList:\n\n// Got one; nothing more to do.\n\ndefault:\n\n// None free, so allocate a new one.\n\nb = new(Buffer)\n\n}\n\nload(b) // Read next message from the net.\n\nserverChan \u003c- b // Send to server.\n\n}\n\n}\n\nThe server loop receives each message from the client, processes it,\n\nand returns the buffer to the free list.\n\nfunc server() {\n\nfor {\n\nb := \u003c-serverChan // Wait for work.\n\nprocess(b)\n\n// Reuse buffer if there's room.\n\nselect {\n\ncase freeList \u003c- b:\n\n// Buffer on free list; nothing more to do.\n\ndefault:\n\n// Free list full, just carry on.\n\n}\n\n}\n\n}\n\nThe client attempts to retrieve a buffer from\n\nfreeList\n\n;\n\nif none is available, it allocates a fresh one.\n\nThe server's send to\n\nfreeList\n\nputs\n\nb\n\nback\n\non the free list unless the list is full, in which case the\n\nbuffer is dropped on the floor to be reclaimed by\n\nthe garbage collector.\n\n(The\n\ndefault\n\nclauses in the\n\nselect\n\nstatements execute when no other case is ready,\n\nmeaning that the\n\nselects\n\nnever block.)\n\nThis implementation builds a leaky bucket free list\n\nin just a few lines, relying on the buffered channel and\n\nthe garbage collector for bookkeeping.\n\nErrors\n\nLibrary routines must often return some sort of error indication to\n\nthe caller.\n\nAs mentioned earlier, Go's multivalue return makes it\n\neasy to return a detailed error description alongside the normal\n\nreturn value.\n\nIt is good style to use this feature to provide detailed error information.\n\nFor example, as we'll see,\n\nos.Open\n\ndoesn't\n\njust return a\n\nnil\n\npointer on failure, it also returns an\n\nerror value that describes what went wrong.\n\nBy convention, errors have type\n\nerror\n\n,\n\na simple built-in interface.\n\ntype error interface {\n\nError() string\n\n}\n\nA library writer is free to implement this interface with a\n\nricher model under the covers, making it possible not only\n\nto see the error but also to provide some context.\n\nAs mentioned, alongside the usual\n\n*os.File\n\nreturn value,\n\nos.Open\n\nalso returns an\n\nerror value.\n\nIf the file is opened successfully, the error will be\n\nnil\n\n,\n\nbut when there is a problem, it will hold an\n\nos.PathError\n\n:\n\n// PathError records an error and the operation and\n\n// file path that caused it.\n\ntype PathError struct {\n\nOp string // \"open\", \"unlink\", etc.\n\nPath string // The associated file.\n\nErr error // Returned by the system call.\n\n}\n\nfunc (e *PathError) Error() string {\n\nreturn e.Op + \" \" + e.Path + \": \" + e.Err.Error()\n\n}\n\nPathError\n\n's\n\nError\n\ngenerates\n\na string like this:\n\nopen /etc/passwx: no such file or directory\n\nSuch an error, which includes the problematic file name, the\n\noperation, and the operating system error it triggered, is useful even\n\nif printed far from the call that caused it;\n\nit is much more informative than the plain\n\n\"no such file or directory\".\n\nWhen feasible, error strings should identify their origin, such as by having\n\na prefix naming the operation or package that generated the error. For example, in package\n\nimage\n\n, the string representation for a decoding error due to an\n\nunknown format is \"image: unknown format\".\n\nCallers that care about the precise error details can\n\nuse a type switch or a type assertion to look for specific\n\nerrors and extract details. For\n\nPathErrors\n\nthis might include examining the internal\n\nErr\n\nfield for recoverable failures.\n\nfor try := 0; try \u003c 2; try++ {\n\nfile, err = os.Create(filename)\n\nif err == nil {\n\nreturn\n\n}\n\nif e, ok := err.(*os.PathError); ok \u0026\u0026 e.Err == syscall.ENOSPC {\n\ndeleteTempFiles() // Recover some space.\n\ncontinue\n\n}\n\nreturn\n\n}\n\nThe second\n\nif\n\nstatement here is another\n\ntype assertion\n\n.\n\nIf it fails,\n\nok\n\nwill be false, and\n\ne\n\nwill be\n\nnil\n\n.\n\nIf it succeeds,\n\nok\n\nwill be true, which means the\n\nerror was of type\n\n*os.PathError\n\n, and then so is\n\ne\n\n,\n\nwhich we can examine for more information about the error.\n\nPanic\n\nThe usual way to report an error to a caller is to return an\n\nerror\n\nas an extra return value. The canonical\n\nRead\n\nmethod is a well-known instance; it returns a byte\n\ncount and an\n\nerror\n\n. But what if the error is\n\nunrecoverable? Sometimes the program simply cannot continue.\n\nFor this purpose, there is a built-in function\n\npanic\n\nthat in effect creates a run-time error that will stop the program\n\n(but see the next section). The function takes a single argument\n\nof arbitrary type—often a string—to be printed as the\n\nprogram dies. It's also a way to indicate that something impossible has\n\nhappened, such as exiting an infinite loop.\n\n// A toy implementation of cube root using Newton's method.\n\nfunc CubeRoot(x float64) float64 {\n\nz := x/3 // Arbitrary initial value\n\nfor i := 0; i \u003c 1e6; i++ {\n\nprevz := z\n\nz -= (z*z*z-x) / (3*z*z)\n\nif veryClose(z, prevz) {\n\nreturn z\n\n}\n\n}\n\n// A million iterations has not converged; something is wrong.\n\npanic(fmt.Sprintf(\"CubeRoot(%g) did not converge\", x))\n\n}\n\nThis is only an example but real library functions should\n\navoid\n\npanic\n\n. If the problem can be masked or worked\n\naround, it's always better to let things continue to run rather\n\nthan taking down the whole program. One possible counterexample\n\nis during initialization: if the library truly cannot set itself up,\n\nit might be reasonable to panic, so to speak.\n\nvar user = os.Getenv(\"USER\")\n\nfunc init() {\n\nif user == \"\" {\n\npanic(\"no value for $USER\")\n\n}\n\n}\n\nRecover\n\nWhen\n\npanic\n\nis called, including implicitly for run-time\n\nerrors such as indexing a slice out of bounds or failing a type\n\nassertion, it immediately stops execution of the current function\n\nand begins unwinding the stack of the goroutine, running any deferred\n\nfunctions along the way. If that unwinding reaches the top of the\n\ngoroutine's stack, the program dies. However, it is possible to\n\nuse the built-in function\n\nrecover\n\nto regain control\n\nof the goroutine and resume normal execution.\n\nA call to\n\nrecover\n\nstops the unwinding and returns the\n\nargument passed to\n\npanic\n\n. Because the only code that\n\nruns while unwinding is inside deferred functions,\n\nrecover\n\nis only useful inside deferred functions.\n\nOne application of\n\nrecover\n\nis to shut down a failing goroutine\n\ninside a server without killing the other executing goroutines.\n\nfunc server(workChan \u003c-chan *Work) {\n\nfor work := range workChan {\n\ngo safelyDo(work)\n\n}\n\n}\n\nfunc safelyDo(work *Work) {\n\ndefer func() {\n\nif err := recover(); err != nil {\n\nlog.Println(\"work failed:\", err)\n\n}\n\n}()\n\ndo(work)\n\n}\n\nIn this example, if\n\ndo(work)\n\npanics, the result will be\n\nlogged and the goroutine will exit cleanly without disturbing the\n\nothers. There's no need to do anything else in the deferred closure;\n\ncalling\n\nrecover\n\nhandles the condition completely.\n\nBecause\n\nrecover\n\nalways returns\n\nnil\n\nunless called directly\n\nfrom a deferred function, deferred code can call library routines that themselves\n\nuse\n\npanic\n\nand\n\nrecover\n\nwithout failing. As an example,\n\nthe deferred function in\n\nsafelyDo\n\nmight call a logging function before\n\ncalling\n\nrecover\n\n, and that logging code would run unaffected\n\nby the panicking state.\n\nWith our recovery pattern in place, the\n\ndo\n\nfunction (and anything it calls) can get out of any bad situation\n\ncleanly by calling\n\npanic\n\n. We can use that idea to\n\nsimplify error handling in complex software. Let's look at an\n\nidealized version of a\n\nregexp\n\npackage, which reports\n\nparsing errors by calling\n\npanic\n\nwith a local\n\nerror type. Here's the definition of\n\nError\n\n,\n\nan\n\nerror\n\nmethod, and the\n\nCompile\n\nfunction.\n\n// Error is the type of a parse error; it satisfies the error interface.\n\ntype Error string\n\nfunc (e Error) Error() string {\n\nreturn string(e)\n\n}\n\n// error is a method of *Regexp that reports parsing errors by\n\n// panicking with an Error.\n\nfunc (regexp *Regexp) error(err string) {\n\npanic(Error(err))\n\n}\n\n// Compile returns a parsed representation of the regular expression.\n\nfunc Compile(str string) (regexp *Regexp, err error) {\n\nregexp = new(Regexp)\n\n// doParse will panic if there is a parse error.\n\ndefer func() {\n\nif e := recover(); e != nil {\n\nregexp = nil // Clear return value.\n\nerr = e.(Error) // Will re-panic if not a parse error.\n\n}\n\n}()\n\nreturn regexp.doParse(str), nil\n\n}\n\nIf\n\ndoParse\n\npanics, the recovery block will set the\n\nreturn value to\n\nnil\n\n—deferred functions can modify\n\nnamed return values. It will then check, in the assignment\n\nto\n\nerr\n\n, that the problem was a parse error by asserting\n\nthat it has the local type\n\nError\n\n.\n\nIf it does not, the type assertion will fail, causing a run-time error\n\nthat continues the stack unwinding as though nothing had interrupted\n\nit.\n\nThis check means that if something unexpected happens, such\n\nas an index out of bounds, the code will fail even though we\n\nare using\n\npanic\n\nand\n\nrecover\n\nto handle\n\nparse errors.\n\nWith error handling in place, the\n\nerror\n\nmethod (because it's a\n\nmethod bound to a type, it's fine, even natural, for it to have the same name\n\nas the builtin\n\nerror\n\ntype)\n\nmakes it easy to report parse errors without worrying about unwinding\n\nthe parse stack by hand:\n\nif pos == 0 {\n\nre.error(\"'*' illegal at start of expression\")\n\n}\n\nUseful though this pattern is, it should be used only within a package.\n\nParse\n\nturns its internal\n\npanic\n\ncalls into\n\nerror\n\nvalues; it does not expose\n\npanics\n\nto its client. That is a good rule to follow.\n\nBy the way, this re-panic idiom changes the panic value if an actual\n\nerror occurs. However, both the original and new failures will be\n\npresented in the crash report, so the root cause of the problem will\n\nstill be visible. Thus this simple re-panic approach is usually\n\nsufficient—it's a crash after all—but if you want to\n\ndisplay only the original value, you can write a little more code to\n\nfilter unexpected problems and re-panic with the original error.\n\nThat's left as an exercise for the reader.\n\nA web server\n\nLet's finish with a complete Go program, a web server.\n\nThis one is actually a kind of web re-server.\n\nGoogle provides a service at\n\nchart.apis.google.com\n\nthat does automatic formatting of data into charts and graphs.\n\nIt's hard to use interactively, though,\n\nbecause you need to put the data into the URL as a query.\n\nThe program here provides a nicer interface to one form of data: given a short piece of text,\n\nit calls on the chart server to produce a QR code, a matrix of boxes that encode the\n\ntext.\n\nThat image can be grabbed with your cell phone's camera and interpreted as,\n\nfor instance, a URL, saving you typing the URL into the phone's tiny keyboard.\n\nHere's the complete program.\n\nAn explanation follows.\n\npackage main\n\nimport (\n\n\"flag\"\n\n\"html/template\"\n\n\"log\"\n\n\"net/http\"\n\n)\n\nvar addr = flag.String(\"addr\", \":1718\", \"http service address\")\n\n// Q=17, R=18\n\nvar templ = template.Must(template.New(\"qr\").Parse(templateStr))\n\nfunc main() {\n\nflag.Parse()\n\nhttp.Handle(\"/\", http.HandlerFunc(QR))\n\nerr := http.ListenAndServe(*addr, nil)\n\nif err != nil {\n\nlog.Fatal(\"ListenAndServe:\", err)\n\n}\n\n}\n\nfunc QR(w http.ResponseWriter, req *http.Request) {\n\ntempl.Execute(w, req.FormValue(\"s\"))\n\n}\n\nconst templateStr = `\n\n\u003chtml\u003e\n\n\u003chead\u003e\n\n\u003ctitle\u003eQR Link Generator\u003c/title\u003e\n\n\u003c/head\u003e\n\n\u003cbody\u003e\n\n{{if .}}\n\n\u003cimg src=\"http://chart.apis.google.com/chart?chs=300x300\u0026cht=qr\u0026choe=UTF-8\u0026chl={{.}}\" /\u003e\n\n\u003cbr\u003e\n\n{{.}}\n\n\u003cbr\u003e\n\n\u003cbr\u003e\n\n{{end}}\n\n\u003cform action=\"/\" name=f method=\"GET\"\u003e\n\n\u003cinput maxLength=1024 size=70 name=s value=\"\" title=\"Text to QR Encode\"\u003e\n\n\u003cinput type=submit value=\"Show QR\" name=qr\u003e\n\n\u003c/form\u003e\n\n\u003c/body\u003e\n\n\u003c/html\u003e\n\n`\n\nThe pieces up to\n\nmain\n\nshould be easy to follow.\n\nThe one flag sets a default HTTP port for our server. The template\n\nvariable\n\ntempl\n\nis where the fun happens. It builds an HTML template\n\nthat will be executed by the server to display the page; more about\n\nthat in a moment.\n\nThe\n\nmain\n\nfunction parses the flags and, using the mechanism\n\nwe talked about above, binds the function\n\nQR\n\nto the root path\n\nfor the server. Then\n\nhttp.ListenAndServe\n\nis called to start the\n\nserver; it blocks while the server runs.\n\nQR\n\njust receives the request, which contains form data, and\n\nexecutes the template on the data in the form value named\n\ns\n\n.\n\nThe template package\n\nhtml/template\n\nis powerful;\n\nthis program just touches on its capabilities.\n\nIn essence, it rewrites a piece of HTML text on the fly by substituting elements derived\n\nfrom data items passed to\n\ntempl.Execute\n\n, in this case the\n\nform value.\n\nWithin the template text (\n\ntemplateStr\n\n),\n\ndouble-brace-delimited pieces denote template actions.\n\nThe piece from\n\n{{if .}}\n\nto\n\n{{end}}\n\nexecutes only if the value of the current data item, called\n\n.\n\n(dot),\n\nis non-empty.\n\nThat is, when the string is empty, this piece of the template is suppressed.\n\nThe two snippets\n\n{{.}}\n\nsay to show the data presented to\n\nthe template—the query string—on the web page.\n\nThe HTML template package automatically provides appropriate escaping so the\n\ntext is safe to display.\n\nThe rest of the template string is just the HTML to show when the page loads.\n\nIf this is too quick an explanation, see the\n\ndocumentation\n\nfor the template package for a more thorough discussion.\n\nAnd there you have it: a useful web server in a few lines of code plus some\n\ndata-driven HTML text.\n\nGo is powerful enough to make a lot happen in a few lines.\n\ngo.dev uses cookies from Google to deliver and enhance the quality of its services and to\n\nanalyze traffic.\n\nLearn more. Okay",
      "metadata": {
        "characters": "99623",
        "title": "Effective Go - The Go Programming Language",
        "type": "html"
      },
      "word_count": 16530,
      "quality": "high",
      "extracted_at": "2025-08-10T20:51:37Z"
    },
    {
      "url": "https://go.dev/doc/tutorial/getting-started",
      "title": "Getting Started with Go",
      "content": "Documentation Tutorials Tutorial: Get started with Go\n\nTutorial: Get started with Go\n\nIn this tutorial, you'll get a brief introduction to Go programming. Along the\n\nway, you will:\n\nInstall Go (if you haven't already).\n\nWrite some simple \"Hello, world\" code.\n\nUse the\n\ngo\n\ncommand to run your code.\n\nUse the Go package discovery tool to find packages you can use in your own\n\ncode.\n\nCall functions of an external module.\n\nPrerequisites\n\nSome programming experience.\n\nThe code here is pretty\n\nsimple, but it helps to know something about functions.\n\nA tool to edit your code.\n\nAny text editor you have will\n\nwork fine. Most text editors have good support for Go. The most popular are\n\nVSCode (free), GoLand (paid), and Vim (free).\n\nA command terminal.\n\nGo works well using any terminal on\n\nLinux and Mac, and on PowerShell or cmd in Windows.\n\nInstall Go\n\nJust use the\n\nDownload and install\n\nsteps.\n\nWrite some code\n\nGet started with Hello, World.\n\nOpen a command prompt and cd to your home directory.\n\nOn Linux or Mac:\n\ncd\n\nOn Windows:\n\ncd %HOMEPATH%\n\nCreate a hello directory for your first Go source code.\n\nFor example, use the following commands:\n\nmkdir hello\n\ncd hello\n\nEnable dependency tracking for your code.\n\nWhen your code imports packages contained in other modules, you manage\n\nthose dependencies through your code's own module. That module is defined\n\nby a go.mod file that tracks the modules that provide those packages. That\n\ngo.mod file stays with your code, including in your source code\n\nrepository.\n\nTo enable dependency tracking for your code by creating a go.mod file, run\n\nthe\n\ngo mod init\n\ncommand,\n\ngiving it the name of the module your code will be in. The name is the\n\nmodule's module path.\n\nIn actual development, the module path will typically be the repository\n\nlocation where your source code will be kept. For example, the module\n\npath might be\n\ngithub.com/mymodule\n\n. If you plan to publish\n\nyour module for others to use, the module path\n\nmust\n\nbe a\n\nlocation from which Go tools can download your module. For more about\n\nnaming a module with a module path, see\n\nManaging\n\ndependencies\n\n.\n\nFor the purposes of this tutorial, just use\n\nexample/hello\n\n.\n\n$ go mod init example/hello\n\ngo: creating new go.mod: module example/hello\n\nIn your text editor, create a file hello.go in which to write your code.\n\nPaste the following code into your hello.go file and save the file.\n\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n\nfmt.Println(\"Hello, World!\")\n\n}\n\nThis is your Go code. In this code, you:\n\nDeclare a\n\nmain\n\npackage (a package is a way to group\n\nfunctions, and it's made up of all the files in the same directory).\n\nImport the popular\n\nfmt package\n\n,\n\nwhich contains functions for formatting text, including printing to the\n\nconsole. This package is one of the\n\nstandard library\n\npackages you got\n\nwhen you installed Go.\n\nImplement a\n\nmain\n\nfunction to print a message to the\n\nconsole. A\n\nmain\n\nfunction executes by default when you run\n\nthe\n\nmain\n\npackage.\n\nRun your code to see the greeting.\n\n$ go run .\n\nHello, World!\n\nThe\n\ngo run\n\ncommand\n\nis one of many\n\ngo\n\ncommands you'll use to get things done with\n\nGo. Use the following command to get a list of the others:\n\n$ go help\n\nCall code in an external package\n\nWhen you need your code to do something that might have been implemented by\n\nsomeone else, you can look for a package that has functions you can use in\n\nyour code.\n\nMake your printed message a little more interesting with a function from an\n\nexternal module.\n\nVisit pkg.go.dev and\n\nsearch for a \"quote\" package\n\n.\n\nIn the search results, locate and click on the v1 of the\n\nrsc.io/quote\n\npackage\n\n(it should be listed with the \"Other major versions\" of\n\nrsc.io/quote/v4\n\n).\n\nIn the\n\nDocumentation\n\nsection, under\n\nIndex\n\n, note the\n\nlist of functions you can call from your code. You'll use the\n\nGo\n\nfunction.\n\nAt the top of this page, note that package\n\nquote\n\nis\n\nincluded in the\n\nrsc.io/quote\n\nmodule.\n\nYou can use the pkg.go.dev site to find published modules whose packages\n\nhave functions you can use in your own code. Packages are published in\n\nmodules -- like\n\nrsc.io/quote\n\n-- where others can use them.\n\nModules are improved with new versions over time, and you can upgrade your\n\ncode to use the improved versions.\n\nIn your Go code, import the\n\nrsc.io/quote\n\npackage and add a call\n\nto its\n\nGo\n\nfunction.\n\nAfter adding the highlighted lines, your code should include the\n\nfollowing:\n\npackage main\n\nimport \"fmt\"\n\nimport \"rsc.io/quote\"\n\nfunc main() {\n\nfmt.Println(quote.Go())\n\n}\n\nAdd new module requirements and sums.\n\nGo will add the\n\nquote\n\nmodule as a requirement, as well as a\n\ngo.sum file for use in authenticating the module. For more, see\n\nAuthenticating modules\n\nin the Go\n\nModules Reference.\n\n$ go mod tidy\n\ngo: finding module for package rsc.io/quote\n\ngo: found rsc.io/quote in rsc.io/quote v1.5.2\n\nRun your code to see the message generated by the function you're calling.\n\n$ go run .\n\nDon't communicate by sharing memory, share memory by communicating.\n\nNotice that your code calls the\n\nGo\n\nfunction, printing a\n\nclever message about communication.\n\nWhen you ran\n\ngo mod tidy\n\n, it located and downloaded the\n\nrsc.io/quote\n\nmodule that contains the package you imported.\n\nBy default, it downloaded the latest version -- v1.5.2.\n\nWrite more code\n\nWith this quick introduction, you got Go installed and learned some of the\n\nbasics. To write some more code with another tutorial, take a look at\n\nCreate a Go module\n\n.\n\ngo.dev uses cookies from Google to deliver and enhance the quality of its services and to\n\nanalyze traffic.\n\nLearn more. Okay",
      "metadata": {
        "characters": "5604",
        "title": "Tutorial: Get started with Go - The Go Programming Language",
        "type": "html"
      },
      "word_count": 940,
      "quality": "medium",
      "extracted_at": "2025-08-10T20:51:39Z"
    },
    {
      "url": "https://en.wikipedia.org/wiki/Machine_learning",
      "title": "Machine Learning",
      "content": "Jump to content\n\nFrom Wikipedia, the free encyclopedia\n\nStudy of algorithms that improve automatically through experience\n\nFor the journal, see\n\nMachine Learning (journal)\n\n.\n\n\"Statistical learning\" redirects here. For statistical learning in linguistics, see\n\nStatistical learning in language acquisition\n\n.\n\nPart of a series on\n\nMachine learning\n\nand\n\ndata mining\n\nParadigms\n\nSupervised learning Unsupervised learning Semi-supervised learning Self-supervised learning Reinforcement learning Meta-learning Online learning Batch learning Curriculum learning Rule-based learning Neuro-symbolic AI Neuromorphic engineering Quantum machine learning\n\nProblems\n\nClassification Generative modeling Regression Clustering Dimensionality reduction Density estimation Anomaly detection Data cleaning AutoML Association rules Semantic analysis Structured prediction Feature engineering Feature learning Learning to rank Grammar induction Ontology learning Multimodal learning Supervised learning ( classification • regression ) Apprenticeship learning Decision trees Ensembles Bagging Boosting Random forest k -NN Linear regression Naive Bayes Artificial neural networks Logistic regression Perceptron Relevance vector machine (RVM) Support vector machine (SVM) Clustering BIRCH CURE Hierarchical k -means Fuzzy Expectation–maximization (EM) DBSCAN OPTICS Mean shift Dimensionality reduction Factor analysis CCA ICA LDA NMF PCA PGD t-SNE SDL Structured prediction Graphical models Bayes net Conditional random field Hidden Markov Anomaly detection RANSAC k -NN Local outlier factor Isolation forest Neural networks Autoencoder Deep learning Feedforward neural network Recurrent neural network LSTM GRU ESN reservoir computing Boltzmann machine Restricted GAN Diffusion model SOM Convolutional neural network U-Net LeNet AlexNet DeepDream Neural field Neural radiance field Physics-informed neural networks Transformer Vision Mamba Spiking neural network Memtransistor Electrochemical RAM\n\n(ECRAM)\n\nReinforcement learning Q-learning Policy gradient SARSA Temporal difference (TD) Multi-agent Self-play\n\nLearning with humans\n\nActive learning Crowdsourcing Human-in-the-loop Mechanistic interpretability RLHF\n\nModel diagnostics\n\nCoefficient of determination Confusion matrix Learning curve ROC curve\n\nMathematical foundations\n\nKernel machines Bias–variance tradeoff Computational learning theory Empirical risk minimization Occam learning PAC learning Statistical learning VC theory Topological deep learning\n\nJournals and conferences\n\nAAAI ECML PKDD NeurIPS ICML ICLR IJCAI ML JMLR\n\nRelated articles\n\nGlossary of artificial intelligence List of datasets for machine-learning research List of datasets in computer vision and image processing Outline of machine learning v t e\n\nPart of a series on\n\nArtificial intelligence (AI) Major goals Artificial general intelligence Intelligent agent Recursive self-improvement Planning Computer vision General game playing Knowledge representation Natural language processing Robotics AI safety\n\nApproaches\n\nMachine learning Symbolic Deep learning Bayesian networks Evolutionary algorithms Hybrid intelligent systems Systems integration Applications Bioinformatics Deepfake Earth sciences Finance Generative AI Art Audio Music Government Healthcare Mental health Industry Software development Translation Military Physics Projects Philosophy Artificial consciousness Chinese room Friendly AI Control problem\n\n/\n\nTakeover Ethics Existential risk Turing test Uncanny valley History Timeline Progress AI winter AI boom\n\nGlossary\n\nGlossary v t e Machine learning\n\n(\n\nML\n\n) is a\n\nfield of study\n\nin\n\nartificial intelligence\n\nconcerned with the development and study of\n\nstatistical algorithms\n\nthat can learn from\n\ndata\n\nand\n\ngeneralise\n\nto unseen data, and thus perform\n\ntasks\n\nwithout explicit\n\ninstructions\n\n.\n\n[ 1 ]\n\nWithin a subdiscipline in machine learning, advances in the field of\n\ndeep learning\n\nhave allowed\n\nneural networks\n\n, a class of statistical algorithms, to surpass many previous machine learning approaches in performance.\n\n[ 2 ]\n\nML finds application in many fields, including\n\nnatural language processing\n\n,\n\ncomputer vision\n\n,\n\nspeech recognition\n\n,\n\nemail filtering\n\n,\n\nagriculture\n\n, and\n\nmedicine\n\n. The application of ML to business problems is known as\n\npredictive analytics\n\n.\n\nStatistics\n\nand\n\nmathematical optimisation\n\n(mathematical programming) methods comprise the foundations of machine learning.\n\nData mining\n\nis a related field of study, focusing on\n\nexploratory data analysis\n\n(EDA) via\n\nunsupervised learning\n\n.\n\n[ 4 ] [ 5 ]\n\nFrom a theoretical viewpoint,\n\nprobably approximately correct learning\n\nprovides a framework for describing machine learning.\n\nHistory\n\n[ edit ]\n\nSee also:\n\nTimeline of machine learning\n\nThe term\n\nmachine learning\n\nwas coined in 1959 by\n\nArthur Samuel\n\n, an\n\nIBM\n\nemployee and pioneer in the field of\n\ncomputer gaming\n\nand\n\nartificial intelligence\n\n.\n\n[ 6 ] [ 7 ]\n\nThe synonym\n\nself-teaching computers\n\nwas also used in this time period.\n\n[ 8 ] [ 9 ]\n\nThe earliest machine learning program was introduced in the 1950s when\n\nArthur Samuel\n\ninvented a\n\ncomputer program\n\nthat calculated the winning chance in checkers for each side, but the history of machine learning roots back to decades of human desire and effort to study human cognitive processes.\n\n[ 10 ]\n\nIn 1949,\n\nCanadian\n\npsychologist\n\nDonald Hebb\n\npublished the book\n\nThe Organization of Behavior\n\n, in which he introduced a\n\ntheoretical neural structure\n\nformed by certain interactions among\n\nnerve cells\n\n.\n\n[ 11 ] Hebb's model\n\nof\n\nneurons\n\ninteracting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or\n\nartificial neurons\n\nused by computers to communicate data.\n\n[ 10 ]\n\nOther researchers who have studied human\n\ncognitive systems\n\ncontributed to the modern machine learning technologies as well, including logician\n\nWalter Pitts\n\nand\n\nWarren McCulloch\n\n, who proposed the early mathematical models of neural networks to come up with\n\nalgorithms\n\nthat mirror human thought processes.\n\n[ 10 ]\n\nBy the early 1960s, an experimental \"learning machine\" with\n\npunched tape\n\nmemory, called Cybertron, had been developed by\n\nRaytheon Company\n\nto analyse\n\nsonar\n\nsignals,\n\nelectrocardiograms\n\n, and speech patterns using rudimentary\n\nreinforcement learning\n\n. It was repetitively \"trained\" by a human operator/teacher to recognise patterns and equipped with a \"\n\ngoof\n\n\" button to cause it to reevaluate incorrect decisions.\n\n[ 12 ]\n\nA representative book on research into machine learning during the 1960s was Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification.\n\n[ 13 ]\n\nInterest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973.\n\n[ 14 ]\n\nIn 1981 a report was given on using teaching strategies so that an\n\nartificial neural network\n\nlearns to recognise 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.\n\n[ 15 ] Tom M. Mitchell\n\nprovided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience\n\nE\n\nwith respect to some class of tasks\n\nT\n\nand performance measure\n\nP\n\nif its performance at tasks in\n\nT\n\n, as measured by\n\nP\n\n, improves with experience\n\nE\n\n.\"\n\n[ 16 ]\n\nThis definition of the tasks in which machine learning is concerned offers a fundamentally\n\noperational definition\n\nrather than defining the field in cognitive terms. This follows\n\nAlan Turing\n\n's proposal in his paper \"\n\nComputing Machinery and Intelligence\n\n\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entities) can do?\".\n\n[ 17 ]\n\nModern-day machine learning has two objectives. One is to classify data based on models which have been developed; the other purpose is to make predictions for future outcomes based on these models. A hypothetical algorithm specific to classifying data may use computer vision of moles coupled with supervised learning in order to train it to classify the cancerous moles. A machine learning algorithm for stock trading may inform the trader of future potential predictions.\n\n[ 18 ]\n\nRelationships to other fields\n\n[ edit ]\n\nArtificial intelligence\n\n[ edit ] Deep learning is a subset of machine learning, which is itself a subset of artificial intelligence . [ 19 ]\n\nAs a scientific endeavour, machine learning grew out of the quest for\n\nartificial intelligence\n\n(AI). In the early days of AI as an\n\nacademic discipline\n\n, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"\n\nneural networks\n\n\"; these were mostly\n\nperceptrons\n\nand\n\nother models\n\nthat were later found to be reinventions of the\n\ngeneralised linear models\n\nof statistics.\n\n[ 20 ] Probabilistic reasoning\n\nwas also employed, especially in\n\nautomated medical diagnosis\n\n.\n\n[ 21 ] : 488\n\nHowever, an increasing emphasis on the\n\nlogical, knowledge-based approach\n\ncaused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.\n\n[ 21 ] : 488\n\nBy 1980,\n\nexpert systems\n\nhad come to dominate AI, and statistics was out of favour.\n\n[ 22 ]\n\nWork on symbolic/knowledge-based learning did continue within AI, leading to\n\ninductive logic programming\n\n(ILP), but the more statistical line of research was now outside the field of AI proper, in\n\npattern recognition\n\nand\n\ninformation retrieval\n\n.\n\n[ 21 ] : 708–710, 755\n\nNeural networks research had been abandoned by AI and\n\ncomputer science\n\naround the same time. This line, too, was continued outside the AI/CS field, as \"\n\nconnectionism\n\n\", by researchers from other disciplines including\n\nJohn Hopfield\n\n,\n\nDavid Rumelhart\n\n, and\n\nGeoffrey Hinton\n\n. Their main success came in the mid-1980s with the reinvention of\n\nbackpropagation\n\n.\n\n[ 21 ] : 25\n\nMachine learning (ML), reorganised and recognised as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the\n\nsymbolic approaches\n\nit had inherited from AI, and toward methods and models borrowed from statistics,\n\nfuzzy logic\n\n, and\n\nprobability theory\n\n.\n\n[ 22 ]\n\nData compression\n\n[ edit ]\n\nThis section is an excerpt from\n\nData compression § Machine learning\n\n.\n\n[ edit ]\n\nThere is a close connection between machine learning and compression. A system that predicts the\n\nposterior probabilities\n\nof a sequence given its entire history can be used for optimal data compression (by using\n\narithmetic coding\n\non the output distribution). Conversely, an optimal compressor can be used for prediction (by finding the symbol that compresses best, given the previous history). This equivalence has been used as a justification for using data compression as a benchmark for \"general intelligence\".\n\n[ 23 ] [ 24 ] [ 25 ]\n\nAn alternative view can show compression algorithms implicitly map strings into implicit\n\nfeature space vectors\n\n, and compression-based similarity measures compute similarity within these feature spaces. For each compressor C(.) we define an associated vector space ℵ, such that C(.) maps an input string x, corresponding to the vector norm ||~x||. An exhaustive examination of the feature spaces underlying all compression algorithms is precluded by space; instead, feature vectors chooses to examine three representative lossless compression methods, LZW, LZ77, and PPM.\n\n[ 26 ]\n\nAccording to\n\nAIXI\n\ntheory, a connection more directly explained in\n\nHutter Prize\n\n, the best possible compression of x is the smallest possible software that generates x. For example, in that model, a zip file's compressed size includes both the zip file and the unzipping software, since you can not unzip it without both, but there may be an even smaller combined form.\n\nExamples of AI-powered audio/video compression software include\n\nNVIDIA Maxine\n\n, AIVC.\n\n[ 27 ]\n\nExamples of software that can perform AI-powered image compression include\n\nOpenCV\n\n,\n\nTensorFlow\n\n,\n\nMATLAB\n\n's Image Processing Toolbox (IPT) and High-Fidelity Generative Image Compression.\n\n[ 28 ]\n\nIn\n\nunsupervised machine learning\n\n,\n\nk-means clustering\n\ncan be utilized to compress data by grouping similar data points into clusters. This technique simplifies handling extensive datasets that lack predefined labels and finds widespread use in fields such as\n\nimage compression\n\n.\n\n[ 29 ]\n\nData compression aims to reduce the size of data files, enhancing storage efficiency and speeding up data transmission. K-means clustering, an unsupervised machine learning algorithm, is employed to partition a dataset into a specified number of clusters, k, each represented by the\n\ncentroid\n\nof its points. This process condenses extensive datasets into a more compact set of representative points. Particularly beneficial in\n\nimage\n\nand\n\nsignal processing\n\n, k-means clustering aids in data reduction by replacing groups of data points with their centroids, thereby preserving the core information of the original data while significantly decreasing the required storage space.\n\n[ 30 ] Large language models\n\n(LLMs) are also efficient lossless data compressors on some data sets, as demonstrated by\n\nDeepMind\n\n's research with the Chinchilla 70B model. Developed by DeepMind, Chinchilla 70B effectively compressed data, outperforming conventional methods such as\n\nPortable Network Graphics\n\n(PNG) for images and\n\nFree Lossless Audio Codec\n\n(FLAC) for audio. It achieved compression of image and audio data to 43.4% and 16.4% of their original sizes, respectively. There is, however, some reason to be concerned that the data set used for testing overlaps the LLM training data set, making it possible that the Chinchilla 70B model is only an efficient compression tool on data it has already been trained on.\n\n[ 31 ] [ 32 ]\n\nData mining\n\n[ edit ]\n\nMachine learning and\n\ndata mining\n\noften employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on\n\nknown\n\nproperties learned from the training data, data mining focuses on the\n\ndiscovery\n\nof (previously)\n\nunknown\n\nproperties in the data (this is the analysis step of\n\nknowledge discovery\n\nin databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"\n\nunsupervised learning\n\n\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals,\n\nECML PKDD\n\nbeing a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to\n\nreproduce known\n\nknowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously\n\nunknown\n\nknowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.\n\nMachine learning also has intimate ties to\n\noptimisation\n\n: Many learning problems are formulated as minimisation of some\n\nloss function\n\non a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a\n\nlabel\n\nto instances, and models are trained to correctly predict the preassigned labels of a set of examples).\n\n[ 33 ]\n\nGeneralization\n\n[ edit ]\n\nCharacterizing the generalisation of various learning algorithms is an active topic of current research, especially for\n\ndeep learning\n\nalgorithms.\n\nStatistics\n\n[ edit ]\n\nMachine learning and\n\nstatistics\n\nare closely related fields in terms of methods, but distinct in their principal goal: statistics draws population\n\ninferences\n\nfrom a\n\nsample\n\n, while machine learning finds generalisable predictive patterns.\n\n[ 34 ]\n\nAccording to\n\nMichael I. Jordan\n\n, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics.\n\n[ 35 ]\n\nHe also suggested the term\n\ndata science\n\nas a placeholder to call the overall field.\n\n[ 35 ]\n\nConventional statistical analyses require the a priori selection of a model most suitable for the study data set. In addition, only significant or theoretically relevant variables based on previous experience are included for analysis. In contrast, machine learning is not built on a pre-structured model; rather, the data shape the model by detecting underlying patterns. The more variables (input) used to train the model, the more accurate the ultimate model will be.\n\n[ 36 ] Leo Breiman\n\ndistinguished two statistical modelling paradigms: data model and algorithmic model,\n\n[ 37 ]\n\nwherein \"algorithmic model\" means more or less the machine learning algorithms like\n\nRandom Forest\n\n.\n\nSome statisticians have adopted methods from machine learning, leading to a combined field that they call\n\nstatistical learning\n\n.\n\n[ 38 ]\n\nStatistical physics\n\n[ edit ]\n\nAnalytical and computational techniques derived from deep-rooted physics of disordered systems can be extended to large-scale problems, including machine learning, e.g., to analyse the weight space of\n\ndeep neural networks\n\n.\n\n[ 39 ]\n\nStatistical physics is thus finding applications in the area of\n\nmedical diagnostics\n\n.\n\n[ 40 ]\n\nTheory\n\n[ edit ]\n\nMain articles:\n\nComputational learning theory\n\nand\n\nStatistical learning theory\n\nA core objective of a learner is to generalise from its experience.\n\n[ 3 ] [ 41 ]\n\nGeneralisation in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.\n\nThe computational analysis of machine learning algorithms and their performance is a branch of\n\ntheoretical computer science\n\nknown as\n\ncomputational learning theory\n\nvia the\n\nprobably approximately correct learning\n\nmodel. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The\n\nbias–variance decomposition\n\nis one way to quantify generalisation\n\nerror\n\n.\n\nFor the best performance in the context of generalisation, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has under fitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to\n\noverfitting\n\nand generalisation will be poorer.\n\n[ 42 ]\n\nIn addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in\n\npolynomial time\n\n. There are two kinds of\n\ntime complexity\n\nresults: Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.\n\nApproaches\n\n[ edit ] In supervised learning , the training data is labelled with the expected answers, while in unsupervised learning , the model identifies patterns or structures in unlabelled data.\n\nMachine learning approaches are traditionally divided into three broad categories, which correspond to learning paradigms, depending on the nature of the \"signal\" or \"feedback\" available to the learning system:\n\nSupervised learning\n\n: The computer is presented with example inputs and their desired outputs, given by a \"teacher\", and the goal is to learn a general rule that\n\nmaps\n\ninputs to outputs.\n\nUnsupervised learning\n\n: No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (\n\nfeature learning\n\n).\n\nReinforcement learning\n\n: A computer program interacts with a dynamic environment in which it must perform a certain goal (such as\n\ndriving a vehicle\n\nor playing a game against an opponent). As it navigates its problem space, the program is provided feedback that's analogous to rewards, which it tries to maximise.\n\n[ 3 ]\n\nAlthough each algorithm has advantages and limitations, no single algorithm works for all problems.\n\n[ 43 ] [ 44 ] [ 45 ]\n\nSupervised learning\n\n[ edit ]\n\nMain article:\n\nSupervised learning A support-vector machine is a supervised learning model that divides the data into regions separated by a linear boundary . Here, the linear boundary divides the black circles from the white.\n\nSupervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs.\n\n[ 46 ]\n\nThe data, known as\n\ntraining data\n\n, consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal. In the mathematical model, each training example is represented by an\n\narray\n\nor vector, sometimes called a\n\nfeature vector\n\n, and the training data is represented by a\n\nmatrix\n\n. Through\n\niterative optimisation\n\nof an\n\nobjective function\n\n, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs.\n\n[ 47 ]\n\nAn optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.\n\n[ 16 ]\n\nTypes of supervised-learning algorithms include\n\nactive learning\n\n,\n\nclassification\n\nand\n\nregression\n\n.\n\n[ 48 ]\n\nClassification algorithms are used when the outputs are restricted to a limited set of values, while regression algorithms are used when the outputs can take any numerical value within a range. For example, in a classification algorithm that filters emails, the input is an incoming email, and the output is the folder in which to file the email. In contrast, regression is used for tasks such as predicting a person's height based on factors like age and genetics or forecasting future temperatures based on historical data.\n\n[ 49 ] Similarity learning\n\nis an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in\n\nranking\n\n,\n\nrecommendation systems\n\n, visual identity tracking, face verification, and speaker verification.\n\nUnsupervised learning\n\n[ edit ]\n\nMain article:\n\nUnsupervised learning\n\nSee also:\n\nCluster analysis\n\nUnsupervised learning algorithms find structures in data that has not been labelled, classified or categorised. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Central applications of unsupervised machine learning include clustering,\n\ndimensionality reduction\n\n,\n\n[ 5 ]\n\nand\n\ndensity estimation\n\n.\n\n[ 50 ]\n\nCluster analysis is the assignment of a set of observations into subsets (called\n\nclusters\n\n) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some\n\nsimilarity metric\n\nand evaluated, for example, by\n\ninternal compactness\n\n, or the similarity between members of the same cluster, and\n\nseparation\n\n, the difference between clusters. Other methods are based on\n\nestimated density\n\nand\n\ngraph connectivity\n\n.\n\nA special type of unsupervised learning called,\n\nself-supervised learning\n\ninvolves training a model by generating the supervisory signal from the data itself.\n\n[ 51 ] [ 52 ]\n\nSemi-supervised learning\n\n[ edit ]\n\nMain article:\n\nSemi-supervised learning\n\nSemi-supervised learning falls between\n\nunsupervised learning\n\n(without any labelled training data) and\n\nsupervised learning\n\n(with completely labelled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabelled data, when used in conjunction with a small amount of labelled data, can produce a considerable improvement in learning accuracy.\n\nIn\n\nweakly supervised learning\n\n, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.\n\n[ 53 ]\n\nReinforcement learning\n\n[ edit ]\n\nMain article:\n\nReinforcement learning\n\nReinforcement learning is an area of machine learning concerned with how\n\nsoftware agents\n\nought to take\n\nactions\n\nin an environment so as to maximise some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as\n\ngame theory\n\n,\n\ncontrol theory\n\n,\n\noperations research\n\n,\n\ninformation theory\n\n,\n\nsimulation-based optimisation\n\n,\n\nmulti-agent systems\n\n,\n\nswarm intelligence\n\n,\n\nstatistics\n\nand\n\ngenetic algorithms\n\n. In reinforcement learning, the environment is typically represented as a\n\nMarkov decision process\n\n(MDP). Many reinforcement learning algorithms use\n\ndynamic programming\n\ntechniques.\n\n[ 54 ]\n\nReinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent.\n\nDimensionality reduction\n\n[ edit ] Dimensionality reduction\n\nis a process of reducing the number of random variables under consideration by obtaining a set of principal variables.\n\n[ 55 ]\n\nIn other words, it is a process of reducing the dimension of the\n\nfeature\n\nset, also called the \"number of features\". Most of the dimensionality reduction techniques can be considered as either feature elimination or\n\nextraction\n\n. One of the popular methods of dimensionality reduction is\n\nprincipal component analysis\n\n(PCA). PCA involves changing higher-dimensional data (e.g., 3D) to a smaller space (e.g., 2D).\n\nThe\n\nmanifold hypothesis\n\nproposes that high-dimensional data sets lie along low-dimensional\n\nmanifolds\n\n, and many dimensionality reduction techniques make this assumption, leading to the area of\n\nmanifold learning\n\nand\n\nmanifold regularisation\n\n.\n\nOther types\n\n[ edit ]\n\nOther approaches have been developed which do not fit neatly into this three-fold categorisation, and sometimes more than one is used by the same machine learning system. For example,\n\ntopic modelling\n\n,\n\nmeta-learning\n\n.\n\n[ 56 ]\n\nSelf-learning\n\n[ edit ]\n\nSelf-learning, as a machine learning paradigm was introduced in 1982 along with a neural network capable of self-learning, named\n\ncrossbar adaptive array\n\n(CAA).\n\n[ 57 ] [ 58 ]\n\nIt gives a solution to the problem learning without any external reward, by introducing emotion as an internal reward. Emotion is used as state evaluation of a self-learning agent. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.\n\n[ 59 ]\n\nThe self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine:\n\nin situation\n\ns\n\nperform action\n\na\n\nreceive a consequence situation\n\ns '\n\ncompute emotion of being in the consequence situation\n\nv(s')\n\nupdate crossbar memory\n\nw'(a,s) = w(a,s) + v(s')\n\nIt is a system with only one input, situation, and only one output, action (or behaviour) a. There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is the behavioural environment where it behaves, and the other is the genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioural environment. After receiving the genome (species) vector from the genetic environment, the CAA learns a goal-seeking behaviour, in an environment that contains both desirable and undesirable situations.\n\n[ 60 ]\n\nFeature learning\n\n[ edit ]\n\nMain article:\n\nFeature learning\n\nSeveral learning algorithms aim at discovering better representations of the inputs provided during training.\n\n[ 61 ]\n\nClassic examples include\n\nprincipal component analysis\n\nand cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual\n\nfeature engineering\n\n, and allows a machine to both learn the features and use them to perform a specific task.\n\nFeature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labelled input data. Examples include\n\nartificial neural networks\n\n,\n\nmultilayer perceptrons\n\n, and supervised\n\ndictionary learning\n\n. In unsupervised feature learning, features are learned with unlabelled input data. Examples include dictionary learning,\n\nindependent component analysis\n\n,\n\nautoencoders\n\n,\n\nmatrix factorisation [ 62 ]\n\nand various forms of\n\nclustering\n\n.\n\n[ 63 ] [ 64 ] [ 65 ] Manifold learning\n\nalgorithms attempt to do so under the constraint that the learned representation is low-dimensional.\n\nSparse coding\n\nalgorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros.\n\nMultilinear subspace learning\n\nalgorithms aim to learn low-dimensional representations directly from\n\ntensor\n\nrepresentations for multidimensional data, without reshaping them into higher-dimensional vectors.\n\n[ 66 ] Deep learning\n\nalgorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.\n\n[ 67 ]\n\nFeature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data has not yielded attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.\n\nSparse dictionary learning\n\n[ edit ]\n\nMain article:\n\nSparse dictionary learning\n\nSparse dictionary learning is a feature learning method where a training example is represented as a linear combination of\n\nbasis functions\n\nand assumed to be a\n\nsparse matrix\n\n. The method is\n\nstrongly NP-hard\n\nand difficult to solve approximately.\n\n[ 68 ]\n\nA popular\n\nheuristic\n\nmethod for sparse dictionary learning is the\n\nk -SVD\n\nalgorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in\n\nimage de-noising\n\n. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.\n\n[ 69 ]\n\nAnomaly detection\n\n[ edit ]\n\nMain article:\n\nAnomaly detection\n\nIn\n\ndata mining\n\n, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data.\n\n[ 70 ]\n\nTypically, the anomalous items represent an issue such as\n\nbank fraud\n\n, a structural defect, medical problems or errors in a text. Anomalies are referred to as\n\noutliers\n\n, novelties, noise, deviations and exceptions.\n\n[ 71 ]\n\nIn particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts of inactivity. This pattern does not adhere to the common statistical definition of an outlier as a rare object. Many outlier detection methods (in particular, unsupervised algorithms) will fail on such data unless aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns.\n\n[ 72 ]\n\nThree broad categories of anomaly detection techniques exist.\n\n[ 73 ]\n\nUnsupervised anomaly detection techniques detect anomalies in an unlabelled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit the least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labelled as \"normal\" and \"abnormal\" and involves training a classifier (the key difference from many other statistical classification problems is the inherently unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behaviour from a given normal training data set and then test the likelihood of a test instance to be generated by the model.\n\nRobot learning\n\n[ edit ] Robot learning\n\nis inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning,\n\n[ 74 ] [ 75 ]\n\nand finally\n\nmeta-learning\n\n(e.g. MAML).\n\nAssociation rules\n\n[ edit ]\n\nMain article:\n\nAssociation rule learning\n\nSee also:\n\nInductive logic programming\n\nAssociation rule learning is a\n\nrule-based machine learning\n\nmethod for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of \"interestingness\".\n\n[ 76 ]\n\nRule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves \"rules\" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilisation of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.\n\n[ 77 ]\n\nRule-based machine learning approaches include\n\nlearning classifier systems\n\n, association rule learning, and\n\nartificial immune systems\n\n.\n\nBased on the concept of strong rules,\n\nRakesh Agrawal\n\n,\n\nTomasz Imieliński\n\nand Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by\n\npoint-of-sale\n\n(POS) systems in supermarkets.\n\n[ 78 ]\n\nFor example, the rule\n\n{ o n i o n s , p o t a t o e s } ⇒ { b u r g e r } {\\displaystyle \\{\\mathrm {onions,potatoes} \\}\\Rightarrow \\{\\mathrm {burger} \\}}\n\nfound in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional\n\npricing\n\nor\n\nproduct placements\n\n. In addition to\n\nmarket basket analysis\n\n, association rules are employed today in application areas including\n\nWeb usage mining\n\n,\n\nintrusion detection\n\n,\n\ncontinuous production\n\n, and\n\nbioinformatics\n\n. In contrast with\n\nsequence mining\n\n, association rule learning typically does not consider the order of items either within a transaction or across transactions.\n\nLearning classifier systems\n\n(LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a\n\ngenetic algorithm\n\n, with a learning component, performing either\n\nsupervised learning\n\n,\n\nreinforcement learning\n\n, or\n\nunsupervised learning\n\n. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a\n\npiecewise\n\nmanner in order to make predictions.\n\n[ 79 ] Inductive logic programming\n\n(ILP) is an approach to rule learning using\n\nlogic programming\n\nas a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that\n\nentails\n\nall positive and no negative examples.\n\nInductive programming\n\nis a related field that considers any kind of programming language for representing hypotheses (and not only logic programming), such as\n\nfunctional programs\n\n.\n\nInductive logic programming is particularly useful in\n\nbioinformatics\n\nand\n\nnatural language processing\n\n.\n\nGordon Plotkin\n\nand\n\nEhud Shapiro\n\nlaid the initial theoretical foundation for inductive machine learning in a logical setting.\n\n[ 80 ] [ 81 ] [ 82 ]\n\nShapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples.\n\n[ 83 ]\n\nThe term\n\ninductive\n\nhere refers to\n\nphilosophical\n\ninduction, suggesting a theory to explain observed facts, rather than\n\nmathematical induction\n\n, proving a property for all members of a well-ordered set.\n\nModels\n\n[ edit ]\n\nA\n\nmachine learning model\n\nis a type of\n\nmathematical model\n\nthat, once \"trained\" on a given dataset, can be used to make predictions or classifications on new data. During training, a learning algorithm iteratively adjusts the model's internal parameters to minimise errors in its predictions.\n\n[ 84 ]\n\nBy extension, the term \"model\" can refer to several levels of specificity, from a general class of models and their associated learning algorithms to a fully trained model with all its internal parameters tuned.\n\n[ 85 ]\n\nVarious types of models have been used and researched for machine learning systems, picking the best model for a task is called\n\nmodel selection\n\n.\n\nArtificial neural networks\n\n[ edit ]\n\nMain article:\n\nArtificial neural network\n\nSee also:\n\nDeep learning An artificial neural network is an interconnected group of nodes, akin to the vast network of neurons in a brain . Here, each circular node represents an artificial neuron and an arrow represents a connection from the output of one artificial neuron to the input of another.\n\nArtificial neural networks (ANNs), or\n\nconnectionist\n\nsystems, are computing systems vaguely inspired by the\n\nbiological neural networks\n\nthat constitute animal\n\nbrains\n\n. Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with any task-specific rules.\n\nAn ANN is a model based on a collection of connected units or nodes called \"\n\nartificial neurons\n\n\", which loosely model the\n\nneurons\n\nin a biological brain. Each connection, like the\n\nsynapses\n\nin a biological brain, can transmit information, a \"signal\", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a\n\nreal number\n\n, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called \"edges\". Artificial neurons and edges typically have a\n\nweight\n\nthat adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times.\n\nThe original goal of the ANN approach was to solve problems in the same way that a\n\nhuman brain\n\nwould. However, over time, attention moved to performing specific tasks, leading to deviations from\n\nbiology\n\n. Artificial neural networks have been used on a variety of tasks, including\n\ncomputer vision\n\n,\n\nspeech recognition\n\n,\n\nmachine translation\n\n,\n\nsocial network\n\nfiltering,\n\nplaying board and video games\n\nand\n\nmedical diagnosis\n\n.\n\nDeep learning\n\nconsists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.\n\n[ 86 ]\n\nDecision trees\n\n[ edit ]\n\nMain article:\n\nDecision tree learning A decision tree showing survival probability of passengers on the Titanic\n\nDecision tree learning uses a\n\ndecision tree\n\nas a\n\npredictive model\n\nto go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modelling approaches used in statistics, data mining, and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures,\n\nleaves\n\nrepresent class labels, and branches represent\n\nconjunctions\n\nof features that lead to those class labels. Decision trees where the target variable can take continuous values (typically\n\nreal numbers\n\n) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and\n\ndecision making\n\n. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision-making.\n\nRandom forest regression\n\n[ edit ]\n\nRandom forest regression (RFR) falls under umbrella of decision\n\ntree-based models\n\n. RFR is an ensemble learning method that builds multiple decision trees and averages their predictions to improve accuracy and to avoid overfitting. To build decision trees, RFR uses bootstrapped sampling, for instance each decision tree is trained on random data of from training set. This random selection of RFR for training enables model to reduce bias predictions and achieve accuracy. RFR generates independent decision trees, and it can work on single output data as well multiple regressor task. This makes RFR compatible to be used in various application.\n\n[ 87 ] [ 88 ]\n\nSupport-vector machines\n\n[ edit ]\n\nMain article:\n\nSupport-vector machine\n\nSupport-vector machines (SVMs), also known as support-vector networks, are a set of related\n\nsupervised learning\n\nmethods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category.\n\n[ 89 ]\n\nAn SVM training algorithm is a non-\n\nprobabilistic\n\n,\n\nbinary\n\n,\n\nlinear classifier\n\n, although methods such as\n\nPlatt scaling\n\nexist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the\n\nkernel trick\n\n, implicitly mapping their inputs into high-dimensional feature spaces.\n\nRegression analysis\n\n[ edit ]\n\nMain article:\n\nRegression analysis Illustration of linear regression on a data set\n\nRegression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. Its most common form is\n\nlinear regression\n\n, where a single line is drawn to best fit the given data according to a mathematical criterion such as\n\nordinary least squares\n\n. The latter is often extended by\n\nregularisation\n\nmethods to mitigate overfitting and bias, as in\n\nridge regression\n\n. When dealing with non-linear problems, go-to models include\n\npolynomial regression\n\n(for example, used for trendline fitting in Microsoft Excel\n\n[ 90 ]\n\n),\n\nlogistic regression\n\n(often used in\n\nstatistical classification\n\n) or even\n\nkernel regression\n\n, which introduces non-linearity by taking advantage of the\n\nkernel trick\n\nto implicitly map input variables to higher-dimensional space.\n\nMultivariate linear regression\n\nextends the concept of linear regression to handle multiple dependent variables simultaneously. This approach estimates the relationships between a set of input variables and several output variables by fitting a\n\nmultidimensional\n\nlinear model. It is particularly useful in scenarios where outputs are interdependent or share underlying patterns, such as predicting multiple economic indicators or reconstructing images,\n\n[ 91 ]\n\nwhich are inherently multi-dimensional.\n\nBayesian networks\n\n[ edit ]\n\nMain article:\n\nBayesian network A simple Bayesian network. Rain influences whether the sprinkler is activated, and both rain and the sprinkler influence whether the grass is wet.\n\nA Bayesian network, belief network, or directed acyclic graphical model is a probabilistic\n\ngraphical model\n\nthat represents a set of\n\nrandom variables\n\nand their\n\nconditional independence\n\nwith a\n\ndirected acyclic graph\n\n(DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform\n\ninference\n\nand learning. Bayesian networks that model sequences of variables, like\n\nspeech signals\n\nor\n\nprotein sequences\n\n, are called\n\ndynamic Bayesian networks\n\n. Generalisations of Bayesian networks that can represent and solve decision problems under uncertainty are called\n\ninfluence diagrams\n\n.\n\nGaussian processes\n\n[ edit ]\n\nMain article:\n\nGaussian processes An example of Gaussian Process Regression (prediction) compared with other regression models [ 92 ]\n\nA Gaussian process is a\n\nstochastic process\n\nin which every finite collection of the random variables in the process has a\n\nmultivariate normal distribution\n\n, and it relies on a pre-defined\n\ncovariance function\n\n, or kernel, that models how pairs of points relate to each other depending on their locations.\n\nGiven a set of observed points, or input–output examples, the distribution of the (unobserved) output of a new point as function of its input data can be directly computed by looking like the observed points and the covariances between those points and the new, unobserved point.\n\nGaussian processes are popular surrogate models in\n\nBayesian optimisation\n\nused to do\n\nhyperparameter optimisation\n\n.\n\nGenetic algorithms\n\n[ edit ]\n\nMain article:\n\nGenetic algorithm\n\nA genetic algorithm (GA) is a\n\nsearch algorithm\n\nand\n\nheuristic\n\ntechnique that mimics the process of\n\nnatural selection\n\n, using methods such as\n\nmutation\n\nand\n\ncrossover\n\nto generate new\n\ngenotypes\n\nin the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s.\n\n[ 93 ] [ 94 ]\n\nConversely, machine learning techniques have been used to improve the performance of genetic and\n\nevolutionary algorithms\n\n.\n\n[ 95 ]\n\nBelief functions\n\n[ edit ]\n\nMain article:\n\nDempster–Shafer theory\n\nThe theory of belief functions, also referred to as evidence theory or Dempster–Shafer theory, is a general framework for reasoning with uncertainty, with understood connections to other frameworks such as\n\nprobability\n\n,\n\npossibility\n\nand\n\nimprecise probability theories\n\n. These theoretical frameworks can be thought of as a kind of learner and have some analogous properties of how evidence is combined (e.g., Dempster's rule of combination), just like how in a\n\npmf\n\n-based Bayesian approach would combine probabilities.\n\n[ 96 ]\n\nHowever, there are many caveats to these beliefs functions when compared to Bayesian approaches in order to incorporate ignorance and\n\nuncertainty quantification\n\n. These belief function approaches that are implemented within the machine learning domain typically leverage a fusion approach of various\n\nensemble methods\n\nto better handle the learner's\n\ndecision boundary\n\n, low samples, and ambiguous class issues that standard machine learning approach tend to have difficulty resolving.\n\n[ 97 ] [ 7 ]\n\nHowever, the computational complexity of these algorithms are dependent on the number of propositions (classes), and can lead to a much higher computation time when compared to other machine learning approaches.\n\nRule-based models\n\n[ edit ]\n\nMain article:\n\nRule-based machine learning\n\nRule-based machine learning (RBML) is a branch of machine learning that automatically discovers and learns 'rules' from data. It provides interpretable models, making it useful for decision-making in fields like healthcare, fraud detection, and cybersecurity. Key RBML techniques includes\n\nlearning classifier systems\n\n,\n\n[ 98 ] association rule learning\n\n,\n\n[ 99 ] artificial immune systems\n\n,\n\n[ 100 ]\n\nand other similar models. These methods extract patterns from data and evolve rules over time.\n\nTraining models\n\n[ edit ]\n\nTypically, machine learning models require a high quantity of reliable data to perform accurate predictions. When training a machine learning model, machine learning engineers need to target and collect a large and representative\n\nsample\n\nof data. Data from the training set can be as varied as a\n\ncorpus of text\n\n, a collection of images,\n\nsensor\n\ndata, and data collected from individual users of a service.\n\nOverfitting\n\nis something to watch out for when training a machine learning model. Trained models derived from biased or non-evaluated data can result in skewed or undesired predictions. Biased models may result in detrimental outcomes, thereby furthering the negative impacts on society or objectives.\n\nAlgorithmic bias\n\nis a potential result of data not being fully prepared for training. Machine learning ethics is becoming a field of study and notably, becoming integrated within machine learning engineering teams.\n\nFederated learning\n\n[ edit ]\n\nMain article:\n\nFederated learning\n\nFederated learning is an adapted form of\n\ndistributed artificial intelligence\n\nto training machine learning models that decentralises the training process, allowing for users' privacy to be maintained by not needing to send their data to a centralised server. This also increases efficiency by decentralising the training process to many devices. For example,\n\nGboard\n\nuses federated machine learning to train search query prediction models on users' mobile phones without having to send individual searches back to\n\nGoogle\n\n.\n\n[ 101 ]\n\nApplications\n\n[ edit ]\n\nThere are many applications for machine learning, including:\n\nAgriculture Anatomy Adaptive website Affective computing Astronomy Automated decision-making Banking Behaviorism Bioinformatics Brain–machine interfaces Cheminformatics Citizen Science Climate Science Computer networks Computer vision Credit-card fraud\n\ndetection\n\nData quality DNA sequence\n\nclassification\n\nEconomics Financial market\n\nanalysis\n\n[ 102 ] General game playing Handwriting recognition Healthcare Information retrieval Insurance Internet fraud\n\ndetection\n\nKnowledge graph embedding Linguistics Machine learning control Machine perception Machine translation Material Engineering Marketing Medical diagnosis Natural language processing Natural language understanding Online advertising Optimisation Recommender systems Robot locomotion Search engines Sentiment analysis Sequence mining Software engineering Speech recognition Structural health monitoring Syntactic pattern recognition Telecommunications Theorem proving Time-series forecasting Tomographic reconstruction [ 103 ] User behaviour analytics\n\nIn 2006, the media-services provider\n\nNetflix\n\nheld the first \"\n\nNetflix Prize\n\n\" competition to find a program to better predict user preferences and improve the accuracy of its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from\n\nAT\u0026T Labs\n\n-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an\n\nensemble model\n\nto win the Grand Prize in 2009 for $1 million.\n\n[ 104 ]\n\nShortly after the prize was awarded, Netflix realised that viewers' ratings were not the best indicators of their viewing patterns (\"everything is a recommendation\") and they changed their recommendation engine accordingly.\n\n[ 105 ]\n\nIn 2010, an article in\n\nThe Wall Street Journal\n\nnoted the use of machine learning by Rebellion Research to predict the\n\n2008 financial crisis\n\n.\n\n[ 106 ]\n\nIn 2012, co-founder of\n\nSun Microsystems\n\n,\n\nVinod Khosla\n\n, predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software.\n\n[ 107 ]\n\nIn 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings and that it may have revealed previously unrecognised influences among artists.\n\n[ 108 ]\n\nIn 2019\n\nSpringer Nature\n\npublished the first research book created using machine learning.\n\n[ 109 ]\n\nIn 2020, machine learning technology was used to help make diagnoses and aid researchers in developing a cure for COVID-19.\n\n[ 110 ]\n\nMachine learning was recently applied to predict the pro-environmental behaviour of travellers.\n\n[ 111 ]\n\nRecently, machine learning technology was also applied to optimise smartphone's performance and thermal behaviour based on the user's interaction with the phone.\n\n[ 112 ] [ 113 ] [ 114 ]\n\nWhen applied correctly, machine learning algorithms (MLAs) can utilise a wide range of company characteristics to predict stock returns without\n\noverfitting\n\n. By employing effective feature engineering and combining forecasts, MLAs can generate results that far surpass those obtained from basic linear techniques like\n\nOLS\n\n.\n\n[ 115 ]\n\nRecent advancements in machine learning have extended into the field of quantum chemistry, where novel algorithms now enable the prediction of solvent effects on chemical reactions, thereby offering new tools for chemists to tailor experimental conditions for optimal outcomes.\n\n[ 116 ]\n\nMachine Learning is becoming a useful tool to investigate and predict evacuation decision making in large scale and small scale disasters. Different solutions have been tested to predict if and when householders decide to evacuate during wildfires and hurricanes.\n\n[ 117 ] [ 118 ] [ 119 ]\n\nOther applications have been focusing on pre evacuation decisions in building fires.\n\n[ 120 ] [ 121 ]\n\nMachine learning is also emerging as a promising tool in geotechnical engineering, where it is used to support tasks such as ground classification, hazard prediction, and site characterization. Recent research emphasizes a move toward data-centric methods in this field, where machine learning is not a replacement for engineering judgment, but a way to enhance it using site-specific data and patterns.\n\n[ 122 ]\n\nLimitations\n\n[ edit ]\n\nAlthough machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results.\n\n[ 123 ] [ 124 ] [ 125 ]\n\nReasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.\n\n[ 126 ]\n\nThe \"\n\nblack box theory\n\n\" poses another yet significant challenge. Black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine extracted out of the data.\n\n[ 127 ]\n\nThe House of Lords Select Committee, which claimed that such an \"intelligence system\" that could have a \"substantial impact on an individual's life\" would not be considered acceptable unless it provided \"a full and satisfactory explanation for the decisions\" it makes.\n\n[ 127 ]\n\nIn 2018, a self-driving car from\n\nUber\n\nfailed to detect a pedestrian, who was killed after a collision.\n\n[ 128 ]\n\nAttempts to use machine learning in healthcare with the\n\nIBM Watson\n\nsystem failed to deliver even after years of time and billions of dollars invested.\n\n[ 129 ] [ 130 ]\n\nMicrosoft's\n\nBing Chat\n\nchatbot has been reported to produce hostile and offensive response against its users.\n\n[ 131 ]\n\nMachine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature. While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research themselves.\n\n[ 132 ]\n\nExplainability\n\n[ edit ]\n\nMain article:\n\nExplainable artificial intelligence\n\nExplainable AI (XAI), or Interpretable AI, or Explainable Machine Learning (XML), is artificial intelligence (AI) in which humans can understand the decisions or predictions made by the AI.\n\n[ 133 ]\n\nIt contrasts with the \"black box\" concept in machine learning where even its designers cannot explain why an AI arrived at a specific decision.\n\n[ 134 ]\n\nBy refining the mental models of users of AI-powered systems and dismantling their misconceptions, XAI promises to help users perform more effectively. XAI may be an implementation of the social right to explanation.\n\nOverfitting\n\n[ edit ]\n\nMain article:\n\nOverfitting The blue line could be an example of overfitting a linear function due to random noise.\n\nSettling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalising the theory in accordance with how complex the theory is.\n\n[ 135 ]\n\nOther limitations and vulnerabilities\n\n[ edit ]\n\nLearners can also disappoint by \"learning the wrong lesson\". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses.\n\n[ 136 ]\n\nA real-world example is that, unlike humans, current image classifiers often do not primarily make judgements from the spatial relationship between components of the picture, and they learn relationships between pixels that humans are oblivious to, but that still correlate with images of certain types of real objects. Modifying these patterns on a legitimate image can result in \"adversarial\" images that the system misclassifies.\n\n[ 137 ] [ 138 ]\n\nAdversarial vulnerabilities can also result in nonlinear systems, or from non-pattern perturbations. For some systems, it is possible to change the output by only changing a single adversarially chosen pixel.\n\n[ 139 ]\n\nMachine learning models are often vulnerable to manipulation or evasion via\n\nadversarial machine learning\n\n.\n\n[ 140 ]\n\nResearchers have demonstrated how\n\nbackdoors\n\ncan be placed undetectably into classifying (e.g., for categories \"spam\" and well-visible \"not spam\" of posts) machine learning models that are often developed or trained by third parties. Parties can change the classification of any input, including in cases for which a type of\n\ndata/software transparency\n\nis provided, possibly including\n\nwhite-box access\n\n.\n\n[ 141 ] [ 142 ] [ 143 ]\n\nModel assessments\n\n[ edit ]\n\nClassification of machine learning models can be validated by accuracy estimation techniques like the\n\nholdout\n\nmethod, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-\n\ncross-validation\n\nmethod randomly partitions the data into K subsets and then K experiments are performed each respectively considering 1 subset for evaluation and the remaining K-1 subsets for training the model. In addition to the holdout and cross-validation methods,\n\nbootstrap\n\n, which samples n instances with replacement from the dataset, can be used to assess model accuracy.\n\n[ 144 ]\n\nIn addition to overall accuracy, investigators frequently report\n\nsensitivity and specificity\n\nmeaning true positive rate (TPR) and true negative rate (TNR) respectively. Similarly, investigators sometimes report the\n\nfalse positive rate\n\n(FPR) as well as the\n\nfalse negative rate\n\n(FNR). However, these rates are ratios that fail to reveal their numerators and denominators.\n\nReceiver operating characteristic\n\n(ROC) along with the accompanying Area Under the ROC Curve (AUC) offer additional tools for classification model assessment. Higher AUC is associated with a better performing model.\n\n[ 145 ]\n\nEthics\n\n[ edit ]\n\nThis section is an excerpt from\n\nEthics of artificial intelligence\n\n.\n\n[ edit ]\n\nThe\n\nethics\n\nof\n\nartificial intelligence\n\ncovers a broad range of topics within AI that are considered to have particular ethical stakes.\n\n[ 146 ]\n\nThis includes\n\nalgorithmic biases\n\n,\n\nfairness\n\n,\n\n[ 147 ] automated decision-making\n\n,\n\n[ 148 ] accountability\n\n,\n\nprivacy\n\n, and\n\nregulation\n\n. It also covers various emerging or potential future challenges such as\n\nmachine ethics\n\n(how to make machines that behave ethically),\n\nlethal autonomous weapon systems\n\n,\n\narms race\n\ndynamics,\n\nAI safety\n\nand\n\nalignment\n\n,\n\ntechnological unemployment\n\n, AI-enabled\n\nmisinformation\n\n,\n\n[ 149 ]\n\nhow to treat certain AI systems if they have a\n\nmoral status\n\n(AI welfare and rights),\n\nartificial superintelligence\n\nand\n\nexistential risks\n\n.\n\n[ 146 ]\n\nSome application areas may also have particularly important ethical implications, like\n\nhealthcare\n\n, education, criminal justice, or the military.\n\nBias\n\n[ edit ]\n\nMain article:\n\nAlgorithmic bias\n\nDifferent machine learning approaches can suffer from different data biases. A machine learning system trained specifically on current customers may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society.\n\n[ 150 ]\n\nSystems that are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitising cultural prejudices.\n\n[ 151 ]\n\nFor example, in 1988, the UK's\n\nCommission for Racial Equality\n\nfound that\n\nSt. George's Medical School\n\nhad been using a computer program trained from data of previous admissions staff and that this program had denied nearly 60 candidates who were found to either be women or have non-European sounding names.\n\n[ 150 ]\n\nUsing job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants by similarity to previous successful applicants.\n\n[ 152 ] [ 153 ]\n\nAnother example includes predictive policing company\n\nGeolitica\n\n's predictive algorithm that resulted in \"disproportionately high levels of over-policing in low-income and minority communities\" after being trained with historical crime data.\n\n[ 154 ]\n\nWhile responsible\n\ncollection of data\n\nand documentation of algorithmic rules used by a system is considered a critical part of machine learning, some researchers blame lack of participation and representation of minority population in the field of AI for machine learning's vulnerability to biases.\n\n[ 155 ]\n\nIn fact, according to research carried out by the Computing Research Association (CRA) in 2021, \"female faculty merely make up 16.1%\" of all faculty members who focus on AI among several universities around the world.\n\n[ 156 ]\n\nFurthermore, among the group of \"new U.S. resident AI PhD graduates,\" 45% identified as white, 22.4% as Asian, 3.2% as Hispanic, and 2.4% as African American, which further demonstrates a lack of diversity in the field of AI.\n\n[ 156 ]\n\nLanguage models learned from data have been shown to contain human-like biases.\n\n[ 157 ] [ 158 ]\n\nBecause human languages contain biases, machines trained on language\n\ncorpora\n\nwill necessarily also learn these biases.\n\n[ 159 ] [ 160 ]\n\nIn 2016, Microsoft tested\n\nTay\n\n, a\n\nchatbot\n\nthat learned from Twitter, and it quickly picked up racist and sexist language.\n\n[ 161 ]\n\nIn an experiment carried out by\n\nProPublica\n\n, an\n\ninvestigative journalism\n\norganisation, a machine learning algorithm's insight into the recidivism rates among prisoners falsely flagged \"black defendants high risk twice as often as white defendants\".\n\n[ 154 ]\n\nIn 2015, Google Photos once tagged a couple of black people as gorillas, which caused controversy. The gorilla label was subsequently removed, and in 2023, it still cannot recognise gorillas.\n\n[ 162 ]\n\nSimilar issues with recognising non-white people have been found in many other systems.\n\n[ 163 ]\n\nBecause of such challenges, the effective use of machine learning may take longer to be adopted in other domains.\n\n[ 164 ]\n\nConcern for\n\nfairness\n\nin machine learning, that is, reducing bias in machine learning and propelling its use for human good, is increasingly expressed by artificial intelligence scientists, including\n\nFei-Fei Li\n\n, who said that \"[t]here's nothing artificial about AI. It's inspired by people, it's created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\"\n\n[ 165 ]\n\nFinancial incentives\n\n[ edit ]\n\nThere are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines. This is especially true in the United States where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is potential for machine learning in health care to provide professionals an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated.\n\n[ 166 ]\n\nHardware\n\n[ edit ]\n\nSince the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training\n\ndeep neural networks\n\n(a particular narrow subdomain of machine learning) that contain many layers of nonlinear hidden units.\n\n[ 167 ]\n\nBy 2019, graphics processing units (\n\nGPUs\n\n), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI.\n\n[ 168 ] OpenAI\n\nestimated the hardware compute used in the largest deep learning projects from\n\nAlexNet\n\n(2012) to\n\nAlphaZero\n\n(2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months.\n\n[ 169 ] [ 170 ]\n\nTensor Processing Units (TPUs)\n\n[ edit ] Tensor Processing Units (TPUs)\n\nare specialised hardware accelerators developed by\n\nGoogle\n\nspecifically for machine learning workloads. Unlike general-purpose\n\nGPUs\n\nand\n\nFPGAs\n\n, TPUs are optimised for tensor computations, making them particularly efficient for deep learning tasks such as training and inference. They are widely used in Google Cloud AI services and large-scale machine learning models like Google's DeepMind AlphaFold and large language models. TPUs leverage matrix multiplication units and high-bandwidth memory to accelerate computations while maintaining energy efficiency.\n\n[ 171 ]\n\nSince their introduction in 2016, TPUs have become a key component of AI infrastructure, especially in cloud-based environments.\n\nNeuromorphic computing\n\n[ edit ] Neuromorphic computing\n\nrefers to a class of computing systems designed to emulate the structure and functionality of biological neural networks. These systems may be implemented through software-based simulations on conventional hardware or through specialised hardware architectures.\n\n[ 172 ]\n\nPhysical neural networks\n\n[ edit ]\n\nA\n\nphysical neural network\n\nis a specific type of neuromorphic hardware that relies on electrically adjustable materials, such as memristors, to emulate the function of\n\nneural synapses\n\n. The term \"physical neural network\" highlights the use of physical hardware for computation, as opposed to software-based implementations. It broadly refers to artificial neural networks that use materials with adjustable resistance to replicate neural synapses.\n\n[ 173 ] [ 174 ]\n\nEmbedded machine learning\n\n[ edit ]\n\nEmbedded machine learning is a sub-field of machine learning where models are deployed on\n\nembedded systems\n\nwith limited computing resources, such as\n\nwearable computers\n\n,\n\nedge devices\n\nand\n\nmicrocontrollers\n\n.\n\n[ 175 ] [ 176 ] [ 177 ] [ 178 ]\n\nRunning models directly on these devices eliminates the need to transfer and store data on cloud servers for further processing, thereby reducing the risk of data breaches, privacy leaks and theft of intellectual property, personal data and business secrets. Embedded machine learning can be achieved through various techniques, such as\n\nhardware acceleration\n\n,\n\n[ 179 ] [ 180 ] approximate computing\n\n,\n\n[ 181 ]\n\nand model optimisation.\n\n[ 182 ] [ 183 ]\n\nCommon optimisation techniques include\n\npruning\n\n,\n\nquantization\n\n,\n\nknowledge distillation\n\n, low-rank factorisation, network architecture search, and parameter sharing.\n\nSoftware\n\n[ edit ] Software suites\n\ncontaining a variety of machine learning algorithms include the following:\n\nFree and open-source software\n\n[ edit ]\n\nSee also:\n\nLists of open-source artificial intelligence software Caffe Deeplearning4j DeepSpeed ELKI Google JAX Infer.NET Jubatus Keras Kubeflow LightGBM Mahout Mallet Microsoft Cognitive Toolkit ML.NET mlpack MXNet OpenNN Orange pandas (software) ROOT\n\n(TMVA with ROOT)\n\nscikit-learn Shogun Spark MLlib SystemML Theano TensorFlow Torch\n\n/\n\nPyTorch Weka\n\n/\n\nMOA XGBoost Yooreeka\n\nProprietary software with free and open-source editions\n\n[ edit ] KNIME RapidMiner\n\nProprietary software\n\n[ edit ] Amazon Machine Learning Angoss\n\nKnowledgeSTUDIO\n\nAzure Machine Learning IBM Watson Studio Google Cloud Vertex AI Google Prediction API IBM SPSS Modeller KXEN Modeller LIONsolver Mathematica MATLAB Neural Designer NeuroSolutions Oracle Data Mining Oracle AI Platform Cloud Service PolyAnalyst RCASE SAS Enterprise Miner SequenceL Splunk STATISTICA\n\nData Miner\n\nJournals\n\n[ edit ] Journal of Machine Learning Research Machine Learning Nature Machine Intelligence Neural Computation IEEE Transactions on Pattern Analysis and Machine Intelligence\n\nConferences\n\n[ edit ] AAAI Conference on Artificial Intelligence Association for Computational Linguistics ( ACL ) European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases ( ECML PKDD ) International Conference on Computational Intelligence Methods for Bioinformatics and Biostatistics ( CIBB ) International Conference on Machine Learning ( ICML ) International Conference on Learning Representations ( ICLR ) International Conference on Intelligent Robots and Systems ( IROS ) Conference on Knowledge Discovery and Data Mining ( KDD ) Conference on Neural Information Processing Systems ( NeurIPS )\n\nSee also\n\n[ edit ] Automated machine learning\n\n– Process of automating the application of machine learning\n\nBig data\n\n– Extremely large or complex datasets\n\nDeep learning\n\n— branch of ML concerned with\n\nartificial neural networks Differentiable programming\n\n– Programming paradigm\n\nList of datasets for machine-learning research M-theory (learning framework) Machine unlearning Solomonoff's theory of inductive inference\n\n– Mathematical theory\n\nReferences\n\n[ edit ] ^ The definition \"without being explicitly programmed\" is often attributed to Arthur Samuel , who coined the term \"machine learning\" in 1959, but the phrase is not found verbatim in this publication, and may be a paraphrase that appeared later. Confer \"Paraphrasing Arthur Samuel (1959), the question is: How can computers learn to solve problems without being explicitly programmed?\" in Koza, John R.; Bennett, Forrest H.; Andre, David; Keane, Martin A. (1996). \"Automated Design of Both the Topology and Sizing of Analog Electrical Circuits Using Genetic Programming\". Artificial Intelligence in Design '96 . Artificial Intelligence in Design '96. Dordrecht, Netherlands: Springer Netherlands. pp. 151– 170. doi : 10.1007/978-94-009-0279-4_9 . ISBN 978-94-010-6610-5 . ^ \"What is Machine Learning?\" . IBM . 22 September 2021. Archived from the original on 27 December 2023 . Retrieved 27 June 2023 . ^ a b c Bishop, C. M. (2006), Pattern Recognition and Machine Learning , Springer, ISBN 978-0-387-31073-2 ^ Machine learning and pattern recognition \"can be viewed as two facets of the same field\". [ 3 ] : vii ^ a b Friedman, Jerome H. (1998). \"Data Mining and Statistics: What's the connection?\". Computing Science and Statistics . 29 (1): 3– 9. ^ Samuel, Arthur (1959). \"Some Studies in Machine Learning Using the Game of Checkers\". IBM Journal of Research and Development . 3 (3): 210– 229. CiteSeerX 10.1.1.368.2254 . doi : 10.1147/rd.33.0210 . S2CID 2126705 . ^ a b R. Kohavi and F. Provost, \"Glossary of terms\", Machine Learning, vol. 30, no. 2–3, pp. 271–274, 1998. ^ Gerovitch, Slava (9 April 2015). \"How the Computer Got Its Revenge on the Soviet Union\" . Nautilus . Archived from the original on 22 September 2021 . Retrieved 19 September 2021 . ^ Lindsay, Richard P. (1 September 1964). \"The Impact of Automation On Public Administration\" . Western Political Quarterly . 17 (3): 78– 81. doi : 10.1177/106591296401700364 . ISSN 0043-4078 . S2CID 154021253 . Archived from the original on 6 October 2021 . Retrieved 6 October 2021 . ^ a b c \"History and Evolution of Machine Learning: A Timeline\" . WhatIs . Archived from the original on 8 December 2023 . Retrieved 8 December 2023 . ^ Milner, Peter M. (1993). \"The Mind and Donald O. Hebb\" . Scientific American . 268 (1): 124– 129. Bibcode : 1993SciAm.268a.124M . doi : 10.1038/scientificamerican0193-124 . ISSN 0036-8733 . JSTOR 24941344 . PMID 8418480 . Archived from the original on 20 December 2023 . Retrieved 9 December 2023 . ^ \"Science: The Goof Button\", Time , 18 August 1961. ^ Nilsson N. Learning Machines, McGraw Hill, 1965. ^ Duda, R., Hart P. Pattern Recognition and Scene Analysis, Wiley Interscience, 1973 ^ S. Bozinovski \"Teaching space: A representation concept for adaptive pattern classification\" COINS Technical Report No. 81-28, Computer and Information Science Department, University of Massachusetts at Amherst, MA, 1981. https://web.cs.umass.edu/publication/docs/1981/UM-CS-1981-028.pdf Archived 25 February 2021 at the Wayback Machine ^ a b Mitchell, T. (1997). Machine Learning . McGraw Hill. p. 2. ISBN 978-0-07-042807-2 . ^ Harnad, Stevan (2008), \"The Annotation Game: On Turing (1950) on Computing, Machinery, and Intelligence\" , in Epstein, Robert; Peters, Grace (eds.), The Turing Test Sourcebook: Philosophical and Methodological Issues in the Quest for the Thinking Computer , Kluwer, pp. 23– 66, ISBN 9781402067082 , archived from the original on 9 March 2012 , retrieved 11 December 2012 ^ \"Introduction to AI Part 1\" . Edzion . 8 December 2020. Archived from the original on 18 February 2021 . Retrieved 9 December 2020 . ^ Sindhu V, Nivedha S, Prakash M (February 2020). \"An Empirical Science Research on Bioinformatics in Machine Learning\" . Journal of Mechanics of Continua and Mathematical Sciences (7). doi : 10.26782/jmcms.spl.7/2020.02.00006 . ^ Sarle, Warren S. (1994). \"Neural Networks and statistical models\". SUGI 19: proceedings of the Nineteenth Annual SAS Users Group International Conference . SAS Institute. pp. 1538– 50. ISBN 9781555446116 . OCLC 35546178 . ^ a b c d Russell, Stuart ; Norvig, Peter (2003) [1995]. Artificial Intelligence: A Modern Approach (2nd ed.). Prentice Hall. ISBN 978-0137903955 . ^ a b Langley, Pat (2011). \"The changing science of machine learning\" . Machine Learning . 82 (3): 275– 9. doi : 10.1007/s10994-011-5242-y . ^ Mahoney, Matt. \"Rationale for a Large Text Compression Benchmark\" . Florida Institute of Technology . Retrieved 5 March 2013 . ^ Shmilovici A.; Kahiri Y.; Ben-Gal I.; Hauser S. (2009). \"Measuring the Efficiency of the Intraday Forex Market with a Universal Data Compression Algorithm\" (PDF) . Computational Economics . 33 (2): 131– 154. CiteSeerX 10.1.1.627.3751 . doi : 10.1007/s10614-008-9153-3 . S2CID 17234503 . Archived (PDF) from the original on 9 July 2009. ^ I. Ben-Gal (2008). \"On the Use of Data Compression Measures to Analyze Robust Designs\" (PDF) . IEEE Transactions on Reliability . 54 (3): 381– 388. doi : 10.1109/TR.2005.853280 . S2CID 9376086 . ^ D. Scully; Carla E. Brodley (2006). \"Compression and Machine Learning: A New Perspective on Feature Space Vectors\". Data Compression Conference (DCC'06) . p. 332. doi : 10.1109/DCC.2006.13 . ISBN 0-7695-2545-8 . S2CID 12311412 . ^ Gary Adcock (5 January 2023). \"What Is AI Video Compression?\" . massive.io . Retrieved 6 April 2023 . ^ Mentzer, Fabian; Toderici, George; Tschannen, Michael; Agustsson, Eirikur (2020). \"High-Fidelity Generative Image Compression\". arXiv : 2006.09965 [ eess.IV ]. ^ \"What is Unsupervised Learning? | IBM\" . www.ibm.com . 23 September 2021 . Retrieved 5 February 2024 . ^ \"Differentially private clustering for large-scale datasets\" . blog.research.google . 25 May 2023 . Retrieved 16 March 2024 . ^ Edwards, Benj (28 September 2023). \"AI language models can exceed PNG and FLAC in lossless compression, says study\" . Ars Technica . Retrieved 7 March 2024 . ^ Delétang, Grégoire; Ruoss, Anian; Duquenne, Paul-Ambroise; Catt, Elliot; Genewein, Tim; Mattern, Christopher; Grau-Moya, Jordi; Li Kevin Wenliang; Aitchison, Matthew; Orseau, Laurent; Hutter, Marcus; Veness, Joel (2023). \"Language Modeling is Compression\". arXiv : 2309.10668 [ cs.LG ]. ^ Le Roux, Nicolas; Bengio, Yoshua; Fitzgibbon, Andrew (2012). \"Improving First and Second-Order Methods by Modeling Uncertainty\" . In Sra, Suvrit; Nowozin, Sebastian; Wright, Stephen J. (eds.). Optimization for Machine Learning . MIT Press. p. 404. ISBN 9780262016469 . Archived from the original on 17 January 2023 . Retrieved 12 November 2020 . ^ Bzdok, Danilo; Altman, Naomi ; Krzywinski, Martin (2018). \"Statistics versus Machine Learning\" . Nature Methods . 15 (4): 233– 234. doi : 10.1038/nmeth.4642 . PMC 6082636 . PMID 30100822 . ^ a b Michael I. Jordan (10 September 2014). \"statistics and machine learning\" . reddit. Archived from the original on 18 October 2017 . Retrieved 1 October 2014 . ^ Hung et al. Algorithms to Measure Surgeon Performance and Anticipate Clinical Outcomes in Robotic Surgery. JAMA Surg. 2018 ^ Cornell University Library (August 2001). \"Breiman: Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author)\" . Statistical Science . 16 (3). doi : 10.1214/ss/1009213726 . S2CID 62729017 . Archived from the original on 26 June 2017 . Retrieved 8 August 2015 . ^ Gareth James; Daniela Witten; Trevor Hastie; Robert Tibshirani (2013). An Introduction to Statistical Learning . Springer. p. vii. Archived from the original on 23 June 2019 . Retrieved 25 October 2014 . ^ Ramezanpour, A.; Beam, A.L.; Chen, J.H.; Mashaghi, A. (17 November 2020). \"Statistical Physics for Medical Diagnostics: Learning, Inference, and Optimization Algorithms\" . Diagnostics . 10 (11): 972. doi : 10.3390/diagnostics10110972 . PMC 7699346 . PMID 33228143 . ^ Mashaghi, A.; Ramezanpour, A. (16 March 2018). \"Statistical physics of medical diagnostics: Study of a probabilistic model\". Physical Review E . 97 ( 3– 1): 032118. arXiv : 1803.10019 . Bibcode : 2018PhRvE..97c2118M . doi : 10.1103/PhysRevE.97.032118 . PMID 29776109 . S2CID 4955393 . ^ Mohri, Mehryar ; Rostamizadeh, Afshin; Talwalkar, Ameet (2012). Foundations of Machine Learning . US, Massachusetts: MIT Press. ISBN 9780262018258 . ^ Alpaydin, Ethem (2010). Introduction to Machine Learning . London: The MIT Press. ISBN 978-0-262-01243-0 . Retrieved 4 February 2017 . ^ Jordan, M. I.; Mitchell, T. M. (17 July 2015). \"Machine learning: Trends, perspectives, and prospects\". Science . 349 (6245): 255– 260. Bibcode : 2015Sci...349..255J . doi : 10.1126/science.aaa8415 . PMID 26185243 . S2CID 677218 . ^ El Naqa, Issam; Murphy, Martin J. (2015). \"What is Machine Learning?\". Machine Learning in Radiation Oncology . pp. 3– 11. doi : 10.1007/978-3-319-18305-3_1 . ISBN 978-3-319-18304-6 . S2CID 178586107 . ^ Okolie, Jude A.; Savage, Shauna; Ogbaga, Chukwuma C.; Gunes, Burcu (June 2022). \"Assessing the potential of machine learning methods to study the removal of pharmaceuticals from wastewater using biochar or activated carbon\" . Total Environment Research Themes . 1– 2 100001. Bibcode : 2022TERT....100001O . doi : 10.1016/j.totert.2022.100001 . S2CID 249022386 . ^ Russell, Stuart J.; Norvig, Peter (2010). Artificial Intelligence: A Modern Approach (Third ed.). Prentice Hall. ISBN 9780136042594 . ^ Mohri, Mehryar; Rostamizadeh, Afshin; Talwalkar, Ameet (2012). Foundations of Machine Learning . The MIT Press. ISBN 9780262018258 . ^ Alpaydin, Ethem (2010). Introduction to Machine Learning . MIT Press. p. 9. ISBN 978-0-262-01243-0 . Archived from the original on 17 January 2023 . Retrieved 25 November 2018 . ^ \"Lecture 2 Notes: Supervised Learning\" . www.cs.cornell.edu . Retrieved 1 July 2024 . ^ Jordan, Michael I.; Bishop, Christopher M. (2004). \"Neural Networks\". In Allen B. Tucker (ed.). Computer Science Handbook, Second Edition (Section VII: Intelligent Systems) . Boca Raton, Florida: Chapman \u0026 Hall/CRC Press LLC. ISBN 978-1-58488-360-9 . ^ Misra, Ishan; Maaten, Laurens van der (2020). Self-Supervised Learning of Pretext-Invariant Representations . 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Seattle, WA, USA: IEEE . pp. 6707– 6717. arXiv : 1912.01991 . doi : 10.1109/CVPR42600.2020.00674 . ^ Jaiswal, Ashish; Babu, Ashwin Ramesh; Zadeh, Mohammad Zaki; Banerjee, Debapriya; Makedon, Fillia (March 2021). \"A Survey on Contrastive Self-Supervised Learning\" . Technologies . 9 (1): 2. arXiv : 2011.00362 . doi : 10.3390/technologies9010002 . ISSN 2227-7080 . ^ Alex Ratner; Stephen Bach; Paroma Varma; Chris. \"Weak Supervision: The New Programming Paradigm for Machine Learning\" . hazyresearch.github.io . referencing work by many other members of Hazy Research. Archived from the original on 6 June 2019 . Retrieved 6 June 2019 . ^ van Otterlo, M.; Wiering, M. (2012). \"Reinforcement Learning and Markov Decision Processes\". Reinforcement Learning . Adaptation, Learning, and Optimization. Vol. 12. pp. 3– 42. doi : 10.1007/978-3-642-27645-3_1 . ISBN 978-3-642-27644-6 . ^ Roweis, Sam T.; Saul, Lawrence K. (22 December 2000). \"Nonlinear Dimensionality Reduction by Locally Linear Embedding\" . Science . 290 (5500): 2323– 2326. Bibcode : 2000Sci...290.2323R . doi : 10.1126/science.290.5500.2323 . PMID 11125150 . S2CID 5987139 . Archived from the original on 15 August 2021 . Retrieved 17 July 2023 . ^ Pavel Brazdil; Christophe Giraud Carrier; Carlos Soares; Ricardo Vilalta (2009). Metalearning: Applications to Data Mining (Fourth ed.). Springer Science+Business Media . pp. 10– 14, passim . ISBN 978-3540732624 . ^ Bozinovski, S. (1982). \"A self-learning system using secondary reinforcement\". In Trappl, Robert (ed.). Cybernetics and Systems Research: Proceedings of the Sixth European Meeting on Cybernetics and Systems Research. North-Holland. pp. 397–402. ISBN 978-0-444-86488-8 . ^ Bozinovski, S. (1999) \"Crossbar Adaptive Array: The first connectionist network that solved the delayed reinforcement learning problem\" In A. Dobnikar, N. Steele, D. Pearson, R. Albert (eds.) Artificial Neural Networks and Genetic Algorithms, Springer Verlag, p. 320-325, ISBN 3-211-83364-1 ^ Bozinovski, Stevo (2014) \"Modeling mechanisms of cognition-emotion interaction in artificial neural networks, since 1981.\" Procedia Computer Science p. 255-263 ^ Bozinovski, S. (2001) \"Self-learning agents: A connectionist theory of emotion based on crossbar value judgment.\" Cybernetics and Systems 32(6) 637–667. ^ Y. Bengio; A. Courville; P. Vincent (2013). \"Representation Learning: A Review and New Perspectives\". IEEE Transactions on Pattern Analysis and Machine Intelligence . 35 (8): 1798– 1828. arXiv : 1206.5538 . doi : 10.1109/tpami.2013.50 . PMID 23787338 . S2CID 393948 . ^ Nathan Srebro; Jason D. M. Rennie; Tommi S. Jaakkola (2004). Maximum-Margin Matrix Factorization . NIPS . ^ Coates, Adam; Lee, Honglak; Ng, Andrew Y. (2011). An analysis of single-layer networks in unsupervised feature learning (PDF) . Int'l Conf. on AI and Statistics (AISTATS). Archived from the original (PDF) on 13 August 2017 . Retrieved 25 November 2018 . ^ Csurka, Gabriella; Dance, Christopher C.; Fan, Lixin; Willamowski, Jutta; Bray, Cédric (2004). Visual categorization with bags of keypoints (PDF) . ECCV Workshop on Statistical Learning in Computer Vision. Archived (PDF) from the original on 13 July 2019 . Retrieved 29 August 2019 . ^ Daniel Jurafsky; James H. Martin (2009). Speech and Language Processing . Pearson Education International. pp. 145– 146. ^ Lu, Haiping; Plataniotis, K.N.; Venetsanopoulos, A.N. (2011). \"A Survey of Multilinear Subspace Learning for Tensor Data\" (PDF) . Pattern Recognition . 44 (7): 1540– 1551. Bibcode : 2011PatRe..44.1540L . doi : 10.1016/j.patcog.2011.01.004 . Archived (PDF) from the original on 10 July 2019 . Retrieved 4 September 2015 . ^ Yoshua Bengio (2009). Learning Deep Architectures for AI . Now Publishers Inc. pp. 1– 3. ISBN 978-1-60198-294-0 . Archived from the original on 17 January 2023 . Retrieved 15 February 2016 . ^ Tillmann, A. M. (2015). \"On the Computational Intractability of Exact and Approximate Dictionary Learning\". IEEE Signal Processing Letters . 22 (1): 45– 49. arXiv : 1405.6664 . Bibcode : 2015ISPL...22...45T . doi : 10.1109/LSP.2014.2345761 . S2CID 13342762 . ^ Aharon, M , M Elad, and A Bruckstein. 2006. \" K-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation Archived 2018-11-23 at the Wayback Machine .\" Signal Processing, IEEE Transactions on 54 (11): 4311–4322 ^ Zimek, Arthur; Schubert, Erich (2017), \"Outlier Detection\", Encyclopedia of Database Systems , Springer New York, pp. 1– 5, doi : 10.1007/978-1-4899-7993-3_80719-1 , ISBN 9781489979933 ^ Hodge, V. J.; Austin, J. (2004). \"A Survey of Outlier Detection Methodologies\" (PDF) . Artificial Intelligence Review . 22 (2): 85– 126. CiteSeerX 10.1.1.318.4023 . doi : 10.1007/s10462-004-4304-y . S2CID 59941878 . Archived (PDF) from the original on 22 June 2015 . Retrieved 25 November 2018 . ^ Dokas, Paul; Ertoz, Levent; Kumar, Vipin; Lazarevic, Aleksandar; Srivastava, Jaideep; Tan, Pang-Ning (2002). \"Data mining for network intrusion detection\" (PDF) . Proceedings NSF Workshop on Next Generation Data Mining . Archived (PDF) from the original on 23 September 2015 . Retrieved 26 March 2023 . ^ Chandola, V.; Banerjee, A.; Kumar, V. (2009). \"Anomaly detection: A survey\". ACM Computing Surveys . 41 (3): 1– 58. doi : 10.1145/1541880.1541882 . S2CID 207172599 . ^ Fleer, S.; Moringen, A.; Klatzky, R. L.; Ritter, H. (2020). \"Learning efficient haptic shape exploration with a rigid tactile sensor array, S. Fleer, A. Moringen, R. Klatzky, H. Ritter\" . PLOS ONE . 15 (1): e0226880. arXiv : 1902.07501 . doi : 10.1371/journal.pone.0226880 . PMC 6940144 . PMID 31896135 . ^ Moringen, Alexandra; Fleer, Sascha; Walck, Guillaume; Ritter, Helge (2020), Nisky, Ilana; Hartcher-O'Brien, Jess; Wiertlewski, Michaël; Smeets, Jeroen (eds.), \"Attention-Based Robot Learning of Haptic Interaction\", Haptics: Science, Technology, Applications , Lecture Notes in Computer Science, vol. 12272, Cham: Springer International Publishing, pp. 462– 470, doi : 10.1007/978-3-030-58147-3_51 , ISBN 978-3-030-58146-6 , S2CID 220069113 ^ Piatetsky-Shapiro, Gregory (1991), Discovery, analysis, and presentation of strong rules , in Piatetsky-Shapiro, Gregory; and Frawley, William J.; eds., Knowledge Discovery in Databases , AAAI/MIT Press, Cambridge, MA. ^ Bassel, George W.; Glaab, Enrico; Marquez, Julietta; Holdsworth, Michael J.; Bacardit, Jaume (1 September 2011). \"Functional Network Construction in Arabidopsis Using Rule-Based Machine Learning on Large-Scale Data Sets\" . The Plant Cell . 23 (9): 3101– 3116. Bibcode : 2011PlanC..23.3101B . doi : 10.1105/tpc.111.088153 . ISSN 1532-298X . PMC 3203449 . PMID 21896882 . ^ Agrawal, R.; Imieliński, T.; Swami, A. (1993). \"Mining association rules between sets of items in large databases\". Proceedings of the 1993 ACM SIGMOD international conference on Management of data - SIGMOD '93 . p. 207. CiteSeerX 10.1.1.40.6984 . doi : 10.1145/170035.170072 . ISBN 978-0897915922 . S2CID 490415 . ^ Urbanowicz, Ryan J.; Moore, Jason H. (22 September 2009). \"Learning Classifier Systems: A Complete Introduction, Review, and Roadmap\" . Journal of Artificial Evolution and Applications . 2009 : 1– 25. doi : 10.1155/2009/736398 . ISSN 1687-6229 . ^ Plotkin G.D. Automatic Methods of Inductive Inference Archived 22 December 2017 at the Wayback Machine , PhD thesis, University of Edinburgh, 1970. ^ Shapiro, Ehud Y. Inductive inference of theories from facts Archived 21 August 2021 at the Wayback Machine , Research Report 192, Yale University, Department of Computer Science, 1981. Reprinted in J.-L. Lassez, G. Plotkin (Eds.), Computational Logic, The MIT Press, Cambridge, MA, 1991, pp. 199–254. ^ Shapiro, Ehud Y. (1983). Algorithmic program debugging . Cambridge, Mass: MIT Press. ISBN 0-262-19218-7 ^ Shapiro, Ehud Y. \" The model inference system Archived 2023-04-06 at the Wayback Machine .\" Proceedings of the 7th international joint conference on Artificial intelligence-Volume 2. Morgan Kaufmann Publishers Inc., 1981. ^ Burkov, Andriy (2019). The hundred-page machine learning book . Polen: Andriy Burkov. ISBN 978-1-9995795-0-0 . ^ Russell, Stuart J.; Norvig, Peter (2021). Artificial intelligence: a modern approach . Pearson series in artificial intelligence (Fourth ed.). Hoboken: Pearson. ISBN 978-0-13-461099-3 . ^ Honglak Lee, Roger Grosse, Rajesh Ranganath, Andrew Y. Ng. \" Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations Archived 2017-10-18 at the Wayback Machine \" Proceedings of the 26th Annual International Conference on Machine Learning, 2009. ^ \"RandomForestRegressor\" . scikit-learn . Retrieved 12 February 2025 . ^ \"What Is Random Forest? | IBM\" . www.ibm.com . 20 October 2021 . Retrieved 12 February 2025 . ^ Cortes, Corinna ; Vapnik, Vladimir N. (1995). \"Support-vector networks\" . Machine Learning . 20 (3): 273– 297. doi : 10.1007/BF00994018 . ^ Stevenson, Christopher. \"Tutorial: Polynomial Regression in Excel\" . facultystaff.richmond.edu . Archived from the original on 2 June 2013 . Retrieved 22 January 2017 . ^ Wanta, Damian; Smolik, Aleksander; Smolik, Waldemar T.; Midura, Mateusz; Wróblewski, Przemysław (2025). \"Image reconstruction using machine-learned pseudoinverse in electrical capacitance tomography\" . Engineering Applications of Artificial Intelligence . 142 109888. doi : 10.1016/j.engappai.2024.109888 . ^ The documentation for scikit-learn also has similar examples Archived 2 November 2022 at the Wayback Machine . ^ Goldberg, David E.; Holland, John H. (1988). \"Genetic algorithms and machine learning\" (PDF) . Machine Learning . 3 (2): 95– 99. doi : 10.1007/bf00113892 . S2CID 35506513 . Archived (PDF) from the original on 16 May 2011 . Retrieved 3 September 2019 . ^ Michie, D.; Spiegelhalter, D. J.; Taylor, C. C. (1994). \"Machine Learning, Neural and Statistical Classification\". Ellis Horwood Series in Artificial Intelligence . Bibcode : 1994mlns.book.....M . ^ Zhang, Jun; Zhan, Zhi-hui; Lin, Ying; Chen, Ni; Gong, Yue-jiao; Zhong, Jing-hui; Chung, Henry S.H.; Li, Yun; Shi, Yu-hui (2011). \"Evolutionary Computation Meets Machine Learning: A Survey\". IEEE Computational Intelligence Magazine . 6 (4): 68– 75. doi : 10.1109/mci.2011.942584 . S2CID 6760276 . ^ Verbert, K.; Babuška, R.; De Schutter, B. (1 April 2017). \"Bayesian and Dempster–Shafer reasoning for knowledge-based fault diagnosis–A comparative study\" . Engineering Applications of Artificial Intelligence . 60 : 136– 150. doi : 10.1016/j.engappai.2017.01.011 . ISSN 0952-1976 . ^ Yoosefzadeh-Najafabadi, Mohsen; Hugh, Earl; Tulpan, Dan; Sulik, John; Eskandari, Milad (2021). \"Application of Machine Learning Algorithms in Plant Breeding: Predicting Yield From Hyperspectral Reflectance in Soybean?\" . Front. Plant Sci . 11 : 624273. Bibcode : 2021FrPS...1124273Y . doi : 10.3389/fpls.2020.624273 . PMC 7835636 . PMID 33510761 . ^ Urbanowicz, Ryan J.; Moore, Jason H. (22 September 2009). \"Learning Classifier Systems: A Complete Introduction, Review, and Roadmap\" . Journal of Artificial Evolution and Applications . 2009 : 1– 25. doi : 10.1155/2009/736398 . ISSN 1687-6229 . ^ Zhang, C. and Zhang, S., 2002. Association rule mining: models and algorithms . Springer-Verlag. ^ De Castro, Leandro Nunes, and Jonathan Timmis. Artificial immune systems: a new computational intelligence approach . Springer Science \u0026 Business Media, 2002. ^ \"Federated Learning: Collaborative Machine Learning without Centralized Training Data\" . Google AI Blog . 6 April 2017. Archived from the original on 7 June 2019 . Retrieved 8 June 2019 . ^ Machine learning is included in the CFA Curriculum (discussion is top-down); see: Kathleen DeRose and Christophe Le Lanno (2020). \"Machine Learning\" Archived 13 January 2020 at the Wayback Machine . ^ Ivanenko, Mikhail; Smolik, Waldemar T.; Wanta, Damian; Midura, Mateusz; Wróblewski, Przemysław; Hou, Xiaohan; Yan, Xiaoheng (2023). \"Image Reconstruction Using Supervised Learning in Wearable Electrical Impedance Tomography of the Thorax\" . Sensors . 23 (18): 7774. Bibcode : 2023Senso..23.7774I . doi : 10.3390/s23187774 . PMC 10538128 . PMID 37765831 . ^ \"BelKor Home Page\" research.att.com ^ \"The Netflix Tech Blog: Netflix Recommendations: Beyond the 5 stars (Part 1)\" . 6 April 2012. Archived from the original on 31 May 2016 . Retrieved 8 August 2015 . ^ Scott Patterson (13 July 2010). \"Letting the Machines Decide\" . The Wall Street Journal . Archived from the original on 24 June 2018 . Retrieved 24 June 2018 . ^ Vinod Khosla (10 January 2012). \"Do We Need Doctors or Algorithms?\" . Tech Crunch. Archived from the original on 18 June 2018 . Retrieved 20 October 2016 . ^ When A Machine Learning Algorithm Studied Fine Art Paintings, It Saw Things Art Historians Had Never Noticed Archived 4 June 2016 at the Wayback Machine , The Physics at ArXiv blog ^ Vincent, James (10 April 2019). \"The first AI-generated textbook shows what robot writers are actually good at\" . The Verge . Archived from the original on 5 May 2019 . Retrieved 5 May 2019 . ^ Vaishya, Raju; Javaid, Mohd; Khan, Ibrahim Haleem; Haleem, Abid (1 July 2020). \"Artificial Intelligence (AI) applications for COVID-19 pandemic\" . Diabetes \u0026 Metabolic Syndrome: Clinical Research \u0026 Reviews . 14 (4): 337– 339. doi : 10.1016/j.dsx.2020.04.012 . PMC 7195043 . PMID 32305024 . ^ Rezapouraghdam, Hamed; Akhshik, Arash; Ramkissoon, Haywantee (10 March 2021). \"Application of machine learning to predict visitors' green behavior in marine protected areas: evidence from Cyprus\" . Journal of Sustainable Tourism . 31 (11): 2479– 2505. doi : 10.1080/09669582.2021.1887878 . hdl : 10037/24073 . ^ Dey, Somdip; Singh, Amit Kumar; Wang, Xiaohang; McDonald-Maier, Klaus (15 June 2020). \"User Interaction Aware Reinforcement Learning for Power and Thermal Efficiency of CPU-GPU Mobile MPSoCs\" . 2020 Design, Automation \u0026 Test in Europe Conference \u0026 Exhibition (DATE) (PDF) . pp. 1728– 1733. doi : 10.23919/DATE48585.2020.9116294 . ISBN 978-3-9819263-4-7 . S2CID 219858480 . Archived from the original on 13 December 2021 . Retrieved 20 January 2022 . ^ Quested, Tony. \"Smartphones get smarter with Essex innovation\" . Business Weekly . Archived from the original on 24 June 2021 . Retrieved 17 June 2021 . ^ Williams, Rhiannon (21 July 2020). \"Future smartphones 'will prolong their own battery life by monitoring owners' behaviour' \" . i . Archived from the original on 24 June 2021 . Retrieved 17 June 2021 . ^ Rasekhschaffe, Keywan Christian; Jones, Robert C. (1 July 2019). \"Machine Learning for Stock Selection\" . Financial Analysts Journal . 75 (3): 70– 88. doi : 10.1080/0015198X.2019.1596678 . ISSN 0015-198X . S2CID 108312507 . Archived from the original on 26 November 2023 . Retrieved 26 November 2023 . ^ Chung, Yunsie; Green, William H. (2024). \"Machine learning from quantum chemistry to predict experimental solvent effects on reaction rates\" . Chemical Science . 15 (7): 2410– 2424. doi : 10.1039/D3SC05353A . ISSN 2041-6520 . PMC 10866337 . PMID 38362410 . ^ Sun, Yuran; Huang, Shih-Kai; Zhao, Xilei (1 February 2024). \"Predicting Hurricane Evacuation Decisions with Interpretable Machine Learning Methods\" . International Journal of Disaster Risk Science . 15 (1): 134– 148. arXiv : 2303.06557 . Bibcode : 2024IJDRS..15..134S . doi : 10.1007/s13753-024-00541-1 . ISSN 2192-6395 . ^ Sun, Yuran; Zhao, Xilei; Lovreglio, Ruggiero; Kuligowski, Erica (1 January 2024), Naser, M. Z. (ed.), \"8 - AI for large-scale evacuation modeling: promises and challenges\" , Interpretable Machine Learning for the Analysis, Design, Assessment, and Informed Decision Making for Civil Infrastructure , Woodhead Publishing Series in Civil and Structural Engineering, Woodhead Publishing, pp. 185– 204, ISBN 978-0-12-824073-1 , archived from the original on 19 May 2024 , retrieved 19 May 2024 ^ Xu, Ningzhe; Lovreglio, Ruggiero; Kuligowski, Erica D.; Cova, Thomas J.; Nilsson, Daniel; Zhao, Xilei (1 March 2023). \"Predicting and Assessing Wildfire Evacuation Decision-Making Using Machine Learning: Findings from the 2019 Kincade Fire\" . Fire Technology . 59 (2): 793– 825. doi : 10.1007/s10694-023-01363-1 . ISSN 1572-8099 . Archived from the original on 19 May 2024 . Retrieved 19 May 2024 . ^ Wang, Ke; Shi, Xiupeng; Goh, Algena Pei Xuan; Qian, Shunzhi (1 June 2019). \"A machine learning based study on pedestrian movement dynamics under emergency evacuation\" . Fire Safety Journal . 106 : 163– 176. Bibcode : 2019FirSJ.106..163W . doi : 10.1016/j.firesaf.2019.04.008 . hdl : 10356/143390 . ISSN 0379-7112 . Archived from the original on 19 May 2024 . Retrieved 19 May 2024 . ^ Zhao, Xilei; Lovreglio, Ruggiero; Nilsson, Daniel (1 May 2020). \"Modelling and interpreting pre-evacuation decision-making using machine learning\" . Automation in Construction . 113 103140. doi : 10.1016/j.autcon.2020.103140 . hdl : 10179/17315 . ISSN 0926-5805 . Archived from the original on 19 May 2024 . Retrieved 19 May 2024 . ^ Phoon, Kok-Kwang; Zhang, Wengang (2 January 2023). \"Future of machine learning in geotechnics\" . Georisk: Assessment and Management of Risk for Engineered Systems and Geohazards . 17 (1): 7– 22. Bibcode : 2023GAMRE..17....7P . doi : 10.1080/17499518.2022.2087884 . ISSN 1749-9518 . ^ \"Why Machine Learning Models Often Fail to Learn: QuickTake Q\u0026A\" . Bloomberg.com . 10 November 2016. Archived from the original on 20 March 2017 . Retrieved 10 April 2017 . ^ \"The First Wave of Corporate AI Is Doomed to Fail\" . Harvard Business Review . 18 April 2017. Archived from the original on 21 August 2018 . Retrieved 20 August 2018 . ^ \"Why the A.I. euphoria is doomed to fail\" . VentureBeat . 18 September 2016. Archived from the original on 19 August 2018 . Retrieved 20 August 2018 . ^ \"9 Reasons why your machine learning project will fail\" . www.kdnuggets.com . Archived from the original on 21 August 2018 . Retrieved 20 August 2018 . ^ a b Babuta, Alexander; Oswald, Marion; Rinik, Christine (2018). Transparency and Intelligibility (Report). Royal United Services Institute (RUSI). pp. 17– 22. Archived from the original on 9 December 2023 . Retrieved 9 December 2023 . ^ \"Why Uber's self-driving car killed a pedestrian\" . The Economist . Archived from the original on 21 August 2018 . Retrieved 20 August 2018 . ^ \"IBM's Watson recommended 'unsafe and incorrect' cancer treatments – STAT\" . STAT . 25 July 2018. Archived from the original on 21 August 2018 . Retrieved 21 August 2018 . ^ Hernandez, Daniela; Greenwald, Ted (11 August 2018). \"IBM Has a Watson Dilemma\" . The Wall Street Journal . ISSN 0099-9660 . Archived from the original on 21 August 2018 . Retrieved 21 August 2018 . ^ Allyn, Bobby (27 February 2023). \"How Microsoft's experiment in artificial intelligence tech backfired\" . National Public Radio . Archived from the original on 8 December 2023 . Retrieved 8 December 2023 . ^ Reddy, Shivani M.; Patel, Sheila; Weyrich, Meghan; Fenton, Joshua; Viswanathan, Meera (2020). \"Comparison of a traditional systematic review approach with review-of-reviews and semi-automation as strategies to update the evidence\" . Systematic Reviews . 9 (1): 243. doi : 10.1186/s13643-020-01450-2 . ISSN 2046-4053 . PMC 7574591 . PMID 33076975 . ^ Rudin, Cynthia (2019). \"Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead\" . Nature Machine Intelligence . 1 (5): 206– 215. doi : 10.1038/s42256-019-0048-x . PMC 9122117 . PMID 35603010 . ^ Hu, Tongxi; Zhang, Xuesong; Bohrer, Gil; Liu, Yanlan; Zhou, Yuyu; Martin, Jay; LI, Yang; Zhao, Kaiguang (2023). \"Crop yield prediction via explainable AI and interpretable machine learning: Dangers of black box models for evaluating climate change impacts on crop yield\" . Agricultural and Forest Meteorology . 336 109458. doi : 10.1016/j.agrformet.2023.109458 . S2CID 258552400 . ^ Domingos 2015 , Chapter 6, Chapter 7. ^ Domingos 2015 , p. 286. ^ \"Single pixel change fools AI programs\" . BBC News . 3 November 2017. Archived from the original on 22 March 2018 . Retrieved 12 March 2018 . ^ \"AI Has a Hallucination Problem That's Proving Tough to Fix\" . WIRED . 2018. Archived from the original on 12 March 2018 . Retrieved 12 March 2018 . ^ Madry, A.; Makelov, A.; Schmidt, L.; Tsipras, D.; Vladu, A. (4 September 2019). \"Towards deep learning models resistant to adversarial attacks\". arXiv : 1706.06083 [ stat.ML ]. ^ \"Adversarial Machine Learning – CLTC UC Berkeley Center for Long-Term Cybersecurity\" . CLTC . Archived from the original on 17 May 2022 . Retrieved 25 May 2022 . ^ \"Machine-learning models vulnerable to undetectable backdoors\" . The Register . Archived from the original on 13 May 2022 . Retrieved 13 May 2022 . ^ \"Undetectable Backdoors Plantable In Any Machine-Learning Algorithm\" . IEEE Spectrum . 10 May 2022. Archived from the original on 11 May 2022 . Retrieved 13 May 2022 . ^ Goldwasser, Shafi; Kim, Michael P.; Vaikuntanathan, Vinod; Zamir, Or (14 April 2022). \"Planting Undetectable Backdoors in Machine Learning Models\". arXiv : 2204.06974 [ cs.LG ]. ^ Kohavi, Ron (1995). \"A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection\" (PDF) . International Joint Conference on Artificial Intelligence . Archived (PDF) from the original on 12 July 2018 . Retrieved 26 March 2023 . ^ Catal, Cagatay (2012). \"Performance Evaluation Metrics for Software Fault Prediction Studies\" (PDF) . Acta Polytechnica Hungarica . 9 (4) . Retrieved 2 October 2016 . ^ a b Müller, Vincent C. (30 April 2020). \"Ethics of Artificial Intelligence and Robotics\" . Stanford Encyclopedia of Philosophy . Archived from the original on 10 October 2020. ^ Van Eyghen, Hans (2025). \"AI Algorithms as (Un)virtuous Knowers\" . Discover Artificial Intelligence . 5 (2) 2. doi : 10.1007/s44163-024-00219-z . ^ Krištofík, Andrej (28 April 2025). \"Bias in AI (Supported) Decision Making: Old Problems, New Technologies\" . International Journal for Court Administration . 16 (1). doi : 10.36745/ijca.598 . ISSN 2156-7964 . ^ \"Assessing potential future artificial intelligence risks, benefits and policy imperatives\" . OECD . 14 November 2024 . Retrieved 4 August 2025 . ^ a b Garcia, Megan (2016). \"Racist in the Machine\". World Policy Journal . 33 (4): 111– 117. doi : 10.1215/07402775-3813015 . ISSN 0740-2775 . S2CID 151595343 . ^ Bostrom, Nick (2011). \"The Ethics of Artificial Intelligence\" (PDF) . Archived from the original (PDF) on 4 March 2016 . Retrieved 11 April 2016 . ^ Edionwe, Tolulope. \"The fight against racist algorithms\" . The Outline . Archived from the original on 17 November 2017 . Retrieved 17 November 2017 . ^ Jeffries, Adrianne. \"Machine learning is racist because the internet is racist\" . The Outline . Archived from the original on 17 November 2017 . Retrieved 17 November 2017 . ^ a b Silva, Selena; Kenney, Martin (2018). \"Algorithms, Platforms, and Ethnic Bias: An Integrative Essay\" (PDF) . Phylon . 55 (1 \u0026 2): 9– 37. ISSN 0031-8906 . JSTOR 26545017 . Archived (PDF) from the original on 27 January 2024. ^ Wong, Carissa (30 March 2023). \"AI 'fairness' research held back by lack of diversity\" . Nature . doi : 10.1038/d41586-023-00935-z . PMID 36997714 . S2CID 257857012 . Archived from the original on 12 April 2023 . Retrieved 9 December 2023 . ^ a b Zhang, Jack Clark. \"Artificial Intelligence Index Report 2021\" (PDF) . Stanford Institute for Human-Centered Artificial Intelligence . Archived (PDF) from the original on 19 May 2024 . Retrieved 9 December 2023 . ^ Caliskan, Aylin; Bryson, Joanna J.; Narayanan, Arvind (14 April 2017). \"Semantics derived automatically from language corpora contain human-like biases\". Science . 356 (6334): 183– 186. arXiv : 1608.07187 . Bibcode : 2017Sci...356..183C . doi : 10.1126/science.aal4230 . ISSN 0036-8075 . PMID 28408601 . S2CID 23163324 . ^ Wang, Xinan; Dasgupta, Sanjoy (2016), Lee, D. D.; Sugiyama, M.; Luxburg, U. V.; Guyon, I. (eds.), \"An algorithm for L1 nearest neighbor search via monotonic embedding\" (PDF) , Advances in Neural Information Processing Systems 29 , Curran Associates, Inc., pp. 983– 991, archived (PDF) from the original on 7 April 2017 , retrieved 20 August 2018 ^ M.O.R. Prates; P.H.C. Avelar; L.C. Lamb (11 March 2019). \"Assessing Gender Bias in Machine Translation – A Case Study with Google Translate\". arXiv : 1809.02208 [ cs.CY ]. ^ Narayanan, Arvind (24 August 2016). \"Language necessarily contains human biases, and so will machines trained on language corpora\" . Freedom to Tinker . Archived from the original on 25 June 2018 . Retrieved 19 November 2016 . ^ Metz, Rachel (24 March 2016). \"Why Microsoft Accidentally Unleashed a Neo-Nazi Sexbot\" . MIT Technology Review . Archived from the original on 9 November 2018 . Retrieved 20 August 2018 . ^ Vincent, James (12 January 2018). \"Google 'fixed' its racist algorithm by removing gorillas from its image-labeling tech\" . The Verge . Archived from the original on 21 August 2018 . Retrieved 20 August 2018 . ^ Crawford, Kate (25 June 2016). \"Opinion | Artificial Intelligence's White Guy Problem\" . New York Times . Archived from the original on 14 January 2021 . Retrieved 20 August 2018 . ^ Simonite, Tom (30 March 2017). \"Microsoft: AI Isn't Yet Adaptable Enough to Help Businesses\" . MIT Technology Review . Archived from the original on 9 November 2018 . Retrieved 20 August 2018 . ^ Hempel, Jessi (13 November 2018). \"Fei-Fei Li's Quest to Make Machines Better for Humanity\" . Wired . ISSN 1059-1028 . Archived from the original on 14 December 2020 . Retrieved 17 February 2019 . ^ Char, D. S.; Shah, N. H.; Magnus, D. (2018). \"Implementing Machine Learning in Health Care—Addressing Ethical Challenges\" . New England Journal of Medicine . 378 (11): 981– 983. doi : 10.1056/nejmp1714229 . PMC 5962261 . PMID 29539284 . ^ Research, AI (23 October 2015). \"Deep Neural Networks for Acoustic Modeling in Speech Recognition\" . airesearch.com . Archived from the original on 1 February 2016 . Retrieved 23 October 2015 . ^ \"GPUs Continue to Dominate the AI Accelerator Market for Now\" . InformationWeek . December 2019. Archived from the original on 10 June 2020 . Retrieved 11 June 2020 . ^ Ray, Tiernan (2019). \"AI is changing the entire nature of compute\" . ZDNet . Archived from the original on 25 May 2020 . Retrieved 11 June 2020 . ^ \"AI and Compute\" . OpenAI . 16 May 2018. Archived from the original on 17 June 2020 . Retrieved 11 June 2020 . ^ Jouppi, Norman P.; Young, Cliff; Patil, Nishant; Patterson, David; Agrawal, Gaurav; Bajwa, Raminder; Bates, Sarah; Bhatia, Suresh; Boden, Nan; Borchers, Al; Boyle, Rick; Cantin, Pierre-luc; Chao, Clifford; Clark, Chris; Coriell, Jeremy (24 June 2017). \"In-Datacenter Performance Analysis of a Tensor Processing Unit\" . Proceedings of the 44th Annual International Symposium on Computer Architecture . ISCA '17. New York, NY, USA: Association for Computing Machinery. pp. 1– 12. arXiv : 1704.04760 . doi : 10.1145/3079856.3080246 . ISBN 978-1-4503-4892-8 . ^ \"What is neuromorphic computing? Everything you need to know about how it is changing the future of computing\" . ZDNET . 8 December 2020 . Retrieved 21 November 2024 . ^ \"Cornell \u0026 NTT's Physical Neural Networks: A \"Radical Alternative for Implementing Deep Neural Networks\" That Enables Arbitrary Physical Systems Training\" . Synced . 27 May 2021. Archived from the original on 27 October 2021 . Retrieved 12 October 2021 . ^ \"Nano-spaghetti to solve neural network power consumption\" . The Register . 5 October 2021. Archived from the original on 6 October 2021 . Retrieved 12 October 2021 . ^ Fafoutis, Xenofon; Marchegiani, Letizia; Elsts, Atis; Pope, James; Piechocki, Robert; Craddock, Ian (7 May 2018). \"Extending the battery lifetime of wearable sensors with embedded machine learning\" . 2018 IEEE 4th World Forum on Internet of Things (WF-IoT) . pp. 269– 274. doi : 10.1109/WF-IoT.2018.8355116 . hdl : 1983/b8fdb58b-7114-45c6-82e4-4ab239c1327f . ISBN 978-1-4673-9944-9 . S2CID 19192912 . Archived from the original on 18 January 2022 . Retrieved 17 January 2022 . ^ \"A Beginner's Guide To Machine learning For Embedded Systems\" . Analytics India Magazine . 2 June 2021. Archived from the original on 18 January 2022 . Retrieved 17 January 2022 . ^ Synced (12 January 2022). \"Google, Purdue \u0026 Harvard U's Open-Source Framework for TinyML Achieves up to 75x Speedups on FPGAs | Synced\" . syncedreview.com . Archived from the original on 18 January 2022 . Retrieved 17 January 2022 . ^ AlSelek, Mohammad; Alcaraz-Calero, Jose M.; Wang, Qi (2024). \"Dynamic AI-IoT: Enabling Updatable AI Models in Ultralow-Power 5G IoT Devices\" . IEEE Internet of Things Journal . 11 (8): 14192– 14205. doi : 10.1109/JIOT.2023.3340858 . ^ Giri, Davide; Chiu, Kuan-Lin; Di Guglielmo, Giuseppe; Mantovani, Paolo; Carloni, Luca P. (15 June 2020). \"ESP4ML: Platform-Based Design of Systems-on-Chip for Embedded Machine Learning\" . 2020 Design, Automation \u0026 Test in Europe Conference \u0026 Exhibition (DATE) . pp. 1049– 1054. arXiv : 2004.03640 . doi : 10.23919/DATE48585.2020.9116317 . ISBN 978-3-9819263-4-7 . S2CID 210928161 . Archived from the original on 18 January 2022 . Retrieved 17 January 2022 . ^ Louis, Marcia Sahaya; Azad, Zahra; Delshadtehrani, Leila; Gupta, Suyog; Warden, Pete; Reddi, Vijay Janapa; Joshi, Ajay (2019). \"Towards Deep Learning using TensorFlow Lite on RISC-V\" . Harvard University . Archived from the original on 17 January 2022 . Retrieved 17 January 2022 . ^ Ibrahim, Ali; Osta, Mario; Alameh, Mohamad; Saleh, Moustafa; Chible, Hussein; Valle, Maurizio (21 January 2019). \"Approximate Computing Methods for Embedded Machine Learning\". 2018 25th IEEE International Conference on Electronics, Circuits and Systems (ICECS) . pp. 845– 848. doi : 10.1109/ICECS.2018.8617877 . ISBN 978-1-5386-9562-3 . S2CID 58670712 . ^ \"dblp: TensorFlow Eager: A Multi-Stage, Python-Embedded DSL for Machine Learning\" . dblp.org . Archived from the original on 18 January 2022 . Retrieved 17 January 2022 . ^ Branco, Sérgio; Ferreira, André G.; Cabral, Jorge (5 November 2019). \"Machine Learning in Resource-Scarce Embedded Systems, FPGAs, and End-Devices: A Survey\" . Electronics . 8 (11): 1289. doi : 10.3390/electronics8111289 . hdl : 1822/62521 . ISSN 2079-9292 .\n\nSources\n\n[ edit ] Domingos, Pedro (22 September 2015). The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World . Basic Books. ISBN 978-0465065707 . Nilsson, Nils (1998). Artificial Intelligence: A New Synthesis . Morgan Kaufmann. ISBN 978-1-55860-467-4 . Archived from the original on 26 July 2020 . Retrieved 18 November 2019 . Poole, David; Mackworth, Alan ; Goebel, Randy (1998). Computational Intelligence: A Logical Approach . New York: Oxford University Press. ISBN 978-0-19-510270-3 . Archived from the original on 26 July 2020 . Retrieved 22 August 2020 . Russell, Stuart J. ; Norvig, Peter (2003), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2\n\n.\n\nFurther reading\n\n[ edit ]\n\nAlpaydin, Ethem (2020).\n\nIntroduction to Machine Learning\n\n, (4th edition) MIT Press,\n\nISBN 9780262043793\n\n.\n\nBishop, Christopher\n\n(1995).\n\nNeural Networks for Pattern Recognition\n\n, Oxford University Press.\n\nISBN 0-19-853864-2\n\n.\n\nBishop, Christopher (2006)\n\nPattern Recognition and Machine Learning\n\n, Springer.\n\nISBN 978-0-387-31073-2 Domingos, Pedro\n\n(September 2015),\n\nThe Master Algorithm\n\n, Basic Books,\n\nISBN 978-0-465-06570-7 Duda, Richard O.\n\n;\n\nHart, Peter E.\n\n; Stork, David G. (2001)\n\nPattern classification\n\n(2nd edition), Wiley, New York,\n\nISBN 0-471-05669-3\n\n.\n\nHastie, Trevor\n\n;\n\nTibshirani, Robert\n\n\u0026\n\nFriedman, Jerome H.\n\n(2009)\n\nThe Elements of Statistical Learning\n\n, Springer.\n\ndoi\n\n:\n\n10.1007/978-0-387-84858-7 ISBN 0-387-95284-5\n\n.\n\nMacKay, David J. C. Information Theory, Inference, and Learning Algorithms\n\nCambridge: Cambridge University Press, 2003.\n\nISBN 0-521-64298-1\n\nMurphy, Kevin P. (2021).\n\nProbabilistic Machine Learning: An Introduction Archived 11 April 2021 at the Wayback Machine\n\n, MIT Press.\n\nNilsson, Nils J. (2015)\n\nIntroduction to Machine Learning Archived 16 August 2019 at the Wayback Machine\n\n.\n\nRussell, Stuart \u0026 Norvig, Peter (2020).\n\nArtificial Intelligence – A Modern Approach\n\n. (4th edition) Pearson,\n\nISBN 978-0134610993\n\n.\n\nSolomonoff, Ray\n\n, (1956)\n\nAn Inductive Inference Machine Archived 26 April 2011 at the Wayback Machine\n\nA privately circulated report from the 1956\n\nDartmouth Summer Research Conference on AI\n\n.\n\nWitten, Ian H. \u0026 Frank, Eibe (2011).\n\nData Mining: Practical machine learning tools and techniques\n\nMorgan Kaufmann, 664pp.,\n\nISBN 978-0-12-374856-0\n\n.\n\nExternal links\n\n[ edit ] International Machine Learning Society mloss\n\nis an academic database of open-source machine learning software.\n\nv t e Artificial intelligence\n\n(AI)\n\nHistory timeline Companies Projects\n\nConcepts\n\nParameter Hyperparameter Loss functions Regression Bias–variance tradeoff Double descent Overfitting Clustering Gradient descent SGD Quasi-Newton method Conjugate gradient method Backpropagation Attention Convolution Normalization Batchnorm Activation Softmax Sigmoid Rectifier Gating Weight initialization Regularization Datasets Augmentation Prompt engineering Reinforcement learning Q-learning SARSA Imitation Policy gradient Diffusion Latent diffusion model Autoregression Adversary RAG Uncanny valley RLHF Self-supervised learning Reflection Recursive self-improvement Hallucination Word embedding Vibe coding\n\nApplications\n\nMachine learning In-context learning Artificial neural network Deep learning Language model Large language model NMT Reasoning language model Model Context Protocol Intelligent agent Artificial human companion Humanity's Last Exam Artificial general intelligence (AGI)\n\nImplementations\n\nAudio–visual\n\nAlexNet WaveNet Human image synthesis HWR OCR Computer vision Speech synthesis 15.ai ElevenLabs Speech recognition Whisper Facial recognition AlphaFold Text-to-image models Aurora DALL-E Firefly Flux Ideogram Imagen Midjourney Recraft Stable Diffusion Text-to-video models Dream Machine Runway Gen Hailuo AI Kling Sora Veo Music generation Riffusion Suno AI Udio\n\nText\n\nWord2vec Seq2seq GloVe BERT T5 Llama Chinchilla AI PaLM GPT 1 2 3 J ChatGPT 4 4o o1 o3 4.5 4.1 o4-mini 5 Claude Gemini chatbot Grok LaMDA BLOOM DBRX Project Debater IBM Watson IBM Watsonx Granite PanGu-Σ DeepSeek Qwen\n\nDecisional\n\nAlphaGo AlphaZero OpenAI Five Self-driving car MuZero Action selection AutoGPT Robot control\n\nPeople\n\nAlan Turing Warren Sturgis McCulloch Walter Pitts John von Neumann Claude Shannon Shun'ichi Amari Kunihiko Fukushima Takeo Kanade Marvin Minsky John McCarthy Nathaniel Rochester Allen Newell Cliff Shaw Herbert A. Simon Oliver Selfridge Frank Rosenblatt Bernard Widrow Joseph Weizenbaum Seymour Papert Seppo Linnainmaa Paul Werbos Geoffrey Hinton John Hopfield Jürgen Schmidhuber Yann LeCun Yoshua Bengio Lotfi A. Zadeh Stephen Grossberg Alex Graves James Goodnight Andrew Ng Fei-Fei Li Ilya Sutskever Alex Krizhevsky Ian Goodfellow Demis Hassabis David Silver Andrej Karpathy Ashish Vaswani Noam Shazeer Aidan Gomez Mustafa Suleyman François Chollet\n\nArchitectures\n\nNeural Turing machine Differentiable neural computer Transformer Vision transformer (ViT) Recurrent neural network (RNN) Long short-term memory (LSTM) Gated recurrent unit (GRU) Echo state network Multilayer perceptron (MLP) Convolutional neural network (CNN) Residual neural network (RNN) Highway network Mamba Autoencoder Variational autoencoder (VAE) Generative adversarial network (GAN) Graph neural network (GNN) Category v t e Computer science\n\nNote: This template roughly follows the 2012\n\nACM Computing Classification System\n\n.\n\nHardware Printed circuit board Peripheral Integrated circuit Very-large-scale integration System on a chip\n\n(SoC)\n\nEnergy consumption\n\n(green computing)\n\nElectronic design automation Hardware acceleration Processor Size\n\n/\n\nForm\n\nComputer systems organization\n\nComputer architecture Computational complexity Dependability Embedded system Real-time computing Cyber-physical system Fault tolerance Wireless sensor network Networks Network architecture Network protocol Network components Network scheduler Network performance evaluation Network service\n\nSoftware organization\n\nInterpreter Middleware Virtual machine Operating system Software quality Software notations\n\nand\n\ntools Programming paradigm Programming language Compiler Domain-specific language Modeling language Software framework Integrated development environment Software configuration management Software library Software repository Software development Control variable Software development process Requirements analysis Software design Software construction Software deployment Software engineering Software maintenance Programming team Open-source model Theory of computation Model of computation Stochastic Formal language Automata theory Computability theory Computational complexity theory Logic Semantics Algorithms Algorithm design Analysis of algorithms Algorithmic efficiency Randomized algorithm Computational geometry\n\nMathematics of\n\ncomputing Discrete mathematics Probability Statistics Mathematical software Information theory Mathematical analysis Numerical analysis Theoretical computer science Information systems Database management system Information storage systems Enterprise information system Social information systems Geographic information system Decision support system Process control system Multimedia information system Data mining Digital library Computing platform Digital marketing World Wide Web Information retrieval Security Cryptography Formal methods Security hacker Security services Intrusion detection system Hardware security Network security Information security Application security Human–centered computing Interaction design Augmented reality Virtual reality Social computing Ubiquitous computing Visualization Accessibility Human–computer interaction Mobile computing Concurrency Concurrent computing Parallel computing Distributed computing Multithreading Multiprocessing Artificial intelligence Natural language processing Knowledge representation and reasoning Computer vision Automated planning and scheduling Search methodology Control method Philosophy of artificial intelligence Distributed artificial intelligence Machine learning Supervised learning Unsupervised learning Reinforcement learning Multi-task learning Cross-validation Graphics Animation Rendering Photograph manipulation Graphics processing unit Image compression Solid modeling\n\nApplied computing\n\nQuantum computing E-commerce Enterprise software Computational mathematics Computational physics Computational chemistry Computational biology Computational social science Computational engineering Differentiable computing Computational healthcare Digital art Electronic publishing Cyberwarfare Electronic voting Video games Word processing Operations research Educational technology Document management Category Outline Glossaries Portals : Computer programming Mathematics Systems science Technology Machine learning\n\nat Wikipedia's\n\nsister projects\n\n:\n\nDefinitions from Wiktionary Media from Commons Quotations from Wikiquote Textbooks from Wikibooks Resources from Wikiversity Data from Wikidata Authority control databases\n\nNational\n\nGermany United States Japan Czech Republic Israel\n\nOther\n\nYale LUX\n\nRetrieved from \"\n\nhttps://en.wikipedia.org/w/index.php?title=Machine_learning\u0026oldid=1304757840\n\n\"\n\nCategories\n\n:\n\nMachine learning Cybernetics Learning Definition\n\nHidden categories:\n\nWebarchive template wayback links Articles with short description Short description is different from Wikidata Use dmy dates from April 2025 Use British English from April 2025 Articles with excerpts Search Search Machine learning 88 languages Add topic",
      "metadata": {
        "characters": "123306",
        "title": "Machine learning - Wikipedia",
        "type": "html"
      },
      "word_count": 18472,
      "quality": "high",
      "extracted_at": "2025-08-10T20:51:42Z"
    }
  ],
  "metadata": {
    "name": "High-Quality Extracted Content",
    "description": "Properly extracted content with improved HTML parser",
    "total_items": 3,
    "total_words": 35942
  },
  "created_at": "2025-08-10T20:51:42Z"
}
