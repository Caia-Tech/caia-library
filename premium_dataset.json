{
  "conversations": [
    {
      "dialogue": [
        {
          "content": "What is Recursion really about at its core?",
          "role": "student"
        },
        {
          "content": "Use of functions that call themselves\nMathematical induction\n. For recursion in computer science acronyms, see\nRecursive acronym § Computer-related examples\n. For general use of the term, see\nTree fractal created using the Logo programming language and relying heavily on recursion. Each branch can be seen as a smaller version of a tree. Recursive drawing of a Sierpiński Triangle through turtle graphics\nis a method of solving a\ncomputational problem\nwhere the solution depends on solutions to smaller instances of the same problem.\nRecursion solves such\nthat call themselves from within their own code. The approach can be applied to many types of problems, and recursion is one of the central ideas of computer science.\nThe power of recursion evidently lies in the possibility of defining an infinite set of objects by a finite statement. In the same manner, an infinite number of computations can be described by a finite recursive program, even if this program contains no explicit repetitions.\nAlgorithms + Data Structures = Programs\nprogramming languages\nsupport recursion by allowing a function to call itself from within its own code. Some\nfunctional programming\nlanguages (for instance,\ndo not define any looping constructs but rely solely on recursion to repeatedly call code. It is proved in\ncomputability theory\nthat these recursive-only languages are\n; this means that they are as powerful (they can be used to solve the same problems) as\nimperative languages\nbased on control structures such as\nRepeatedly calling a function from within itself may cause the\nto have a size equal to the sum of the input sizes of all involved calls. It follows that, for problems that can be solved easily by iteration, recursion is generally less\n, and, for certain problems, algorithmic or compiler-optimization techniques such as\noptimization may improve computational performance over a naive recursive implementation.\nRecursive functions and algorithms\ntactic is to divide a problem into sub-problems of the same type as the original, solve those sub-problems, and combine the results. This is often referred to as the\ndivide-and-conquer method\n; when combined with a\nthat stores the results of previously solved sub-problems (to avoid solving them repeatedly and incurring extra computation time), it can be referred to as\nA recursive function definition has one or more\n, meaning input(s) for which the function produces a result\n(without recurring), and one or more\n, meaning input(s) for which the program recurs (calls itself). For example, the\nfunction can be defined recursively by the equations\n. Neither equation by itself constitutes a complete definition; the first is the base case, and the second is the recursive case. Because the base case breaks the chain of recursion, it is sometimes also called the \"terminating case\".\nThe job of the recursive cases can be seen as breaking down complex inputs into simpler ones. In a properly designed recursive function, with each recursive call, the input problem must be simplified in such a way that eventually the base case must be reached. (Functions that are not intended to terminate under normal circumstances—for example, some\nsystem and server processes\n—are an exception to this.) Neglecting to write a base case, or testing for it incorrectly, can cause an\nFor some functions (such as one that computes the\ne = 1/0! + 1/1! + 1/2! + 1/3! + ...\n) there is not an obvious base case implied by the input data; for these one may add a\n(such as the number of terms to be added, in our series example) to provide a 'stopping criterion' that establishes the base case. Such an example is more naturally treated by\nwhere successive terms in the output are the partial sums; this can be converted to a recursion by using the indexing parameter to say \"compute the\nRecursive data types\nmust process or generate an arbitrarily large quantity of\n. Recursion is a technique for representing data whose exact size is unknown to the\n: the programmer can specify this data with a\ndefinition. There are two types of self-referential definitions: inductive and\nFurther information:\nInductively defined data\nAn inductively defined recursive data definition is one that specifies how to construct instances of the data. For example,\ncan be defined inductively (here, using\ndata ListOfStrings = EmptyList | Cons String ListOfStrings\nThe code above specifies a list of strings to be either empty, or a structure that contains a string and a list of strings. The self-reference in the definition permits the construction of lists of any (finite) number of strings.\nAnother example of inductive\nA natural number is either 1 or n+1, where n is a natural number.\nare often used to model the structure of\nin programming languages. Language designers often express grammars in a syntax such as\n; here is such a grammar, for a simple language of arithmetic expressions with multiplication and addition:\n\u003c expr \u003e ::= \u003c number \u003e\nThis says that an expression is either a number, a product of two expressions, or a sum of two expressions. By recursively referring to expressions in the second and third lines, the grammar permits arbitrarily complicated arithmetic expressions such as\n, with more than one product or sum operation in a single expression.\nCoinductively defined data and corecursion\nA coinductive data definition is one that specifies the operations that may be performed on a piece of data; typically, self-referential coinductive definitions are used for data structures of infinite size.\nA coinductive definition of infinite\nof strings, given informally, might look like this:\nA stream of strings is an object s such that:\nhead(s) is a string, and\ntail(s) is a stream of strings.\nThis is very similar to an inductive definition of lists of strings; the difference is that this definition specifies how to access the contents of the data structure—namely, via the\n—and what those contents may be, whereas the inductive definition specifies how to create the structure and what it may be created from.\nCorecursion is related to coinduction, and can be used to compute particular instances of (possibly) infinite objects. As a programming technique, it is used most often in the context of\nprogramming languages, and can be preferable to recursion when the desired size or precision of a program's output is unknown. In such cases the program requires both a definition for an infinitely large (or infinitely precise) result, and a mechanism for taking a finite portion of that result. The problem of computing the first n\nis one that can be solved with a corecursive program (e.g.\nSingle recursion and multiple recursion\nRecursion that contains only a single self-reference is known as\n, while recursion that contains multiple self-references is known as\n. Standard examples of single recursion include list traversal, such as in a linear search, or computing the factorial function, while standard examples of multiple recursion include\n, such as in a depth-first search.\nSingle recursion is often much more efficient than multiple recursion, and can generally be replaced by an iterative computation, running in linear time and requiring constant space. Multiple recursion, by contrast, may require exponential time and space, and is more fundamentally recursive, not being able to be replaced by iteration without an explicit stack.\nMultiple recursion can sometimes be converted to single recursion (and, if desired, thence to iteration). For example, while computing the Fibonacci sequence naively entails multiple iteration, as each value requires two previous values, it can be computed by single recursion by passing two successive values as parameters. This is more naturally framed as corecursion, building up from the initial values, while tracking two successive values at each step – see\ncorecursion: examples\n. A more sophisticated example involves using a\nthreaded binary tree\n, which allows iterative tree traversal, rather than multiple recursion.\nMost basic examples of recursion, and most of the examples presented here, demonstrate\n, in which a function calls itself.\nrecursion occurs when a function is called not by itself but by another function that it called (either directly or indirectly). For example, if\nthat is direct recursion, but if\nthen that is indirect recursion of\nChains of three or more functions are possible; for example, function 1 calls function 2, function 2 calls function 3, and function 3 calls function 1 again.\nIndirect recursion is also called\n, which is a more symmetric term, though this is simply a difference of emphasis, not a different notion. That is, if\nagain, from the point of view of\nis indirectly recursing, while from the point of view of\nalone, it is indirectly recursing, while from the point of view of both,\nare mutually recursing on each other. Similarly a set of three or more functions that call each other can be called a set of mutually recursive functions.\nRecursion is usually done by explicitly calling a function by name. However, recursion can also be done via implicitly calling a function based on the current context, which is particularly useful for\nStructural versus generative recursion\nStructural recursion\nSome authors classify recursion as either \"structural\" or \"generative\". The distinction is related to where a recursive procedure gets the data that it works on, and how it processes that data:\n[Functions that consume structured data] typically decompose their arguments into their immediate structural components and then process those components. If one of the immediate components belongs to the same class of data as the input, the function is recursive. For that reason, we refer to these functions as (STRUCTURALLY) RECURSIVE FUNCTIONS.\n— Felleisen, Findler, Flatt, and Krishnaurthi,\nHow to Design Programs\nThus, the defining characteristic of a structurally recursive function is that the argument to each recursive call is the content of a field of the original input. Structural recursion includes nearly all tree traversals, including XML processing, binary tree creation and search, etc. By considering the algebraic structure of the natural numbers (that is, a natural number is either zero or the successor of a natural number), functions such as factorial may also be regarded as structural recursion.\nGenerative recursion\nMany well-known recursive algorithms generate an entirely new piece of data from the given data and recur on it.\nHtDP ( How to Design Programs )\nrefers to this kind as generative recursion. Examples of generative recursion include:\nadaptive integration\n— Matthias Felleisen,\nAdvanced Functional Programming\nThis distinction is important in\nAll structurally recursive functions on finite (\n) data structures can easily be shown to terminate, via\nstructural induction\n: intuitively, each recursive call receives a smaller piece of input data, until a base case is reached.\nGeneratively recursive functions, in contrast, do not necessarily feed smaller input to their recursive calls, so proof of their termination is not necessarily as simple, and avoiding\nrequires greater care. These generatively recursive functions can often be interpreted as corecursive functions – each step generates the new data, such as successive approximation in Newton's method – and terminating this corecursion requires that the data eventually satisfy some condition, which is not necessarily guaranteed.\n, structural recursion is when there is an obvious loop variant, namely size or complexity, which starts off finite and decreases at each recursive step.\nBy contrast, generative recursion is when there is not such an obvious loop variant, and termination depends on a function, such as \"error of approximation\" that does not necessarily decrease to zero, and thus termination is not guaranteed without further analysis.\nImplementation issues\nIn actual implementation, rather than a pure recursive function (single check for base case, otherwise recursive step), a number of modifications may be made, for purposes of clarity or efficiency. These include:\nWrapper function (at top)\nShort-circuiting the base case, aka \"Arm's-length recursion\" (at bottom)\nHybrid algorithm (at bottom) – switching to a different algorithm once data is small enough\nOn the basis of elegance, wrapper functions are generally approved, while short-circuiting the base case is frowned upon, particularly in academia. Hybrid algorithms are often used for efficiency, to reduce the overhead of recursion in small cases, and arm's-length recursion is a special case of this.\nis a function that is directly called but does not recurse itself, instead calling a separate auxiliary function which actually does the recursion.\nWrapper functions can be used to validate parameters (so the recursive function can skip these), perform initialization (allocate memory, initialize variables), particularly for auxiliary variables such as \"level of recursion\" or partial computations for\n, and handle exceptions and errors. In languages that support\n, the auxiliary function can be nested inside the wrapper function and use a shared scope. In the absence of nested functions, auxiliary functions are instead a separate function, if possible private (as they are not called directly), and information is shared with the wrapper function by using\nShort-circuiting the base case\n[ edit ] Factorial: ordinary vs. short-circuit\nShort-circuit recursion\nint fac1 ( int n ) { if ( n \u003c= 0 ) return 1 ; else return fac1 ( n -1 ) * n ; } int fac2 ( int n ) { // assert(n \u003e= 2); if ( n == 2 ) return 2 ; else return fac2 ( n -1 ) * n ; } int fac2wrapper ( int n ) { if ( n \u003c= 1 ) return 1 ; else return fac2 ( n ); }\nShort-circuiting the base case, also known as\narm's-length recursion\n, consists of checking the base case\nmaking a recursive call – i.e., checking if the next call will be the base case, instead of calling and then checking for the base case. Short-circuiting is particularly done for efficiency reasons, to avoid the overhead of a function call that immediately returns. Note that since the base case has already been checked for (immediately before the recursive step), it does not need to be checked for separately, but one does need to use a wrapper function for the case when the overall recursion starts with the base case itself. For example, in the factorial function, properly the base case is 0! = 1, while immediately returning 1 for 1! is a short circuit, and may miss 0; this can be mitigated by a wrapper function. The box shows\ncode to shortcut factorial cases 0 and 1.\nShort-circuiting is primarily a concern when many base cases are encountered, such as Null pointers in a tree, which can be linear in the number of function calls, hence significant savings for\nalgorithms; this is illustrated below for a depth-first search. Short-circuiting on a tree corresponds to considering a leaf (non-empty node with no children) as the base case, rather than considering an empty node as the base case. If there is only a single base case, such as in computing the factorial, short-circuiting provides only\nConceptually, short-circuiting can be considered to either have the same base case and recursive step, checking the base case only before the recursion, or it can be considered to have a different base case (one step removed from standard base case) and a more complex recursive step, namely \"check valid then recurse\", as in considering leaf nodes rather than Null nodes as base cases in a tree. Because short-circuiting has a more complicated flow, compared with the clear separation of base case and recursive step in standard recursion, it is often considered poor style, particularly in academia.\nA basic example of short-circuiting is given in\n(DFS) of a binary tree; see\nsection for standard recursive discussion.\nThe standard recursive algorithm for a DFS is:\nbase case: If current node is Null, return false\nrecursive step: otherwise, check value of current node, return true if match, otherwise recurse on children\nIn short-circuiting, this is instead:\ncheck value of current node, return true if match,\notherwise, on children, if not Null, then recurse.\nIn terms of the standard steps, this moves the base case check\nthe recursive step. Alternatively, these can be considered a different form of base case and recursive step, respectively. Note that this requires a wrapper function to handle the case when the tree itself is empty (root node is Null).\nNull pointers as children (2 for each of the 2\nleaves), so short-circuiting cuts the number of function calls in half in the worst case.\nIn C, the standard recursive algorithm may be implemented as:\nbool tree_contains ( struct node * tree_node , int i ) { if ( tree_node == NULL ) return false ; // base case else if ( tree_node -\u003e data == i ) return true ; else return tree_contains ( tree_node -\u003e left , i ) || tree_contains ( tree_node -\u003e right , i ); }\nThe short-circuited algorithm may be implemented as:\n// Wrapper function to handle empty tree bool tree_contains ( struct node * tree_node , int i ) { if ( tree_node == NULL ) return false ; // empty tree else return tree_contains_do ( tree_node , i ); // call auxiliary function } // Assumes tree_node != NULL bool tree_contains_do ( struct node * tree_node , int i ) { if ( tree_node -\u003e data == i ) return true ; // found else // recurse return ( tree_node -\u003e left \u0026\u0026 tree_contains_do ( tree_node -\u003e left , i )) || ( tree_node -\u003e right \u0026\u0026 tree_contains_do ( tree_node -\u003e right , i )); }\nshort-circuit evaluation\nof the Boolean \u0026\u0026 (AND) operators, so that the recursive call is made only if the node is valid (non-Null). Note that while the first term in the AND is a pointer to a node, the second term is a Boolean, so the overall expression evaluates to a Boolean. This is a common idiom in recursive short-circuiting. This is in addition to the short-circuit evaluation of the Boolean || (OR) operator, to only check the right child if the left child fails. In fact, the entire\nof these functions can be replaced with a single Boolean expression in a return statement, but legibility suffers at no benefit to efficiency.\nRecursive algorithms are often inefficient for small data, due to the overhead of repeated function calls and returns. For this reason efficient implementations of recursive algorithms often start with the recursive algorithm, but then switch to a different algorithm when the input becomes small. An important example is\n, which is often implemented by switching to the non-recursive\nwhen the data is sufficiently small, as in the\n. Hybrid recursive algorithms can often be further refined, as in\n, derived from a hybrid merge sort/insertion sort.\nRecursion versus iteration\nare equally expressive: recursion can be replaced by iteration with an explicit\n, while iteration can be replaced with\n. Which approach is preferable depends on the problem under consideration and the language used. In\nimperative programming\n, iteration is preferred, particularly for simple recursion, as it avoids the overhead of function calls and call stack management, but recursion is generally used for multiple recursion. By contrast, in\nfunctional languages\nrecursion is preferred, with tail recursion optimization leading to little overhead. Implementing an algorithm using iteration may not be easily achievable.\nCompare the templates to compute x\nf(n, recursive(n - 1))\nFor an imperative language the overhead is to define the function, and for a functional language the overhead is to define the accumulator variable x.\nfunction may be implemented iteratively in\nby assigning to a loop index variable and accumulator variable, rather than by passing arguments and returning values by recursion:\nunsigned int factorial ( unsigned int n ) { unsigned int product = 1 ; // empty product is 1 while ( n ) { product *= n ; -- n ; } return product ; }\nprogramming languages\nin use today allow the direct specification of recursive functions and procedures. When such a function is called, the program's\nkeeps track of the various\nof the function (often using a\n, although other methods may be used). Every recursive function can be transformed into an iterative function by replacing recursive calls with\niterative control constructs\nand simulating the call stack with a\nexplicitly managed by the program.\nConversely, all iterative functions and procedures that can be evaluated by a computer (see\n) can be expressed in terms of recursive functions; iterative control constructs such as\nare routinely rewritten in recursive form in\nfunctional languages\nHowever, in practice this rewriting depends on\ntail call elimination\n, which is not a feature of all languages.\nare notable mainstream languages in which all function calls, including\n, may cause stack allocation that would not occur with the use of looping constructs; in these languages, a working iterative program rewritten in recursive form may\noverflow the call stack\n, although tail call elimination may be a feature that is not covered by a language's specification, and different implementations of the same language may differ in tail call elimination capabilities.\nIn languages (such as\n) that favor iterative looping constructs, there is usually significant time and space cost associated with recursive programs, due to the overhead required to manage the stack and the relative slowness of function calls; in\nfunctional languages\n, a function call (particularly a\n) is typically a very fast operation, and the difference is usually less noticeable.\nAs a concrete example, the difference in performance between recursive and iterative implementations of the \"factorial\" example above depends highly on the\nused. In languages where looping constructs are preferred, the iterative version may be as much as several\nfaster than the recursive one. In functional languages, the overall time difference of the two implementations may be negligible; in fact, the cost of multiplying the larger numbers first rather than the smaller numbers (which the iterative version given here happens to do) may overwhelm any time saved by choosing iteration.\nIn some programming languages, the maximum size of the\nis much less than the space available in the\n, and recursive algorithms tend to require more stack space than iterative algorithms. Consequently, these languages sometimes place a limit on the depth of recursion to avoid\nis one such language.\nNote the caveat below regarding the special case of\nBecause recursive algorithms can be subject to stack overflows, they may be vulnerable to\nSome malware specifically targets a program's call stack and takes advantage of the stack's inherently recursive nature.\nEven in the absence of malware, a stack overflow caused by unbounded recursion can be fatal to the program, and\nexception handling logic\nmay not prevent the corresponding\nMultiply recursive problems\nMultiply recursive problems are inherently recursive, because of prior state they need to track. One example is\n; though both recursive and iterative methods are used,\nthey contrast with list traversal and linear search in a list, which is a singly recursive and thus naturally iterative method. Other examples include\ndivide-and-conquer algorithms\n, and functions such as the\n. All of these algorithms can be implemented iteratively with the help of an explicit\n, but the programmer effort involved in managing the stack, and the complexity of the resulting program, arguably outweigh any advantages of the iterative solution.\nRefactoring recursion\nRecursive algorithms can be replaced with non-recursive counterparts.\nOne method for replacing recursive algorithms is to simulate them using\nAn alternative is to develop a replacement algorithm entirely based on non-recursive methods, which can be challenging.\nFor example, recursive algorithms for\nwere once typical. Non-recursive algorithms for the same purpose, such as the\nKrauss matching wildcards algorithm\n, have been developed to avoid the drawbacks of recursion\nand have improved only gradually based on techniques such as collecting\nTail-recursive functions\nTail-recursive functions are functions in which all recursive calls are\nand hence do not build up any deferred operations. For example, the gcd function (shown again below) is tail-recursive. In contrast, the factorial function (also below) is\ntail-recursive; because its recursive call is not in tail position, it builds up deferred multiplication operations that must be performed after the final recursive call completes. With a\nthat treats tail-recursive calls as\nrather than function calls, a tail-recursive function such as gcd will execute using constant space. Thus the program is essentially iterative, equivalent to using imperative language control structures like the \"for\" and \"while\" loops.\nAugmenting recursion:\n//INPUT: Integers x, y such that x \u003e= y and y \u003e= 0 int gcd ( int x , int y ) { if ( y == 0 ) return x ; else return gcd ( y , x % y ); } //INPUT: n is an Integer such that n \u003e= 0 int fact ( int n ) { if ( n == 0 ) return 1 ; else return n * fact ( n - 1 ); }\nThe significance of tail recursion is that when making a tail-recursive call (or any tail call), the caller's return position need not be saved on the\n; when the recursive call returns, it will branch directly on the previously saved return position. Therefore, in languages that recognize this property of tail calls, tail recursion saves both space and time.\nConsider these two functions:\n[ edit ] void recursiveFunction ( int num ) { printf ( \"%d \\n \" , num ); if ( num \u003c 4 ) recursiveFunction ( num + 1 ); }\n[ edit ] void recursiveFunction ( int num ) { if ( num \u003c 4 ) recursiveFunction ( num + 1 ); printf ( \"%d \\n \" , num ); }\nThe output of function 2 is that of function 1 with the lines swapped.\nIn the case of a function calling itself only once, instructions placed before the recursive call are executed once per recursion before any of the instructions placed after the recursive call. The latter are executed repeatedly after the maximum recursion has been reached.\nof the print statements is reversed, which is due to the way the functions and statements are stored on the\nRecursive procedures\nA classic example of a recursive procedure is the function used to calculate the\nfact ⁡ ( n ) = { 1 if n = 0 n ⋅ fact ⁡ ( n − 1 ) if n \u003e 0 {\\displaystyle \\operatorname {fact} (n)={\\begin{cases}1\u0026{\\mbox{if }}n=0\\\\n\\cdot \\operatorname {fact} (n-1)\u0026{\\mbox{if }}n\u003e0\\\\\\end{cases}}} Pseudocode\nThe function can also be written as a\nb n = n b n − 1 {\\displaystyle b_{n}=nb_{n-1}} b 0 = 1 {\\displaystyle b_{0}=1}\nThis evaluation of the recurrence relation demonstrates the computation that would be performed in evaluating the pseudocode above:\nComputing the recurrence relation for n = 4:\n= 4 × (3 × (2 × b\n= 4 × (3 × (2 × (1 × b\n= 4 × (3 × (2 × (1 × 1)))\n= 4 × (3 × (2 × 1))\nThis factorial function can also be described without using recursion by making use of the typical looping constructs found in imperative programming languages:\nPseudocode (iterative):\nreturn running_total end\nThe imperative code above is equivalent to this mathematical definition using an accumulator variable\nfact ⁡ ( n ) = f a c t a c c ⁡ ( n , 1 ) f a c t a c c ⁡ ( n , t ) = { t if n = 0 f a c t a c c ⁡ ( n − 1 , n t ) if n \u003e 0 {\\displaystyle {\\begin{aligned}\\operatorname {fact} (n)\u0026=\\operatorname {fact_{acc}} (n,1)\\\\\\operatorname {fact_{acc}} (n,t)\u0026={\\begin{cases}t\u0026{\\mbox{if }}n=0\\\\\\operatorname {fact_{acc}} (n-1,nt)\u0026{\\mbox{if }}n\u003e0\\\\\\end{cases}}\\end{aligned}}}\nThe definition above translates straightforwardly to\nfunctional programming languages\n; this is an example of iteration implemented recursively.\nGreatest common divisor\n, which computes the\ngreatest common divisor\nof two integers, can be written recursively.\ngcd ( x , y ) = { x if y = 0 gcd ( y , remainder ⁡ ( x , y ) ) if y \u003e 0 {\\displaystyle \\gcd(x,y)={\\begin{cases}x\u0026{\\mbox{if }}y=0\\\\\\gcd(y,\\operatorname {remainder} (x,y))\u0026{\\mbox{if }}y\u003e0\\\\\\end{cases}}} Pseudocode\nfor greatest common divisor, where\nx % y {\\displaystyle x\\%y}\nx / y {\\displaystyle x/y}\ngcd ( x , y ) = gcd ( y , x % y ) {\\displaystyle \\gcd(x,y)=\\gcd(y,x\\%y)}\ny ≠ 0 {\\displaystyle y\\neq 0} gcd ( x , 0 ) = x {\\displaystyle \\gcd(x,0)=x}\nComputing the recurrence relation for x = 27 and y = 9:\ngcd(27, 9) = gcd(9, 27 % 9)\nComputing the recurrence relation for x = 111 and y = 259:\ngcd(111, 259) = gcd(259, 111 % 259)\n= gcd(111, 259 % 111)\nThe recursive program above is\n; it is equivalent to an iterative algorithm, and the computation shown above shows the steps of evaluation that would be performed by a language that eliminates tail calls. Below is a version of the same algorithm using explicit iteration, suitable for a language that does not eliminate tail calls. By maintaining its state entirely in the variables\nand using a looping construct, the program avoids making recursive calls and growing the call stack.\nPseudocode (iterative):\nto the remainder of x/y\nThe iterative algorithm requires a temporary variable, and even given knowledge of the Euclidean algorithm it is more difficult to understand the process by simple inspection, although the two algorithms are very similar in their steps.\n[ edit ] Towers of Hanoi\nThe Towers of Hanoi is a mathematical puzzle whose solution illustrates recursion.\nThere are three pegs which can hold stacks of disks of different diameters. A larger disk may never be stacked on top of a smaller. Starting with\ndisks on one peg, they must be moved to another peg one at a time. What is the smallest number of steps to move the stack?\nhanoi ⁡ ( n ) = { 1 if n = 1 2 ⋅ hanoi ⁡ ( n − 1 ) + 1 if n \u003e 1 {\\displaystyle \\operatorname {hanoi} (n)={\\begin{cases}1\u0026{\\mbox{if }}n=1\\\\2\\cdot \\operatorname {hanoi} (n-1)+1\u0026{\\mbox{if }}n\u003e1\\\\\\end{cases}}} Recurrence relation for hanoi\nh n = 2 h n − 1 + 1 {\\displaystyle h_{n}=2h_{n-1}+1} h 1 = 1 {\\displaystyle h_{1}=1}\nComputing the recurrence relation for n = 4:\nhanoi(4) = 2×hanoi(3) + 1\n= 2×(2×hanoi(2) + 1) + 1\n= 2×(2×(2×hanoi(1) + 1) + 1) + 1\n= 2×(2×(2×1 + 1) + 1) + 1\n= 2×(2×(3) + 1) + 1\nExample implementations:\nAlthough not all recursive functions have an explicit solution, the Tower of Hanoi sequence can be reduced to an explicit formula.\nAn explicit formula for Towers of Hanoi:\nalgorithm is a method of searching a\nfor a single element by cutting the array in half with each recursive pass. The trick is to pick a midpoint near the center of the array, compare the data at that point with the data being searched and then responding to one of three possible conditions: the data is found at the midpoint, the data at the midpoint is greater than the data being searched for, or the data at the midpoint is less than the data being searched for.\nRecursion is used in this algorithm because with each pass a new array is created by cutting the old one in half. The binary search procedure is then called recursively, this time on the new (and smaller) array. Typically the array's size is adjusted by manipulating a beginning and ending index. The algorithm exhibits a logarithmic order of growth because it essentially divides the problem domain in half with each pass.\nExample implementation of binary search in C:\n/* Call binary_search with proper initial conditions. INPUT: data is an array of integers SORTED in ASCENDING order, toFind is the integer to search for, count is the total number of elements in the array OUTPUT: result of binary_search */ int search ( int * data , int toFind , int count ) { // Start = 0 (beginning index) // End = count - 1 (top index) return binary_search ( data , toFind , 0 , count -1 ); } /* Binary Search Algorithm. INPUT: data is a array of integers SORTED in ASCENDING order, toFind is the integer to search for, start is the minimum array index, end is the maximum array index OUTPUT: position of the integer toFind within array data, -1 if not found */ int binary_search ( int * data , int toFind , int start , int end ) { //Get the midpoint. int mid = start + ( end - start ) / 2 ; //Integer division if ( start \u003e end ) //Stop condition (base case) return -1 ; else if ( data [ mid ] == toFind ) //Found, return index return mid ; else if ( data [ mid ] \u003e toFind ) //Data is greater than toFind, search lower half return binary_search ( data , toFind , start , mid -1 ); else //Data is less than toFind, search upper half return binary_search ( data , toFind , mid + 1 , end ); }\nRecursive data structures (structural recursion)\nAn important application of recursion in computer science is in defining dynamic data structures such as\n. Recursive data structures can dynamically grow to a theoretically infinite size in response to runtime requirements; in contrast, the size of a static array must be set at compile time.\n\"Recursive algorithms are particularly appropriate when the underlying problem or the data to be treated are defined in recursive terms.\"\nThe examples in this section illustrate what is known as \"structural recursion\". This term refers to the fact that the recursive procedures are acting on data that is defined recursively.\nAs long as a programmer derives the template from a data definition, functions employ structural recursion. That is, the recursions in a function's body consume some immediate piece of a given compound value.\nBelow is a C definition of a linked list node structure. Notice especially how the node is defined in terms of itself. The \"next\" element of\nis a pointer to another\n, effectively creating a list type.\nstruct node { int data ; // some integer data struct node * next ; // pointer to another struct node };\ndata structure is defined recursively, procedures that operate on it can be implemented naturally as recursive procedures. The\nprocedure defined below walks down the list until the list is empty (i.e., the list pointer has a value of NULL). For each node it prints the data element (an integer). In the C implementation, the list remains unchanged by the\nvoid list_print ( struct node * list ) { if ( list != NULL ) // base case { printf ( \"%d \" , list -\u003e data ); // print integer data followed by a space list_print ( list -\u003e next ); // recursive call on the next node } }\nBelow is a simple definition for a binary tree node. Like the node for linked lists, it is defined in terms of itself, recursively. There are two self-referential pointers: left (pointing to the left sub-tree) and right (pointing to the right sub-tree).\nstruct node { int data ; // some integer data struct node * left ; // pointer to the left subtree struct node * right ; // point to the right subtree };\nOperations on the tree can be implemented using recursion. Note that because there are two self-referencing pointers (left and right), tree operations may require two recursive calls:\n// Test if tree_node contains i; return 1 if so, 0 if not. int tree_contains ( struct node * tree_node , int i ) { if ( tree_node == NULL ) return 0 ; // base case else if ( tree_node -\u003e data == i ) return 1 ; else return tree_contains ( tree_node -\u003e left , i ) || tree_contains ( tree_node -\u003e right , i ); }\nAt most two recursive calls will be made for any given call to\n// Inorder traversal: void tree_print ( struct node * tree_node ) { if ( tree_node != NULL ) { // base case tree_print ( tree_node -\u003e left ); // go left printf ( \"%d \" , tree_node -\u003e data ); // print the integer followed by a space tree_print ( tree_node -\u003e right ); // go right } }\nThe above example illustrates an\nof the binary tree. A\nis a special case of the binary tree where the data elements of each node are in order.\nFilesystem traversal\nSince the number of files in a\nis the only practical way to traverse and thus enumerate its contents. Traversing a filesystem is very similar to that of\n, therefore the concepts behind tree traversal are applicable to traversing a filesystem. More specifically, the code below would be an example of a\nimport java.io.File ; public class FileSystem { public static void main ( String [] args ) { traverse (); } /** * Obtains the filesystem roots * Proceeds with the recursive filesystem traversal */ private static void traverse () { File [] fs = File . listRoots (); for ( int i = 0 ; i \u003c fs . length ; i ++ ) { System . out . println ( fs [ i ] ); if ( fs [ i ] . isDirectory () \u0026\u0026 fs [ i ] . canRead ()) { rtraverse ( fs [ i ] ); } } } /** * Recursively traverse a given directory * * @param fd indicates the starting point of traversal */ private static void rtraverse ( File fd ) { File [] fss = fd . listFiles (); for ( int i = 0 ; i \u003c fss . length ; i ++ ) { System . out . println ( fss [ i ] ); if ( fss [ i ] . isDirectory () \u0026\u0026 fss [ i ] . canRead ()) { rtraverse ( fss [ i ] ); } } } }\nThis code is both recursion and\n- the files and directories are iterated, and each directory is opened recursively.\nThe \"rtraverse\" method is an example of direct recursion, whilst the \"traverse\" method is a wrapper function.\nThe \"base case\" scenario is that there will always be a fixed number of files and/or directories in a given filesystem.\nTime-efficiency of recursive algorithms\nof recursive algorithms can be expressed in a\n. They can (usually) then be simplified into a single Big-O term.\nShortcut rule (master theorem)\nMaster theorem (analysis of algorithms)\nIf the time-complexity of the function is in the form\nT ( n ) = a ⋅ T ( n / b ) + f ( n ) {\\displaystyle T(n)=a\\cdot T(n/b)+f(n)}\nThen the Big O of the time-complexity is thus:\nf ( n ) = O ( n log b ⁡ a − ε ) {\\displaystyle f(n)=O(n^{\\log _{b}a-\\varepsilon })}\nε \u003e 0 {\\displaystyle \\varepsilon \u003e0}\nT ( n ) = Θ ( n log b ⁡ a ) {\\displaystyle T(n)=\\Theta (n^{\\log _{b}a})}\nf ( n ) = Θ ( n log b ⁡ a ) {\\displaystyle f(n)=\\Theta (n^{\\log _{b}a})}\nT ( n ) = Θ ( n log b ⁡ a log ⁡ n ) {\\displaystyle T(n)=\\Theta (n^{\\log _{b}a}\\log n)}\nf ( n ) = Ω ( n log b ⁡ a + ε ) {\\displaystyle f(n)=\\Omega (n^{\\log _{b}a+\\varepsilon })}\nε \u003e 0 {\\displaystyle \\varepsilon \u003e0}\na ⋅ f ( n / b ) ≤ c ⋅ f ( n ) {\\displaystyle a\\cdot f(n/b)\\leq c\\cdot f(n)}\nand all sufficiently large\nT ( n ) = Θ ( f ( n ) ) {\\displaystyle T(n)=\\Theta (f(n))}\nrepresents the number of recursive calls at each level of recursion,\nrepresents by what factor smaller the input is for the next level of recursion (i.e. the number of pieces you divide the problem into), and\nrepresents the work that the function does independently of any recursion (e.g. partitioning, recombining) at each level of recursion.\nRecursion in Logic Programming\nIn the procedural interpretation of\n, clauses (or rules) of the form\nare treated as procedures, which reduce goals of the form\nto subgoals of the form\npath ( X , Y ) :- arc ( X , Y ). path ( X , Y ) :- arc ( X , Z ), path ( Z , Y ).\ndefine a procedure, which can be used to search for a path from\n, either by finding a direct arc from\n, or by finding an arc from\n, and then searching recursively for a path from\n. Prolog executes the procedure by reasoning top-down (or\n) and searching the space of possible paths depth-first, one branch at a time. If it tries the second clause, and finitely fails to find a path from\n, it backtracks and tries to find an arc from\nto another node, and then searches for a path from that other node to\nHowever, in the logical reading of logic programs, clauses are understood\nas universally quantified conditionals. For example, the recursive clause of the path-finding procedure is understood as\nrepresenting the knowledge\n, if there is an arc from\nthen there is a path from\n∀ X , Y , Z ( a r c ( X , Z ) ∧ p a t h ( Z , Y ) → p a t h ( X , Y ) ) . {\\displaystyle \\forall X,Y,Z(arc(X,Z)\\land path(Z,Y)\\rightarrow path(X,Y)).}\nThe logical reading frees the reader from needing to know how the clause is used to solve problems. The clause can be used top-down, as in Prolog, to reduce problems to subproblems. Or it can be used bottom-up (or\n, to derive conclusions from conditions. This\nseparation of concerns\n, which separates declarative knowledge from problem solving methods (see\nAlgorithm#Algorithm = Logic + Control\nInfinite loop § Infinite recursion\nA common mistake among programmers is not providing a way to exit a recursive function, often by omitting or incorrectly checking the base case, letting it run (at least theoretically) infinitely by endlessly calling itself recursively. This is called\n, and the program will never terminate. In practice, this typically exhausts the available\nspace. In most programming environments, a program with infinite recursion will not really run forever. Eventually, something will break and the program will report an error.\nBelow is a Java code that would use infinite recursion:\npublic class InfiniteRecursion { static void recursive () { // Recursive Function with no way out recursive (); } public static void main ( String [] args ) { recursive (); // Executes the recursive function upon runtime } }\nRunning this code will result in a\n[ edit ] Functional programming Computational problem Hierarchical and recursive queries in SQL Kleene–Rosser paradox Open recursion Recursion\nSierpiński curve McCarthy 91 function μ-recursive functions Primitive recursive functions Tak (function) Logic programming\n[ edit ] ^ Graham, Ronald; Knuth, Donald; Patashnik, Oren (1990). \"1: Recurrent Problems\" . Concrete Mathematics . Addison-Wesley. ISBN 0-201-55802-5 . ^ Kuhail, M. A.; Negreiros, J.; Seffah, A. (2021). \"Teaching Recursive Thinking using Unplugged Activities\" (PDF) . World Transactions on Engineering and Technology Education . 19 : 169– 175. ^ Epp, Susanna (1995). Discrete Mathematics with Applications (2nd ed.). PWS Publishing Company. p. 427 . ISBN 978-0-53494446-9 . ^ Wirth, Niklaus (1976). Algorithms + Data Structures = Programs . Prentice-Hall . p. 126 . ISBN 978-0-13022418-7 . ^ \"Functional Programming | Clojure for the Brave and True\" . www.braveclojure.com . Retrieved 2020-10-21 . ^ Felleisen et al. 2001 , art V \"Generative Recursion ^ a b Felleisen, Matthias (2002). \"Developing Interactive Web Programs\" . In Jeuring, Johan (ed.). Advanced Functional Programming: 4th International School (PDF) . Springer. p. 108. ISBN 9783540448334 . ^ Mongan, John; Giguère, Eric; Kindler, Noah (2013). Programming Interviews Exposed: Secrets to Landing Your Next Job (3rd ed.). Wiley . p. 115 . ISBN 978-1-118-26136-1 . ^ Hetland, Magnus Lie (2010), Python Algorithms: Mastering Basic Algorithms in the Python Language , Apress, p. 79, ISBN 9781430232384 . ^ Drozdek, Adam (2012), Data Structures and Algorithms in C++ (4th ed.), Cengage Learning, p. 197, ISBN 9781285415017 . ^ Shivers, Olin. \"The Anatomy of a Loop - A story of scope and control\" (PDF) . Georgia Institute of Technology . Retrieved 2012-09-03 . ^ Lambda the Ultimate. \"The Anatomy of a Loop\" . Lambda the Ultimate . Retrieved 2012-09-03 . ^ \"27.1. sys — System-specific parameters and functions — Python v2.7.3 documentation\" . Docs.python.org . Retrieved 2012-09-03 . ^ Krauss, Kirk J. (2014). \"Matching Wildcards: An Empirical Way to Tame an Algorithm\" . Dr. Dobb's Journal . ^ Mueller, Oliver (2012). \"Anatomy of a Stack Smashing Attack and How GCC Prevents It\" . Dr. Dobb's Journal . ^ \"StackOverflowException Class\" . .NET Framework Class Library . Microsoft Developer Network . 2018. ^ \"Depth First Search (DFS): Iterative and Recursive Implementation\" . Techie Delight. 2018. ^ Mitrovic, Ivan. \"Replace Recursion with Iteration\" . ThoughtWorks . ^ La, Woong Gyu (2015). \"How to replace recursive functions using stack and while-loop to avoid the stack-overflow\" . CodeProject. ^ Moertel, Tom (2013). \"Tricks of the trade: Recursion to Iteration, Part 2: Eliminating Recursion with the Time-Traveling Secret Feature Trick\" . ^ Salz, Rich (1991). \"wildmat.c\" . GitHub . ^ Krauss, Kirk J. (2008). \"Matching Wildcards: An Algorithm\" . Dr. Dobb's Journal . ^ Krauss, Kirk J. (2018). \"Matching Wildcards: An Improved Algorithm for Big Data\" . Develop for Performance. ^ Graham, Knuth \u0026 Patashnik 1990 , §1.1: The Tower of Hanoi ^ Epp 1995 , pp. 427–430: The Tower of Hanoi ^ Epp 1995 , pp. 447–448: An Explicit Formula for the Tower of Hanoi Sequence ^ Wirth 1976 , p. 127 ^ Russell, Stuart J. ; Norvig, Peter. (2021). Artificial Intelligence: A Modern Approach §9.3, §9.4 (4th ed.). Hoboken: Pearson. ISBN 978-0134610993 . LCCN 20190474 . ^ \"4.8. Infinite Recursion\" .\n[ edit ] Barron, David William (1968) [1967]. Written at Cambridge, UK. Gill, Stanley (ed.). Recursive techniques in programming . Macdonald Computer Monographs (1 ed.). London, UK: Macdonald \u0026 Co. (Publishers) Ltd. SBN 356-02201-3 .\nFelleisen, Matthias; Findler, Robert B.; Flatt, Matthew; Krishnamurthi, Shriram (2001). How To Design Programs: An Introduction to Computing and Programming . MIT Press . ISBN 0262062186 . Rubio-Sanchez, Manuel (2017). Introduction to Recursive Programming . CRC Press . ISBN 978-1-351-64717-5 . Pevac, Irena (2016). Practicing Recursion in Java . CreateSpace Independent. ISBN 978-1-5327-1227-2 . Roberts, Eric (2005). Thinking Recursively with Java . Wiley . ISBN 978-0-47170146-0 . Rohl, Jeffrey S. (1984). Recursion Via Pascal . Cambridge University Press . ISBN 978-0-521-26934-6 . Helman, Paul; Veroff, Robert. Walls and Mirrors . Abelson, Harold ; Sussman, Gerald Jay ; Sussman, Julie (1996). Structure and Interpretation of Computer Programs (2nd ed.). MIT Press . ISBN 0-262-51087-1 . Dijkstra, Edsger W. (1960). \"Recursive Programming\". Numerische Mathematik . 2 (1): 312– 318. doi : 10.1007/BF01386232 . S2CID 127891023 . v t e Programming paradigms\nComparison by language\nImperative Structured Jackson structures Block-structured Modular Non-structured Procedural Programming in the large and in the small Design by contract Invariant-based Nested function Object-oriented\nObject-based Agent Immutable object Persistent Uniform function call syntax Declarative Functional\nRecursive Anonymous function\nHigher-order Purely functional Total Strict GADTs Dependent types Functional logic Point-free style Expression-oriented Applicative\nConcatenative Function-level\nValue-level Dataflow Flow-based Reactive\nSignals Streams Synchronous Logic Abductive logic Answer set Constraint\nInductive logic Nondeterministic Ontology Probabilistic logic Query DSL Algebraic modeling Array Automata-based\nDifferentiable End-user Grammar-oriented Interface description Language-oriented List comprehension Low-code Modeling Natural language Non-English-based Page description Pipes\nfilters Probabilistic Quantum Scientific Scripting Set-theoretic Simulation Stack-based System Tactile Templating Transformation\nparallel Actor-based Automatic mutual exclusion Choreographic programming Concurrent logic\nConcurrent constraint logic\nConcurrent OO Macroprogramming Multitier programming Organic computing Parallel programming models Partitioned global address space Process-oriented Relativistic programming Service-oriented Structured concurrency Metaprogramming Attribute-oriented Automatic\nDynamic Extensible Generic Homoiconicity Interactive Macro\nMetalinguistic abstraction Multi-stage Program synthesis\nReflective Self-modifying code Symbolic Template Separation of concerns Aspects Components Data-driven Data-oriented Event-driven Features Literate Roles Subjects v t e Data structures\nArray Associative array Binary search tree Fenwick tree Graph Hash table Heap Linked list Queue Segment tree Stack String Tree Trie\nalgorithmic paradigms Backtracking Binary search Breadth-first search Brute-force search Depth-first search Divide and conquer Dynamic programming Graph traversal Fold Greedy Hash function Minimax Online Randomized Recursion Root-finding Sorting Streaming Sweep line String-searching Topological sorting List of data structures List of algorithms\nhttps://en.wikipedia.org/w/index.php?title=Recursion_(computer_science)\u0026oldid=1301600240\nTheoretical computer science Recursion Computability theory Programming idioms Subroutines\nArticles with short description Short description is different from Wikidata Use dmy dates from March 2020 Wikipedia articles needing clarification from September 2020 CS1 location test Articles with example pseudocode Pages that use a deprecated format of the math tags Search Search Recursion (computer science) 28 languages Add topic",
          "role": "teacher"
        },
        {
          "content": "Why is this important to understand?",
          "role": "student"
        },
        {
          "content": "Understanding Recursion is crucial for building efficient, scalable, and maintainable software systems. It forms the foundation for solving complex computational problems.",
          "role": "teacher"
        },
        {
          "content": "Can you give me a concrete example?",
          "role": "student"
        },
        {
          "content": "For example, Recursion can be applied in scenarios where you need to optimize performance, manage complexity, or implement scalable solutions.",
          "role": "teacher"
        }
      ],
      "format": "socratic_dialogue",
      "metadata": {
        "domain": "CS",
        "source": "https://en.wikipedia.org/wiki/Recursion_(computer_science)",
        "words": 9497
      },
      "topic": "Recursion"
    },
    {
      "content": {
        "applications": "application of recursion in computer science is in defining dynamic data structures such as\n. Recursive data structures can dynamically grow to a theoretically infinite size in response to runtime requirements; in contrast, the size of a static array must be set at compile time.\n\"Recursive algorithms are particularly appropriate when the underlying problem or the data to be treated are defined in recursive terms.\"\nThe examples in this section illustrate what is known as \"structural recursion\". T",
        "considerations": "considering the algebraic structure of the natural numbers (that is, a natural number is either zero or the successor of a natural number), functions such as factorial may also be regarded as structural recursion.\nGenerative recursion\nMany well-known recursive algorithms generate an entirely new piece of data from the given data and recur on it.\nHtDP ( How to Design Programs )\nrefers to this kind ",
        "key_concepts": [
          "the files and directories are iterated, and each directory is opened recursively."
        ],
        "overview": "A comprehensive technical concept with wide applications."
      },
      "format": "conceptual_breakdown",
      "metadata": {
        "domain": "CS",
        "structure": "hierarchical"
      },
      "topic": "Recursion"
    },
    {
      "format": "problem_solving",
      "scenario": {
        "approach": "solving a\ncomputational problem\nwhere the solution depends on solutions to smaller instances of the same problem.\nRecursion solves such\nthat call themselves from within their own code. The approach can be applied to many types of problems, and recursion is one of the central ideas of computer science.\nThe power of recursion evidently lies in the possibility of defining an infinite set of objects by a finite statement. In the same manner, an infinite number of computations can be described by a f",
        "benefits": "benefit to efficiency.\nRecursive algorithms are often inefficient for small data, due to the overhead of repeated function calls and returns. For this reason efficient implementations of recursive algorithms often start with the recursive algorithm, but then switch to a different algorithm when the ",
        "problem": "How can Recursion solve real-world problems?",
        "tradeoffs": "cost associated with recursive programs, due to the overhead required to manage the stack and the relative slowness of function calls; in\nfunctional languages\n, a function call (particularly a\n) is typically a very fast operation, and the difference is usually less noticeable.\nAs a concrete example,"
      },
      "topic": "Recursion"
    },
    {
      "comparison": "compared with the clear separation of base case and recursive step in standard recursion, it is often considered poor style, particularly in academia.\nA basic example of short-circuiting is given in\n(DFS) of a binary tree; see\nsection for standard recursive discussion.\nThe standard recursive algorithm for a DFS is:\nbase case: If current node is Null, return false\nrecursive step: otherwise, check value of current node, return true if match, otherwise recurse on children\nIn short-circuiting, this ",
      "format": "comparative_analysis",
      "topic": "Recursion"
    },
    {
      "dialogue": [
        {
          "content": "What is OOP really about at its core?",
          "role": "student"
        },
        {
          "content": "Programming paradigm based on objects\nUML notation for a class. This Button class has variables for data, and functions . Through inheritance, a subclass can be created as a subset of the Button class. Objects are instances of a class. Object-oriented programming\nprogramming paradigm\nconsists of objects that interact with one another.\nprogramming language\nthat provides OOP features is classified as an\nbut as the set of features that contribute to OOP is contended, classifying a language as OOP and the degree to which it supports or is OOP, are debatable. As paradigms are not mutually exclusive, a language can be\n; can be categorized as more than only OOP.\nSometimes, objects represent real-world things and processes in digital form.\nFor example, a graphics program may have objects such as\n. An online shopping system might have objects such as\nsaid, \"This paradigm [OOP] closely reflects the structure of systems in the real world and is therefore well suited to model complex systems with complex behavior\".\nHowever, more often, objects represent abstract entities, like an open file or a unit converter. Not everyone agrees that OOP makes it easy to copy the real world exactly or that doing so is even necessary.\nsuggests that because classes are software, their relationships don't match the real-world relationships they represent.\n[ 6 ] Bertrand Meyer\nargues that a program is not a model of the world but a model of some part of the world; \"Reality is a cousin twice removed\".\nlack the OOP approach of naming a thing (object) before an action (method), as opposed to\nfunctional programming\nwhich does the reverse.\nThis can make an OOP solution more complex than one written via\nprocedural programming\nNotable languages with OOP support include\nThe idea of \"objects\" in programming began with the\nartificial intelligence\nMassachusetts Institute of Technology\n(MIT) in the late 1950s and early 1960s. Here, \"object\" referred to\natoms with identified properties (attributes).\nAnother early example was\nat MIT in 1960–1961. In the glossary of his technical report, Sutherland defined terms like \"object\" and \"instance\" (with the class concept covered by \"master\" or \"definition\"), albeit specialized to graphical interaction.\nLater, in 1968, AED-0, MIT's version of the\nprogramming language, connected data structures (\"plexes\") and procedures, prefiguring what were later termed \"messages\", \"methods\", and \"member functions\".\nwere common points of discussion at this time.\nMeanwhile, in Norway,\nwas developed during the years 1961–1967.\nSimula introduced essential object-oriented ideas, such as\nSimula was used mainly by researchers involved with\n, like the movement of ships and their content through cargo ports.\nSimula is generally accepted as being the first language with the primary features and framework of an object-oriented language.\nI thought of objects being like biological cells and/or individual computers on a network, only able to communicate with messages (so messaging came at the very beginning – it took a while to see how to do messaging in a programming language efficiently enough to be useful).\nInfluenced by both MIT and Simula,\nbegan developing his own ideas in November 1966. He would go on to create\n, an influential OOP language. By 1967, Kay was already using the term \"object-oriented programming\" in conversation.\nAlthough sometimes called the \"father\" of OOP,\nKay has said his ideas differ from how OOP is commonly understood, and has implied that the computer science establishment did not adopt his notion.\nA 1976 MIT memo co-authored by\nas object-oriented languages, but does not mention Smalltalk.\nIn the 1970s, the first version of the\nprogramming language was developed at\n. Smalltalk-72 was notable for use of objects at the language level and its graphical development environment.\nSmalltalk was a fully dynamic system, allowing users to create and modify classes as they worked.\nMuch of the theory of OOP was developed in the context of Smalltalk, for example multiple inheritance.\nIn the late 1970s and 1980s, OOP rose to prominence. The\nobject-oriented Lisp was developed starting 1979, introducing\nmultiple inheritance\nhighlighted Smalltalk and OOP, introducing these ideas to a wide audience.\nLOOPS, the object system for\n-D, was influenced by Smalltalk and Flavors, and a paper about it was published in 1982.\nConference on Object-Oriented Programming, Systems, Languages, and Applications\n) was attended by 1,000 people. This conference marked the start of efforts to consolidate Lisp object systems, eventually resulting in the\nCommon Lisp Object System\n. In the 1980s, there were a few attempts to design\nprocessor architectures\nsupport for objects in\n, but these were not successful. Examples include the\nIn the mid-1980s, new object-oriented languages like\nemerged. Objective-C was developed by\n, who had used Smalltalk at\nITT Inc. Bjarne Stroustrup\nbased on his experience using Simula for his PhD thesis.\n[ 20 ] Bertrand Meyer\nproduced the first design of the\nin 1985, which focused on software quality using a\nIn the 1990s, OOP became the main way of programming, especially as more languages supported it. These included\nDelphi [ citation needed ]\n. OOP became even more popular with the rise of\ngraphical user interfaces\n, which used objects for buttons, menus and other elements. One well-known example is Apple's\n. OOP toolkits also enhanced the popularity of\nevent-driven programming\nand his colleagues created new approaches to OOP.\n(1987), included a distinctive approach to object orientation, classes, and type checking across module boundaries. Inheritance is not obvious in Wirth's design since his nomenclature looks in the opposite direction: It is called type extension and the viewpoint is from the parent down to the inheritor.\nMany programming languages that were initially developed before OOP was popular have been augmented with object-oriented features, including\nComparison of programming languages (object-oriented programming)\nList of object-oriented programming terms\nThe OOP features provided by languages varies. Below are some common features of OOP languages.\n[ 30 ] [ 31 ] [ 32 ] [ 33 ]\nComparing OOP with other styles, like\nrelational programming\n, is difficult because there isn't a clear, agreed-upon definition of OOP.\n. A field (a.k.a. attribute or property) contains information (a.k.a. state) as a\nor action) defines behavior via logic code. Encapsulation is about keeping\n[ edit ] Information hiding\nis organizing code so that it is accessible only to the code that needs it; not to the rest of the\n. The internal details of an object are hidden from the outside code, allowing for changing how an object works without affecting its interface and therefore other code. Hiding information helps prevent problems when changing the code.\nObjects act as a barrier between their internal workings and external, consuming code. Consuming code can only interact with an object via its public members.\nSome programming languages, like Java, provide information hiding via visibility key words (\nSome languages don't provide a visibility feature, but developers might follow a convention such as starting a private member name with an underscore. Intermediate levels of access also exist, such as Java's\nkeyword, (which allows access from the same class and its subclasses, but not objects of a different class), and the\nkeyword in C#, Swift, and Kotlin, which restricts access to files within the same module.\nSupporters of information hiding and data abstraction say it makes code easier to reuse and intuitively represents real-world situations.\nHowever, others argue that OOP does not enhance readability or modularity.\n[ 40 ] [ 41 ] Eric S. Raymond\nhas written that OOP languages tend to encourage thickly layered programs that destroy transparency.\nRaymond compares this unfavourably to the approach taken with\nopen/closed principle\n, which says that classes and functions should be \"open for extension, but closed for modification\".\nhas stated that OOP languages have \"extremely poor modularity properties with respect to class extension and modification\", and tend to be extremely complex.\nThe latter point is reiterated by\n, the principal inventor of\n, who is quoted as saying:\nThe problem with object-oriented languages is they've got all this implicit environment that they carry around with them. You wanted a banana but what you got was a gorilla holding the banana and the entire jungle.\nLeo Brodie says that information hiding can lead to\nwhich goes against the\ndon't repeat yourself\nrule of software development.\n, an object can contain other objects. For example, an\nobject might contain an\nobject, along with other information like\n. Composition is a \"has-a\" relationships, like \"an employee has an address\".\n[ edit ] Inheritance\nbe supported via the\n, which have differences but use similar terms like object and\nclass-based programming\n, the most common type of OOP, an object is an instance of a class. The class defines the data (variables) and methods (logic). An object is created via the\n. Elements of class may include:\n– belongs to the class itself; all objects of the class share one copy\n– belongs to an object; every object has its own version of these variables\n– refers to both the class and instance variables of a class\nClass method – can only use class variables\nInstance method – belongs to an objects; can use both instance and class variables\nClasses may inherit from other classes, creating a hierarchy of classes: a case of a subclass inheriting from a super-class. For example, an\nclass might inherit from a\nclass which endows the Employee object with the variables from\n. The subclass may add variables and methods that do not affect the super-class. Most languages also allow the subclass to override super-class methods. Some languages support\nmultiple inheritance\n, where a class can inherit from more than one class, and other languages similarly support\n. For example, a mixin called UnicodeConversionMixin might add a method unicode_to_ascii() to both a FileReader and a WebPageScraper class.\ncannot be directly instantiated as an object. It is only used as a super-class.\nOther classes are utility classes which contain only class variables and methods and are not meant to be instantiated or subclassed.\nInstead of providing a class concept, in\nprototype-based programming\n, an object is linked to another object, called its\n. In Self, an object may have multiple or no parents,\nbut in the most popular prototype-based language,\n, an object has exactly one prototype link, up to the base object whose prototype is null.\nA prototype acts as a model for new objects. For example, if you have an object\n, you can make two objects\nthat share traits of the\nprototype. Prototype-based languages also allow objects to have their own unique properties, so the\nobject might have an attribute\nSome languages, like\n, don't support inheritance.\nInstead, they encourage \"\ncomposition over inheritance\n\", where objects are built using smaller parts instead of parent-child relationships. For example, instead of inheriting from class Person, the Employee class could simply contain a Person object. This lets the Employee class control how much of Person it exposes to other parts of the program.\nis another language feature that can be used as an alternative to inheritance.\nProgrammers have different opinions on inheritance. Bjarne Stroustrup, author of C++, has stated that it is possible to do OOP without inheritance.\nhas criticized inheritance for creating complex hierarchies instead of simpler solutions.\nInheritance and behavioral subtyping\nPeople often think that if one class inherits from another, it means the subclass \"\n\" more specific version of the original class. This presumes the\nare that objects from the subclass can always replace objects from the original class without problems. This concept is known as\nbehavioral subtyping\n, more specifically the\nLiskov substitution principle\nHowever, this is often not true, especially in programming languages that allow\nobjects, objects that change after they are created. In fact,\nsubtype polymorphism\nin OOP languages cannot guarantee behavioral subtyping in most if not all contexts. For example, the\ncircle-ellipse problem\nis notoriously difficult to handle using OOP's concept of inheritance. Behavioral subtyping is undecidable in general, so it cannot be easily implemented by a compiler. Because of this, programmers must carefully design class hierarchies to avoid mistakes that the programming language itself cannot catch.\nA method my be invoked via\nsuch that the method is selected at runtime instead of compile time. If the method choice depends on more than one type of object (such as other objects passed as parameters), it's called\nDynamic dispatch works together with inheritance: if an object doesn't have the requested method, it looks up to its parent class (\n), and continues up the chain to find a matching method.\n[ edit ] Message passing\nis when the method name and its inputs are sent like a message to the object for it to act on.\n[ edit ] Polymorphism\nor subtype polymorphism, where a function can work with a specific\nand thus manipulate entities of different classes in a uniform manner.\nFor example, imagine a program has two shapes: a circle and a square. Both come from a common class called \"Shape.\" Each shape has its own way of drawing itself. With subtype polymorphism, the program doesn't need to know the type of each shape, and can simply call the \"Draw\" method for each shape. The programming language runtime will ensure the correct version of the \"Draw\" method runs for each shape. Because the details of each shape are handled inside their own classes, this makes the code simpler and more organized, enabling strong\nseparation of concerns\nAn object's methods can access the object's data. Many programming languages use a special word, like\n, to refer to the current object. In languages that support\n, a method in an object can call other methods in the same object, including itself, using this special word. This allows a method in one class to call another method defined later in a subclass, a feature known as\n[ edit ] Design patterns\nare common solutions to problems in software design. Some design patterns are especially useful for OOP, and design patterns are typically introduced in an OOP context.\nThe following are notable\nsoftware design patterns\n[ 51 ] Function object\n: Class with one main method that acts like an\n(in C++, the function operator,\n: does not change state after creation\n: can be used without restriction\n: contains other objects\n: creates other objects\n: Used to create other objects (similar to a\n: a specialized metaobject that creates new objects by copying itself\n: only instance of its class for the lifetime of the program\n: receives a stream of data as its input and transforms it into the object's output\n, an object that knows or does too much.\nGang of Four design patterns\nDesign pattern (computer science) Design Patterns: Elements of Reusable Object-Oriented Software\nis a famous book published in 1994 by four authors:\n. People often call them the \"Gang of Four\". The book talks about the strengths and weaknesses of OOP and explains 23 common ways to solve programming problems.\nThese solutions, called \"design patterns,\" are grouped into three types:\nFactory method pattern\nAbstract factory pattern\nPrototype pattern Structural patterns\nProxy pattern Behavioral patterns\nChain-of-responsibility pattern\nTemplate method pattern\nObject-orientation and databases\nObject-relational impedance mismatch\nObject-relational mapping\nrelational database management systems\n(RDBMSs) are widely used in software today. However,\nrelational databases\ndon't store objects directly, which creates a challenge when using them together. This issue is called\nobject-relational impedance mismatch\nTo solve this problem, developers use different methods, but none of them are perfect.\nOne of the most common solutions is\nobject-relational mapping\n(ORM), which helps connect object-oriented programs to relational databases. Examples of ORM tools include\nSome databases, called\n, are designed to work with OOP. However, they have not been as popular or successful as relational databases.\nDate and Darwen have proposed a theoretical foundation that uses OOP as a kind of customizable\nto support RDBMSs, but it forbids objects containing pointers to other objects.\nResponsibility- vs. data-driven design\nresponsibility-driven design\n, classes are built around what they need to do and the information they share, in the form of a contract. This is different from\n, where classes are built based on the data they need to store. According to Wirfs-Brock and Wilkerson, the originators of responsibility-driven design, responsibility-driven design is the better approach.\nSOLID and GRASP guidelines\nis a set of five rules for designing good software, created by Michael Feathers:\nSingle responsibility principle\n: A class should have only one reason to change.\nOpen/closed principle\n: Software entities should be open for extension, but closed for modification.\nLiskov substitution principle\n: Functions that use pointers or references to base classes must be able to use objects of derived classes without knowing it.\nInterface segregation principle\n: Clients should not be forced to depend upon interfaces that they do not use.\nDependency inversion principle\n: Depend upon abstractions, not concretes.\n(General Responsibility Assignment Software Patterns) is another set of software design rules, created by\n, that helps developers assign responsibilities to different parts of a program:\nCreator Principle: allows classes create objects they closely use.\nInformation Expert Principle: assigns tasks to classes with the needed information.\nLow Coupling Principle: reduces class dependencies to improve flexibility and maintainability.\nHigh Cohesion Principle: designing classes with a single, focused responsibility.\nController Principle: assigns system operations to separate classes that manage flow and interactions.\nPolymorphism: allows different classes to be used through a common interface, promoting flexibility and reuse.\nPure Fabrication Principle: create helper classes to improve design, boost cohesion, and reduce coupling.\nResearchers have tried to formally define the\npresents difficulties, particularly with the interactions between open recursion and encapsulated state. Researchers have used\nco-algebraic data types\nto incorporate essential features of OOP.\nAbadi and Cardelli defined several extensions of\nthat deal with mutable objects, allowing both\nsubtype polymorphism\nparametric polymorphism\n(generics), and were able to formally model many OOP concepts and constructs.\nAlthough far from trivial, static analysis of object-oriented programming languages such as Java is a mature field,\nwith several commercial tools.\nSome believe that OOP places too much focus on using objects rather than on\nFor example, programmer\npointed out that OOP can make programmers think more about type hierarchy than composition.\nHe has called OOP \"the\n, described OOP as overly simplistic, especially when it comes to representing real-world things that change over time.\n[ 61 ] Alexander Stepanov\nsaid that OOP tries to fit everything into a single type, which can be limiting. He argued that sometimes we need multisorted algebras: families of interfaces that span multiple types, such as in\n. Stepanov also said that calling everything an \"object\" doesn't add much understanding.\nOOP was created to make code easier to\nHowever, it was not designed to clearly show the flow of a program's instructions. That was left to the compiler. As computers began using more parallel processing and multiple\n, it became more important to understand and control how instructions flow. This is difficult to do with OOP.\n[ 65 ] [ 66 ] [ 67 ] [ 68 ]\nMany popular programming languages, like C++, Java, and Python, use OOP. In the past, OOP was widely accepted,\nbut recently, some programmers have criticized it and prefer functional programming instead.\nA study by Potok et al. found no major difference in productivity between OOP and other methods.\n, a well-known computer scientist, believes big companies like OOP because it helps manage large teams of average programmers. He argues that OOP adds structure, making it harder for one person to make serious mistakes, but at the same time restrains smart programmers.\n[ 72 ] Eric S. Raymond\nopen-source software\nadvocate, argues that OOP is not the best way to write programs.\nRichard Feldman says that, while OOP features helped some languages stay organized, their popularity comes from other reasons.\nLawrence Krubner argues that OOP doesn't offer special advantages compared to other styles, like functional programming, and can complicate coding.\n[ 74 ] Luca Cardelli\nsays that OOP is slower and takes longer to compile than procedural programming.\n[ edit ] Computer programming portal CADES Common Object Request Broker Architecture\nComparison of programming languages (object-oriented programming) Component-based software engineering Distributed Component Object Model Interface description language IDEF4 Jeroo List of object-oriented programming languages Object association Object-oriented analysis and design Object-oriented modeling Object-oriented ontology UML\n[ edit ] ^ a b c d \"Dr. Alan Kay on the Meaning of \"Object-Oriented Programming\" \" . 2003 . Retrieved 11 February 2010 . ^ Kindler, E.; Krivy, I. (2011). \"Object-Oriented Simulation of systems with sophisticated control\". International Journal of General Systems . 40 (3): 313– 343. doi : 10.1080/03081079.2010.539975 . ^ Lewis, John; Loftus, William (2008). \"1.6: Object-Oriented Programming\". Java Software Solutions . Foundations of Programming Design (6th ed.). Pearson Education Inc. ISBN 978-0-321-53205-3 . ^ Booch, Grady (1986). Software Engineering with Ada . Addison Wesley. p. 220. ISBN 978-0-8053-0608-8 . Perhaps the greatest strength of an object-oriented approach to development is that it offers a mechanism that captures a model of the real world. ^ Wirth, Niklaus (23 January 2006). \"Good ideas, through the looking glass\" (PDF) . IEEE Computer . Cover Feature. 39 (1): 28– 39. doi : 10.1109/MC.2006.20 . S2CID 6582369 . Archived from the original (PDF) on 12 October 2016. ^ \"Uncle Bob SOLID principles\" . YouTube . 2 August 2018. ^ Meyer 1997 , p. 230. ^ Yegge, Steve (30 March 2006). \"Execution in the Kingdom of Nouns\" . steve-yegge.blogspot.com . Retrieved 3 July 2010 . ^ Boronczyk, Timothy (11 June 2009). \"What's Wrong with OOP\" . zaemis.blogspot.com . Retrieved 3 July 2010 . ^ Bloch 2018 , pp. xi–xii, Foreword. ^ McCarthy, J. ; Brayton, R.; Edwards, D.; Fox, P. ; Hodes, L. ; Luckham, D. ; Maling, K.; Park, D. ; Russell, S. (March 1969). \"LISP I Programmers Manual\" (PDF) . Computation Center and Research Laboratory of Electronics . Boston , Massachusetts : Artificial Intelligence Group, M.I.T. Computation Center and Research Laboratory: 88f. Archived from the original (PDF) on 17 July 2010. In the local M.I.T. patois, association lists [of atomic symbols] are also referred to as \"property lists\", and atomic symbols are sometimes called \"objects\". ^ McCarthy, John ; Abrahams, Paul W.; Edwards, Daniel J.; Hart, Swapnil D.; Levin, Michael I. (1962). LISP 1.5 Programmer's Manual . MIT Press . p. 105 . ISBN 978-0-262-13011-0 . Object – a synonym for atomic symbol {{ cite book }} : ISBN / Date incompatibility ( help ) ^ Ivan E. Sutherland (May 1963). Sketchpad: a man-machine graphical communication system . AFIPS '63 (Spring): Proceedings of the May 21–23, 1963 Spring Joint Computer Conference. AFIPS Press. pp. 329– 346. doi : 10.1145/1461551.1461591 . ^ a b Nygaard, Kristen ; Dahl, Ole-Johan (1 August 1978). \"The development of the SIMULA languages\" . ACM SIGPLAN Notices . 13 (8): 245– 272. doi : 10.1145/960118.808391 . ^ Ross, Doug. \"The first software engineering language\" . LCS/AI Lab Timeline . MIT Computer Science and Artificial Intelligence Laboratory . Retrieved 13 May 2010 . ^ a b Holmevik, Jan Rune (Winter 1994). \"Compiling Simula: A historical study of technological genesis\" (PDF) . IEEE Annals of the History of Computing . 16 (4): 25– 37. doi : 10.1109/85.329756 . S2CID 18148999 . Archived from the original (PDF) on 30 August 2017 . Retrieved 3 March 2018 . ^ Madsen, Ole Lehrman. \"Kristen Nygaard\" . A.M. Turing Award Laureates . Retrieved 4 February 2025 . ^ Butcher, Paul (30 June 2014). Seven Concurrency Models in Seven Weeks: When Threads Unravel . Pragmatic Bookshelf. p. 204. ISBN 978-1-68050-466-8 . ^ Jones, Anita K.; Liskov, Barbara H. (April 1976). An Access Control Facility for Programming Languages (PDF) (Technical report). MIT. CSG Memo 137. ^ a b Bertrand Meyer (2009). Touch of Class: Learning to Program Well with Objects and Contracts . Springer Science \u0026 Business Media. p. 329. Bibcode : 2009tclp.book.....M . ISBN 978-3-540-92144-8 . ^ Alan C. Kay (March 1993). \"The early history of Smalltalk\" . ACM SIGPLAN Notices . 28 (3): 69– 95. doi : 10.1145/155360.155364 . ^ Borning, Alan Hamilton (1979). Thinglab: a constraint-oriented simulation laboratory (PDF) (Report). Stanford University. ^ Moon, David A. (June 1986). \"Object-Oriented Programming with Flavors\" (PDF) . Conference proceedings on Object-oriented Programming Systems Languages and Applications . OOPSLA '86. pp. 1– 8. doi : 10.1145/28697.28698 . ISBN 978-0-89791-204-4 . S2CID 17150741 . Retrieved 17 March 2022 . ^ \"Introducing the Smalltalk Zoo\" . CHM . 17 December 2020. ^ Bobrow, D. G.; Stefik, M. J (1982). LOOPS: data and object oriented Programming for Interlisp (PDF) . European AI Conference. ^ Meyer 1997 . ^ 1995 (June) Visual FoxPro 3.0, FoxPro evolves from a procedural language to an object-oriented language. Visual FoxPro 3.0 introduces a database container, seamless client/server capabilities, support for ActiveX technologies, and OLE Automation and null support. Summary of Fox releases ^ 1995 Reviewers Guide to Visual FoxPro 3.0: DFpug.de ^ Khurana, Rohit (1 November 2009). Object Oriented Programming with C++, 1E . Vikas Publishing House Pvt Limited. ISBN 978-81-259-2532-3 . ^ Deborah J. Armstrong. The Quarks of Object-Oriented Development . A survey of nearly 40 years of computing literature identified several fundamental concepts found in the large majority of definitions of OOP, in descending order of popularity: Inheritance, Object, Class, Encapsulation, Method, Message Passing, Polymorphism, and Abstraction. ^ John C. Mitchell , Concepts in programming languages , Cambridge University Press, 2003, ISBN 0-521-78098-5 , p.278. Lists: Dynamic dispatch, abstraction, subtype polymorphism, and inheritance. ^ Michael Lee Scott, Programming language pragmatics , Edition 2, Morgan Kaufmann, 2006, ISBN 0-12-633951-1 , p. 470. Lists encapsulation, inheritance, and dynamic dispatch. ^ Pierce, Benjamin (2002). Types and Programming Languages . MIT Press. ISBN 978-0-262-16209-8 . , section 18.1 \"What is Object-Oriented Programming?\" Lists: Dynamic dispatch, encapsulation or multi-methods (multiple dispatch), subtype polymorphism, inheritance or delegation, open recursion (\"this\"/\"self\") ^ C. J. Date, Introduction to Database Systems, 6th-ed., Page 650 ^ McDonough, James E. (2017). \"Encapsulation\". Object-Oriented Design with ABAP: A Practical Approach . Apress . doi : 10.1007/978-1-4842-2838-8 . ISBN 978-1-4842-2837-1 – via O'Reilly . ^ Bloch 2018 , pp. 73–77, Chapter §4 Item15 Minimize the accessibility of classes and members. ^ \"What is Object Oriented Programming (OOP) In Simple Words? – Software Geek Bytes\" . 5 January 2023 . Retrieved 17 January 2023 . [ permanent dead link ] ^ Cardelli, Luca; Wegner, Peter (10 December 1985). \"On understanding types, data abstraction, and polymorphism\" . ACM Computing Surveys . 17 (4): 471– 523. doi : 10.1145/6041.6042 . ISSN 0360-0300 . ^ Jacobsen, Ivar; Magnus Christerson; Patrik Jonsson; Gunnar Overgaard (1992). Object Oriented Software Engineering . Addison-Wesley ACM Press. pp. 43–69 . ISBN 978-0-201-54435-0 . ^ a b c Cardelli, Luca (1996). \"Bad Engineering Properties of Object-Oriented Languages\" . ACM Comput. Surv . 28 (4es): 150–es. doi : 10.1145/242224.242415 . ISSN 0360-0300 . S2CID 12105785 . Retrieved 21 April 2010 . ^ a b Armstrong, Joe . Seibel, Peter (ed.). Coders at Work: Reflections on the Craft of Programming . Codersatwork.com. Archived from the original on 5 March 2010 . Retrieved 13 November 2009 . ^ a b c Raymond, Eric S. (2003). \"The Art of Unix Programming: Unix and Object-Oriented Languages\" . Retrieved 6 August 2014 . ^ Brodie, Leo (1984). Thinking Forth (PDF) . pp. 92– 93 . Retrieved 4 May 2018 . ^ Hunt, Andrew. \"Don't Repeat Yourself\" . Category Extreme Programming . Retrieved 4 May 2018 . ^ Bloch 2018 , p. 19, Chapter §2 Item 4 Enforce noninstantiability with a private constructor. ^ Dony, C; Malenfant, J; Bardon, D (1999). \"Classifying prototype-based programming languages\" (PDF) . Prototype-based programming: concepts, languages and applications . Singapore Berlin Heidelberg: Springer. ISBN 9789814021258 . ^ \"Is Go an object-oriented language?\" . Retrieved 13 April 2019 . Although Go has types and methods and allows an object-oriented style of programming, there is no type hierarchy. ^ Stroustrup, Bjarne (2015). Object-Oriented Programming without Inheritance (Invited Talk) . 29th European Conference on Object-Oriented Programming (ECOOP 2015). 1:34. doi : 10.4230/LIPIcs.ECOOP.2015.1 . ^ Pike, Rob (14 November 2012). \"A few years ago I saw this page\" . Archived from the original on 14 August 2018 . Retrieved 1 October 2016 . ^ Stroustrup, Bjarne (19 February 2007). \"Bjarne Stroustrup's C++ Glossary\" . polymorphism – providing a single interface to entities of different types. ^ Martin, Robert C. \"Design Principles and Design Patterns\" (PDF) . Archived from the original (PDF) on 6 September 2015 . Retrieved 28 April 2017 . ^ Neward, Ted (26 June 2006). \"The Vietnam of Computer Science\" . Interoperability Happens. Archived from the original on 4 July 2006 . Retrieved 2 June 2010 . ^ C. J. Date, Hugh Darwen. Foundation for Future Database Systems: The Third Manifesto (2nd Edition) ^ Wirfs-Brock, Rebecca; Wilkerson, Brian (1989). \"Object-Oriented Design: A Responsibility-Driven Approach\" . ACM SIGPLAN Notices . 24 (10): 74. doi : 10.1145/74878.74885 . ^ Karsh, Patrick (19 July 2023). \"GRASP Principles: Object-Oriented Design Patterns\" . Medium . Retrieved 30 March 2025 . ^ Poll, Erik. \"Subtyping and Inheritance for Categorical Datatypes\" (PDF) . Retrieved 5 June 2011 . ^ Abadi, Martin ; Cardelli, Luca (1996). A Theory of Objects . Springer-Verlag New York, Inc. ISBN 978-0-387-94775-4 . Retrieved 21 April 2010 . ^ Tan, Tian; Li, Yue (12 July 2023). Tai-e: A Developer-Friendly Static Analysis Framework for Java by Harnessing the Good Designs of Classics . ISSTA 2023. pp. 1093– 1105. doi : 10.1145/3597926.3598120 . ^ Bhutani, Vikram; Toosi, Farshad Ghassemi; Buckley, Jim (1 June 2024). \"Analysing the Analysers: An Investigation of Source Code Analysis Tools\". Applied Computer Systems . 29 (1): 98– 111. doi : 10.2478/acss-2024-0013 . ^ a b Stepanov, Alexander (2001–2008). \"STLport: An Interview with A. Stepanov\" . Retrieved 21 April 2010 . ^ a b Hickey, Rich (November 2009). Are We There Yet? (keynote) . JVM Languages Summit. ^ Pike, Rob (25 June 2012). \"Less is exponentially more\" . Retrieved 1 October 2016 . ^ Pike, Rob (2 March 2004). \"[9fans] Re: Threads: Sewing badges of honor onto a Kernel\" . comp.os.plan9 (Mailing list) . Retrieved 17 November 2016 . ^ Ambler, Scott (1 January 1998). \"A Realistic Look at Object-Oriented Reuse\" . drdobbs.com . Retrieved 5 August 2025 . ^ Shelly, Asaf (22 August 2008). \"Flaws of Object Oriented Modeling\" . Intel Software Network . Retrieved 4 July 2010 . ^ James, Justin (1 October 2007). \"Multithreading is a verb not a noun\" . techrepublic.com. Archived from the original on 10 October 2007 . Retrieved 4 July 2010 . ^ Shelly, Asaf (22 August 2008). \"HOW TO: Multicore Programming (Multiprocessing) Visual C++ Class Design Guidelines, Member Functions\" . support.microsoft.com . Retrieved 4 July 2010 . ^ Robert Harper (17 April 2011). \"Some thoughts on teaching FP\" . Existential Type Blog . Retrieved 5 December 2011 . ^ Brucker, Achim D.; Wolff, Burkhart (2008). \"Extensible Universes for Object-Oriented Data Models\". ECOOP 2008 – Object-Oriented Programming . Lecture Notes in Computer Science. Vol. 5142. pp. 438– 462. doi : 10.1007/978-3-540-70592-5_19 . ISBN 978-3-540-70591-8 . object-oriented programming is a widely accepted programming paradigm ^ Cassel, David (21 August 2019). \"Why Are So Many Developers Hating on Object-Oriented Programming?\" . The New Stack . ^ Potok, Thomas; Vouk, Mladen; Rindos, Andy (1999). \"Productivity Analysis of Object-Oriented Software Developed in a Commercial Environment\" (PDF) . Software: Practice and Experience . 29 (10): 833– 847. doi : 10.1002/(SICI)1097-024X(199908)29:10\u003c833::AID-SPE258\u003e3.0.CO;2-P . S2CID 57865731 . Retrieved 21 April 2010 . ^ Graham, Paul . \"Why ARC isn't especially Object-Oriented\" . PaulGraham.com . Retrieved 13 November 2009 . ^ Feldman, Richard (30 September 2019). \"Why Isn't Functional Programming the Norm?\" . YouTube . ^ Krubner, Lawrence. \"Object Oriented Programming is an expensive disaster which must end\" . smashcompany.com. Archived from the original on 14 October 2014 . Retrieved 14 October 2014 .\n[ edit ] Abadi, Martin ; Luca Cardelli (1998). A Theory of Objects . Springer Verlag . ISBN 978-0-387-94775-4 . Abelson, Harold ; Gerald Jay Sussman (1997). Structure and Interpretation of Computer Programs . MIT Press . ISBN 978-0-262-01153-2 . Archived from the original on 26 December 2017 . Retrieved 22 January 2006 . Armstrong, Deborah J. (February 2006). \"The Quarks of Object-Oriented Development\". Communications of the ACM . 49 (2): 123– 128. doi : 10.1145/1113034.1113040 . ISSN 0001-0782 . S2CID 11485502 . Bloch, Joshua (2018). \"Effective Java: Programming Language Guide\" (third ed.). Addison-Wesley. ISBN 978-0134685991 . Booch, Grady (1997). Object-Oriented Analysis and Design with Applications . Addison-Wesley . ISBN 978-0-8053-5340-2 . Eeles, Peter; Oliver Sims (1998). Building Business Objects . John Wiley \u0026 Sons . ISBN 978-0-471-19176-6 . Gamma, Erich ; Richard Helm ; Ralph Johnson ; John Vlissides (1995). Design Patterns: Elements of Reusable Object Oriented Software . Addison-Wesley. Bibcode : 1995dper.book.....G . ISBN 978-0-201-63361-0 . Harmon, Paul ; William Morrissey (1996). The Object Technology Casebook – Lessons from Award-Winning Business Applications . John Wiley \u0026 Sons. ISBN 978-0-471-14717-6 . Jacobson, Ivar (1992). Object-Oriented Software Engineering: A Use Case-Driven Approach . Addison-Wesley. Bibcode : 1992oose.book.....J . ISBN 978-0-201-54435-0 . Kay, Alan . The Early History of Smalltalk . Archived from the original on 4 April 2005 . Retrieved 18 April 2005 . Meyer, Bertrand (1997). Object-Oriented Software Construction . Prentice Hall . ISBN 978-0-13-629155-8 . Pecinovsky, Rudolf (2013). OOP – Learn Object Oriented Thinking \u0026 Programming . Bruckner Publishing. ISBN 978-80-904661-8-0 . Rumbaugh, James ; Blaha, Michael; Premerlani, William; Eddy, Frederick; Lorensen, William (1991). Object-Oriented Modeling and Design . Prentice Hall. ISBN 978-0-13-629841-0 . Schach, Stephen (2006). Object-Oriented and Classical Software Engineering, Seventh Edition . McGraw-Hill . ISBN 978-0-07-319126-3 . Schreiner, Axel-Tobias (1993). Object oriented programming with ANSI-C . Hanser. hdl : 1850/8544 . ISBN 978-3-446-17426-9 . Taylor, David A. (1992). Object-Oriented Information Systems – Planning and Implementation . John Wiley \u0026 Sons. ISBN 978-0-471-54364-0 . Weisfeld, Matt (2009). The Object-Oriented Thought Process, Third Edition . Addison-Wesley . ISBN 978-0-672-33016-2 . West, David (2004). Object Thinking (Developer Reference) . Microsoft Press . ISBN 978-0-7356-1965-4 .\nWikiquote has quotations related to\nWikiversity has learning resources about\nObject-oriented programming\nTopic:Object-Oriented Programming\nWikibooks has a book on the topic of:\nObject Oriented Programming Introduction to Object Oriented Programming Concepts (OOP) and More\nDiscussion on Cons of OOP OOP Concepts (Java Tutorials) v t e Programming paradigms\nComparison by language\nImperative Structured Jackson structures Block-structured Modular Non-structured Procedural Programming in the large and in the small Design by contract Invariant-based Nested function Object-oriented\nObject-based Agent Immutable object Persistent Uniform function call syntax Declarative Functional\nRecursive Anonymous function\nHigher-order Purely functional Total Strict GADTs Dependent types Functional logic Point-free style Expression-oriented Applicative\nConcatenative Function-level\nValue-level Dataflow Flow-based Reactive\nSignals Streams Synchronous Logic Abductive logic Answer set Constraint\nInductive logic Nondeterministic Ontology Probabilistic logic Query DSL Algebraic modeling Array Automata-based\nDifferentiable End-user Grammar-oriented Interface description Language-oriented List comprehension Low-code Modeling Natural language Non-English-based Page description Pipes\nfilters Probabilistic Quantum Scientific Scripting Set-theoretic Simulation Stack-based System Tactile Templating Transformation\nparallel Actor-based Automatic mutual exclusion Choreographic programming Concurrent logic\nConcurrent constraint logic\nConcurrent OO Macroprogramming Multitier programming Organic computing Parallel programming models Partitioned global address space Process-oriented Relativistic programming Service-oriented Structured concurrency Metaprogramming Attribute-oriented Automatic\nDynamic Extensible Generic Homoiconicity Interactive Macro\nMetalinguistic abstraction Multi-stage Program synthesis\nReflective Self-modifying code Symbolic Template Separation of concerns Aspects Components Data-driven Data-oriented Event-driven Features Literate Roles Subjects v t e Types of programming languages\nMachine Assembly Compiled Interpreted Low-level High-level Very high-level Esoteric Generation First Second Third Fourth Fifth v t e Software engineering\nComputer programming DevOps Empirical software engineering Experimental software engineering Formal methods Requirements engineering Search-based software engineering Site reliability engineering Social software engineering Software deployment Software design Software maintenance Software testing Systems analysis\nAbstraction CI/CD Compatibility Backward compatibility Compatibility layer Compatibility mode Forward compatibility Software incompatibility Component-based software engineering Data modeling Enterprise architecture Functional specification Modeling language Programming paradigm Software Software archaeology Software architecture Software configuration management Software development process/methodology Software quality Software quality assurance Software system Software verification and validation Structured analysis Essential analysis\nAgile Aspect-oriented Object orientation Ontology SDLC Service orientation\nAgile EUP Executable UML Incremental model Iterative model Prototype model RAD Scrum Spiral model UP V-model Waterfall model XP Model-driven engineering Round-trip engineering\nCMMI Data model ER model Function model Information model Metamodeling Object model SPICE Systems model View model\nIDEF SysML UML USL Systems design Requirements Functional requirement Non-functional requirement Capacity Users Compute Memory Storage Network Web traffic Interface API SOAP REST GraphQL gRPC Webhook High-level design Client Load balancing Application server Message queue Integration Vertical Peer-to-peer API Gateway pub-sub Event Source ETL Batch Stream Orchestration Architectures Monolithic Microservices Event-Driven Client–server Serverless Circuit breaker Rate limiting Database Relational NoSQL Graph Key–value LSM Tree Time series Column Oriented Principles Consistency Reliability CAP theorem Maintainability Configurability Modularity Durability Scalability Stateless Asynchronous I/O Loose coupling Availability Fault tolerance Failover Single point of failure Replication Consistent hashing Circuit breaker Latency Cache CDN Shard Database index Pre-caching Storage Object storage Block storage Observability Logging Metrics Tracing Error rate Queries per second Notification system Anomaly detection\nComputer engineering Computer science Information science Project management Risk management Systems engineering Category Commons Authority control databases\nGermany United States France BnF data Japan Czech Republic Spain Israel\nhttps://en.wikipedia.org/w/index.php?title=Object-oriented_programming\u0026oldid=1305079732\nObject-oriented programming Programming paradigms\nCS1 errors: ISBN date All articles with dead external links Articles with dead external links from July 2025 Articles with permanently dead external links Articles with short description Short description is different from Wikidata Use dmy dates from January 2020 All articles with unsourced statements Articles with unsourced statements from February 2010 Articles with unsourced statements from February 2025 Articles with example code Search Search Object-oriented programming 80 languages Add topic",
          "role": "teacher"
        },
        {
          "content": "Why is this important to understand?",
          "role": "student"
        },
        {
          "content": "Understanding OOP is crucial for building efficient, scalable, and maintainable software systems. It forms the foundation for solving complex computational problems.",
          "role": "teacher"
        },
        {
          "content": "Can you give me a concrete example?",
          "role": "student"
        },
        {
          "content": "For example, OOP can be applied in scenarios where you need to optimize performance, manage complexity, or implement scalable solutions.",
          "role": "teacher"
        }
      ],
      "format": "socratic_dialogue",
      "metadata": {
        "domain": "Programming",
        "source": "https://en.wikipedia.org/wiki/Object-oriented_programming",
        "words": 7207
      },
      "topic": "OOP"
    },
    {
      "conversation": [
        {
          "content": "How do I implement OOP?",
          "role": "user"
        },
        {
          "content": "Here's how to implement OOP:\n\n2. Choose appropriate data structures\n3. Handle edge cases and validation\n4. Optimize for your use case\n5. Test thoroughly with various inputs\n\nBasic structure:\n```\nfunction implementOOP(input) {\n    // Initialize\n    // Process\n    // Return result\n}\n```",
          "role": "assistant"
        }
      ],
      "format": "implementation_guide",
      "metadata": {
        "domain": "Programming",
        "type": "code_focused"
      },
      "topic": "OOP"
    },
    {
      "content": {
        "applications": "Applications\n) was attended by 1,000 people. This conference marked the start of efforts to consolidate Lisp object systems, eventually resulting in the\nCommon Lisp Object System\n. In the 1980s, there were a few attempts to design\nprocessor architectures\nsupport for objects in\n, but these were not successful. Examples include the\nIn the mid-1980s, new object-oriented languages like\nemerged. Objective-C was developed by\n, who had used Smalltalk at\nITT Inc. Bjarne Stroustrup\nbased on his experienc",
        "considerations": "important to understand and control how instructions flow. This is difficult to do with OOP.\n[ 65 ] [ 66 ] [ 67 ] [ 68 ]\nMany popular programming languages, like C++, Java, and Python, use OOP. In the past, OOP was widely accepted,\nbut recently, some programmers have criticized it and prefer functional programming instead.\nA study by Potok et al. found no major difference in productivity between O",
        "key_concepts": [
          "D, was influenced by Smalltalk and Flavors, and a paper about it was published in 1982."
        ],
        "overview": "A comprehensive technical concept with wide applications."
      },
      "format": "conceptual_breakdown",
      "metadata": {
        "domain": "Programming",
        "structure": "hierarchical"
      },
      "topic": "OOP"
    },
    {
      "format": "problem_solving",
      "scenario": {
        "approach": "solve programming problems.\nThese solutions, called \"design patterns,\" are grouped into three types:\nFactory method pattern\nAbstract factory pattern\nPrototype pattern Structural patterns\nProxy pattern Behavioral patterns\nChain-of-responsibility pattern\nTemplate method pattern\nObject-orientation and databases\nObject-relational impedance mismatch\nObject-relational mapping\nrelational database management systems\n(RDBMSs) are widely used in software today. However,\nrelational databases\ndon't store ob",
        "benefits": "advantages compared to other styles, like functional programming, and can complicate coding.\n[ 74 ] Luca Cardelli\nsays that OOP is slower and takes longer to compile than procedural programming.\n[ edit ] Computer programming portal CADES Common Object Request Broker Architecture\nComparison of progra",
        "problem": "How can OOP solve real-world problems?",
        "tradeoffs": "Consider the tradeoffs between complexity and performance, as well as development time versus optimization."
      },
      "topic": "OOP"
    },
    {
      "comparison": "Comparison of programming languages (object-oriented programming)\nList of object-oriented programming terms\nThe OOP features provided by languages varies. Below are some common features of OOP languages.\n[ 30 ] [ 31 ] [ 32 ] [ 33 ]\nComparing OOP with other styles, like\nrelational programming\n, is difficult because there isn't a clear, agreed-upon definition of OOP.\n. A field (a.k.a. attribute or property) contains information (a.k.a. state) as a\nor action) defines behavior via logic code. Encaps",
      "format": "comparative_analysis",
      "topic": "OOP"
    },
    {
      "dialogue": [
        {
          "content": "What is Functional Programming really about at its core?",
          "role": "student"
        },
        {
          "content": "Programming paradigm based on applying and composing functions\nFor subroutine-oriented programming, see\nProcedural programming\nfunctional programming\nprogramming paradigm\nwhere programs are constructed by\ndeclarative programming\nparadigm in which function definitions are\nto other values, rather than a sequence of\nimperative statements\nIn functional programming, functions are treated as\nfirst-class citizens\n, meaning that they can be bound to names (including local\nfrom other functions, just as any other\ncan. This allows programs to be written in a\nstyle, where small functions are combined in a\nFunctional programming is sometimes treated as synonymous with\npurely functional programming\n, a subset of functional programming that treats all functions as\n. When a pure function is called with some given arguments, it will always return the same result, and cannot be affected by any mutable\n. This is in contrast with impure\nimperative programming\n, which can have side effects (such as modifying the program's state or taking input from a user). Proponents of purely functional programming claim that by restricting side effects, programs can have fewer\n, and be more suited to\nFunctional programming has its roots in\n, a formal system of computation based only on functions. Functional programming has historically been less popular than imperative programming, but many functional languages are seeing use today in industry and education, including\n[ 3 ] [ 4 ] [ 5 ] [ 6 ] Clojure\n[ 10 ] [ 11 ] [ 12 ] Elixir\n[ 14 ] [ 15 ] Haskell\nis a functional programming language commonly used for verifying mathematical theorems.\nFunctional programming is also key to some languages that have found success in specific domains, like\nin financial analysis, and\nDomain-specific declarative languages like\nuse some elements of functional programming, such as not allowing\nIn addition, many other programming languages support programming in a functional style or have implemented features from functional programming, such as\n, developed in the 1930s by\nfunction application\nproved that the lambda calculus and\nare equivalent models of computation,\nshowing that the lambda calculus is\n. Lambda calculus forms the basis of all functional programming languages. An equivalent theoretical formulation,\nin the 1920s and 1930s.\nChurch later developed a weaker system, the\nsimply typed lambda calculus\n, which extended the lambda calculus by assigning a\nThis forms the basis for statically typed functional programming.\nfunctional programming language,\n, was developed in the late 1950s for the\nof scientific computers by\nMassachusetts Institute of Technology\nLisp functions were defined using Church's lambda notation, extended with a label construct to allow\nLisp first introduced many paradigmatic features of functional programming, though early Lisps were\nmulti-paradigm languages\n, and incorporated support for numerous programming styles as new paradigms evolved. Later dialects, such as\n, and offshoots such as\n, sought to simplify and rationalise Lisp around a cleanly functional core, while\nwas designed to preserve and update the paradigmatic features of the numerous older dialects it replaced.\n[ 42 ] Information Processing Language\n(IPL), 1956, is sometimes cited as the first computer-based functional programming language.\nassembly-style language\nfor manipulating lists of symbols. It does have a notion of\n, which amounts to a function that accepts a function as an argument, and, since it is a\nlow-level programming language\n, code can be data, so IPL can be regarded as having higher-order functions. However, it relies heavily on the mutating list structure and similar imperative features.\nin the early 1960s, described in his 1962 book\nA Programming Language\n). APL was the primary influence on\n. In the early 1990s, Iverson and\n, who had previously worked with Iverson, created\n, which is used commercially in financial industries along with its descendant\nfor a functional programming language,\ndescribed a correspondence between\nprogramming language.\nlecture \"Can Programming Be Liberated From the\nStyle? A Functional Style and its Algebra of Programs\".\nHe defines functional programs as being built up in a hierarchical way by means of \"combining forms\" that allow an \"algebra of programs\"; in modern language, this means that functional programs follow the\nprinciple of compositionality\nBackus's paper popularized research into functional programming, though it emphasized\nfunction-level programming\nrather than the lambda-calculus style now associated with functional programming.\nUniversity of Edinburgh\ndeveloped the language\nUniversity of St Andrews\n. Also in Edinburgh in the 1970s, Burstall and Darlington developed the functional language\nKleene Recursion Equations\nand was first introduced in their work on program transformation.\nBurstall, MacQueen and Sannella then incorporated the\ntype checking from ML to produce the language\nML eventually developed into several dialects, the most common of which are now\n, as described in the\nand the 1985 textbook\nStructure and Interpretation of Computer Programs\n. Scheme was the first dialect of lisp to use\ntail-call optimization\n, features that encourage functional programming.\nintuitionistic type theory\ntype theory), which associated functional programs with\n. This led to new approaches to\ninteractive theorem proving\nand has influenced the development of subsequent functional programming languages.\nThe lazy functional language,\n, developed by David Turner, initially appeared in 1985 and had a strong influence on\n. With Miranda being proprietary, Haskell began with a consensus in 1987 to form an\nfor functional programming research; implementation releases have been ongoing as of 1990.\nMore recently it has found use in niches such as parametric\nlanguage built on the\nframework, although its restriction on reassigning values (all values are treated as constants) has led to confusion among users who are unfamiliar with functional programming as a concept.\nFunctional programming continues to be used in commercial settings.\n[ 54 ] [ 55 ] [ 56 ]\nA number of concepts\nand paradigms are specific to functional programming, and generally foreign to\nimperative programming\nobject-oriented programming\n). However, programming languages often cater to several programming paradigms, so programmers using \"mostly imperative\" languages may have utilized some of these concepts.\nFirst-class and higher-order functions\nFirst-class function\nHigher-order function Higher-order functions\nare functions that can either take other functions as arguments or return them as results. In calculus, an example of a higher-order function is the\ndifferential operator d / d x {\\displaystyle d/dx}\nHigher-order functions are closely related to\nfirst-class functions\nin that higher-order functions and first-class functions both allow functions as arguments and results of other functions. The distinction between the two is subtle: \"higher-order\" describes a mathematical concept of functions that operate on other functions, while \"first-class\" is a computer science term for programming language entities that have no restriction on their use (thus first-class functions can appear anywhere in the program that other first-class entities like numbers can, including as arguments to other functions and as their return values).\nHigher-order functions enable\n, a technique that applies a function to its arguments one at a time, with each application returning a new function that accepts the next argument. This lets a programmer succinctly express, for example, the\nas the addition operator partially applied to the\nPure function Pure functions\n(or expressions) have no\n(memory or I/O). This means that pure functions have several useful properties, many of which can be used to optimize the code:\nIf the result of a pure expression is not used, it can be removed without affecting other expressions.\nIf a pure function is called with arguments that cause no side-effects, the result is constant with respect to that argument list (sometimes called\nreferential transparency\n), i.e., calling the pure function again with the same arguments returns the same result. (This can enable caching optimizations such as\nIf there is no data dependency between two pure expressions, their order can be reversed, or they can be performed in\nand they cannot interfere with one another (in other terms, the evaluation of any pure expression is\nIf the entire language does not allow side-effects, then any evaluation strategy can be used; this gives the compiler freedom to reorder or combine the evaluation of expressions in a program (for example, using\nWhile most compilers for imperative programming languages detect pure functions and perform common-subexpression elimination for pure function calls, they cannot always do this for pre-compiled libraries, which generally do not expose this information, thus preventing optimizations that involve those external functions. Some compilers, such as\n, add extra keywords for a programmer to explicitly mark external functions as pure, to enable such optimizations.\nalso lets functions be designated\nkeyword with similar semantics.\nRecursion (computer science) Iteration\n(looping) in functional languages is usually accomplished via\ninvoke themselves, letting an operation be repeated until it reaches the\n. In general, recursion requires maintaining a\n, which consumes space in a linear amount to the depth of recursion. This could make recursion prohibitively expensive to use instead of imperative loops. However, a special form of recursion known as\ncan be recognized and optimized by a compiler into the same code used to implement iteration in imperative languages. Tail recursion optimization can be implemented by transforming the program into\ncontinuation passing style\nduring compiling, among other approaches.\nlanguage standard requires implementations to support proper tail recursion, meaning they must allow an unbounded number of active tail calls.\nProper tail recursion is not simply an optimization; it is a language feature that assures users that they can use recursion to express a loop and doing so would be safe-for-space.\nMoreover, contrary to its name, it accounts for all tail calls, not just tail recursion. While proper tail recursion is usually implemented by turning code into imperative loops, implementations might implement it in other ways. For example,\nintentionally maintains a stack and lets the\n. However, when this happens, its\nwill claim space back,\nallowing an unbounded number of active tail calls even though it does not turn tail recursion into a loop.\nCommon patterns of recursion can be abstracted away using higher-order functions, with\n(or \"folds\" and \"unfolds\") being the most obvious examples. Such recursion schemes play a role analogous to built-in control structures such as\nimperative languages\nMost general purpose functional programming languages allow unrestricted recursion and are\nhalting problem undecidable\n, can cause unsoundness of\nequational reasoning\n, and generally requires the introduction of\ninto the logic expressed by the language's\n. Some special purpose languages such as\nstrongly normalizing\n(nonterminating computations can be expressed only with infinite streams of values called\n). As a consequence, these languages fail to be Turing complete and expressing certain functions in them is impossible, but they can still express a wide class of interesting computations while avoiding the problems introduced by unrestricted recursion. Functional programming limited to well-founded recursion with a few other constraints is called\ntotal functional programming\nStrict versus non-strict evaluation\nFunctional languages can be categorized by whether they use\nevaluation, concepts that refer to how function arguments are processed when an expression is being evaluated. The technical difference is in the\ndenotational semantics\nof expressions containing failing or divergent computations. Under strict evaluation, the evaluation of any term containing a failing subterm fails. For example, the expression:\nprint length([2+1, 3*2, 1/0, 5-4])\nfails under strict evaluation because of the division by zero in the third element of the list. Under lazy evaluation, the length function returns the value 4 (i.e., the number of items in the list), since evaluating it does not attempt to evaluate the terms making up the list. In brief, strict evaluation always fully evaluates function arguments before invoking the function. Lazy evaluation does not evaluate function arguments unless their values are required to evaluate the function call itself.\nThe usual implementation strategy for lazy evaluation in functional languages is\nLazy evaluation is used by default in several pure functional languages, including\nargues for lazy evaluation as a mechanism for improving program modularity through\nseparation of concerns\n, by easing independent implementation of producers and consumers of data streams.\nLaunchbury 1993 describes some difficulties that lazy evaluation introduces, particularly in analyzing a program's storage requirements, and proposes an\noperational semantics\nto aid in such analysis.\nHarper 2009 proposes including both strict and lazy evaluation in the same language, using the language's type system to distinguish them.\nEspecially since the development of\nHindley–Milner type inference\nin the 1970s, functional programming languages have tended to use\ntyped lambda calculus\n, rejecting all invalid programs at compilation time and risking\nfalse positive errors\nuntyped lambda calculus\n, that accepts all valid programs at compilation time and risks\nfalse negative errors\n, used in Lisp and its variants (such as\n), as they reject all invalid programs at runtime when the information is enough to not reject valid programs. The use of\nalgebraic data types\nmakes manipulation of complex data structures convenient; the presence of strong compile-time type checking makes programs more reliable in absence of other reliability techniques like\ntest-driven development\nfrees the programmer from the need to manually declare types to the compiler in most cases.\nSome research-oriented functional languages such as\nintuitionistic type theory\n, which lets types depend on terms. Such types are called\n. These type systems do not have decidable type inference and are difficult to understand and program with.\n[ 68 ] [ 69 ] [ 70 ] [ 71 ]\nBut dependent types can express arbitrary propositions in\nCurry–Howard isomorphism\n, then, well-typed programs in these languages become a means of writing formal\nfrom which a compiler can generate\n. While these languages are mainly of interest in academic research (including in\nformalized mathematics\n), they have begun to be used in engineering as well.\nfor a subset of the language\nthat is written in Coq and formally verified.\nA limited form of dependent types called\ngeneralized algebraic data types\n(GADT's) can be implemented in a way that provides some of the benefits of dependently typed programming while avoiding most of its inconvenience.\nGADT's are available in the\nGlasgow Haskell Compiler\nand have been proposed as additions to other languages including Java and C#.\nReferential transparency\nReferential transparency\nFunctional programs do not have assignment statements, that is, the value of a variable in a functional program never changes once defined. This eliminates any chances of side effects because any variable can be replaced with its actual value at any point of execution. So, functional programs are referentially transparent.\nassignment statement\n, this changes the value assigned to the variable\n. Let us say that the initial value of\n, then two consecutive evaluations of the variable\nrespectively. Clearly, replacing\ngives a program a different meaning, and so the expression\nreferentially transparent. In fact, assignment statements are never referentially transparent.\nNow, consider another function such as\nint plusone ( int x ) { return x + 1 ;} is\ntransparent, as it does not implicitly change the input x and thus has no such\nFunctional programs exclusively use this type of function and are therefore referentially transparent.\nPurely functional data structure\nare often represented in a different way to their\nwith constant access and update times is a basic component of most imperative languages, and many imperative data-structures, such as the\n, are based on arrays. Arrays can be replaced by\nor random access lists, which admit purely functional implementation, but have\naccess and update times. Purely functional data structures have\n, a property of keeping previous versions of the data structure unmodified. In Clojure, persistent data structures are used as functional alternatives to their imperative counterparts. Persistent vectors, for example, use trees for partial updating. Calling the insert method will result in some but not all nodes being created.\nComparison to imperative programming\nFunctional programming is very different from\nimperative programming\n. The most significant differences stem from the fact that functional programming avoids\n, which are used in imperative programming to implement state and I/O. Pure functional programming completely prevents side-effects and provides referential transparency.\nHigher-order functions are rarely used in older imperative programming. A traditional imperative program might use a loop to traverse and modify a list. A functional program, on the other hand, would probably use a higher-order \"map\" function that takes a function and a list, generating and returning a new list by applying the function to each list item.\nImperative vs. functional programming\nThe following two examples (written in\n) achieve the same effect: they multiply all even numbers in an array by 10 and add them all, storing the final sum in the variable\nTraditional imperative loop:\nconst numList = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ]; let result = 0 ; for ( let i = 0 ; i \u003c numList . length ; i ++ ) { if ( numList [ i ] % 2 === 0 ) { result += numList [ i ] * 10 ; } }\nFunctional programming with higher-order functions:\nconst result = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] . filter ( n =\u003e n % 2 === 0 ) . map ( a =\u003e a * 10 ) . reduce (( a , b ) =\u003e a + b , 0 );\nSometimes the abstractions offered by functional programming might lead to development of more robust code that avoids certain issues that might arise when building upon large amount of complex, imperative code, such as\nGreenspun's tenth rule\nThere are tasks (for example, maintaining a bank account balance) that often seem most naturally implemented with state. Pure functional programming performs these tasks, and I/O tasks such as accepting user input and printing to the screen, in a different way.\nThe pure functional programming language\nimplements them using\nMonads offer a way to abstract certain types of computational patterns, including (but not limited to) modeling of computations with mutable state (and other side effects such as I/O) in an imperative manner without losing purity. While existing monads may be easy to apply in a program, given appropriate templates and examples, many students find them difficult to understand conceptually, e.g., when asked to define new monads (which is sometimes needed for certain types of libraries).\nFunctional languages also simulate states by passing around immutable states. This can be done by making a function accept the state as one of its parameters, and return a new state together with the result, leaving the old state unchanged.\nImpure functional languages usually include a more direct method of managing mutable state.\n, for example, uses managed references that can be updated by applying pure functions to the current state. This kind of approach enables mutability while still promoting the use of pure functions as the preferred way to express computations.\nAlternative methods such as\nhave been developed to track side effects in programs. Some modern research languages use\nto make the presence of side effects explicit.\nFunctional programming languages are typically less efficient in their use of\nand memory than imperative languages such as\nThis is related to the fact that some mutable data structures like arrays have a very straightforward implementation using present hardware. Flat arrays may be accessed very efficiently with deeply pipelined CPUs, prefetched efficiently through caches (with no complex\n), or handled with SIMD instructions. It is also not easy to create their equally efficient general-purpose immutable counterparts. For purely functional languages, the worst-case slowdown is logarithmic in the number of memory cells used, because mutable memory can be represented by a purely functional data structure with logarithmic access time (such as a balanced tree).\nHowever, such slowdowns are not universal. For programs that perform intensive numerical computations, functional languages such as\nare only slightly slower than C according to\nThe Computer Language Benchmarks Game\nFor programs that handle large\nand multidimensional\nfunctional languages (such as\n) were designed with speed optimizations.\nImmutability of data can in many cases lead to execution efficiency by allowing the compiler to make assumptions that are unsafe in an imperative language, thus increasing opportunities for\nEven if the involved copying that may seem implicit when dealing with persistent immutable data structures might seem computationally costly, some functional programming languages, like\nsolve this issue by implementing mechanisms for safe memory sharing between\ndistinguishes itself by its approach to data immutability which involves immutable\nand a concept called\nImmutable data with separation of identity and state and\nschemes can also potentially be more well-suited for\nconcurrent and parallel\nprogramming by the virtue of reducing or eliminating the risk of certain concurrency hazards, since concurrent operations are usually\nand this allows eliminating the need for locks. This is how for example\njava.util.concurrent\nclasses are implemented, where some of them are immutable variants of the corresponding classes that are not suitable for concurrent use.\nFunctional programming languages often have a concurrency model that instead of shared state and synchronization, leverages\nmechanisms (such as the\n, where each actor is a container for state, behavior, child actors and a message queue).\nThis approach is common in\nmay also speed up the program, even asymptotically, whereas it may slow it down at most by a constant factor (however, it may introduce\nif used improperly). Launchbury 1993\ndiscusses theoretical issues related to memory leaks from lazy evaluation, and O'Sullivan\ngive some practical advice for analyzing and fixing them.\nHowever, the most general implementations of lazy evaluation making extensive use of dereferenced code and data perform poorly on modern processors with deep pipelines and multi-level caches (where a cache miss may cost hundreds of cycles)\nSome functional programming languages might not optimize abstractions such as higher order functions like \"\n\" as efficiently as the underlying imperative operations. Consider, as an example, the following two ways to check if 5 is an even number in\n( even? 5 ) ( .equals ( mod 5 2 ) 0 )\nversion 22 and Clojure version 1.11.1, the first implementation, which is implemented as:\n( defn even? \"Returns true if n is even, throws an exception if n is not an integer\" { :added \"1.0\" :static true } [ n ] ( if ( integer? n ) ( zero? ( bit-and ( clojure.lang.RT/uncheckedLongCast n ) 1 )) ( throw ( IllegalArgumentException. ( str \"Argument must be an integer: \" n )))))\nhas the mean execution time of 4.76 ms, while the second one, in which\nis a direct invocation of the underlying\nmethod, has a mean execution time of 2.8 μs – roughly 1700 times faster. Part of that can be attributed to the type checking and exception handling involved in the implementation of\n, which implements various higher-order functions common in functional programming languages using\n. In a benchmark provided by the library's author, calling\nis 4% slower than an equivalent\nloop and has the same\nwhich can be attributed to various compiler optimizations, such as\nOne distinguishing feature of\nzero-cost abstractions\n. This means that using them imposes no additional runtime overhead. This is achieved thanks to the compiler using\n, where each iteration of a loop, be it imperative or using iterators, is converted into a standalone\ninstruction, without the overhead of the loop controlling code. If an iterative operation writes to an array, the resulting array's elements\nwill be stored in specific CPU registers\nconstant-time access\nFunctional programming in non-functional languages\nIt is possible to use a functional style of programming in languages that are not traditionally considered functional languages.\nexplicitly support pure functions.\nfirst class functions\nfrom their inception.\nPython had support for \"\n\" in 1994, as well as closures in Python 2.2,\nthough Python 3 relegated \"reduce\" to the\nstandard library module.\nFirst-class functions have been introduced into other mainstream languages such as\n[ 28 ] [ citation needed ]\nare fully supported and frequently used. The book\n, released in 2005, was written to provide an expansive guide on using Perl for functional programming.\nand lambdas are fully supported. Libraries and language extensions for immutable data structures are being developed to aid programming in the functional style.\n, anonymous classes can sometimes be used to simulate closures;\nhowever, anonymous classes are not always proper replacements to closures because they have more limited capabilities.\nJava 8 supports lambda expressions as a replacement for some anonymous classes.\n, anonymous classes are not necessary, because closures and lambdas are fully supported. Libraries and language extensions for immutable data structures are being developed to aid programming in the functional style in C#.\nobject-oriented design patterns\nare expressible in functional programming terms: for example, the\nsimply dictates use of a higher-order function, and the\npattern roughly corresponds to a\nSimilarly, the idea of immutable data from functional programming is often included in imperative programming languages,\nfor example the tuple in Python, which is an immutable array, and Object.freeze() in JavaScript.\nComparison to logic programming\n[ edit ] Logic programming\ncan be viewed as a generalisation of functional programming, in which functions are a special case of relations.\nFor example, the function, mother(X) = Y, (every X has only one mother Y) can be represented by the relation mother(X, Y). Whereas functions have a strict input-output pattern of arguments, relations can be queried with any pattern of inputs and outputs. Consider the following logic program:\nmother ( charles , elizabeth ). mother ( harry , diana ).\nThe program can be queried, like a functional program, to generate mothers from children:\n?- mother ( harry , X ). X = diana . ?- mother ( charles , X ). X = elizabeth .\nBut it can also be queried\n, to generate children:\n?- mother ( X , elizabeth ). X = charles . ?- mother ( X , diana ). X = harry .\nIt can even be used to generate all instances of the mother relation:\n?- mother ( X , Y ). X = charles , Y = elizabeth . X = harry , Y = diana .\nCompared with relational syntax, functional syntax is a more compact notation for nested functions. For example, the definition of maternal grandmother in functional syntax can be written in the nested form:\nmaternal_grandmother ( X ) = mother ( mother ( X )).\nThe same definition in relational notation needs to be written in the unnested form:\nmaternal_grandmother ( X , Y ) :- mother ( X , Z ), mother ( Z , Y ).\nHowever, the difference between the two representations is simply syntactic. In\nProlog, relations can be nested, like functions in functional programming:\n[ 111 ] grandparent ( X ) := parent ( parent ( X )). parent ( X ) := mother ( X ). parent ( X ) := father ( X ). mother ( charles ) := elizabeth . father ( charles ) := phillip . mother ( harry ) := diana . father ( harry ) := charles . ?- grandparent ( X , Y ). X = harry , Y = elizabeth . X = harry , Y = phillip .\nCiao transforms the function-like notation into relational form and executes the resulting logic program using the standard Prolog execution strategy.\n, a highly extensible text editor family uses its own\nfor writing plugins. The original author of the most popular Emacs implementation,\nconsiders Lisp one of his favorite programming languages.\n, since version 24.03 supports previewing\n, which are also the core feature of the Lisp programming language family.\n[ edit ] Spreadsheets\ncan be considered a form of pure,\n, strict-evaluation functional programming system.\nHowever, spreadsheets generally lack higher-order functions as well as code reuse, and in some implementations, also lack recursion. Several extensions have been developed for spreadsheet programs to enable higher-order and reusable functions, but so far remain primarily academic in nature.\n, functional programming paradigms can be suitable for\n-based architectures.\nFunctional programming is an active area of research in the field of\nprogramming language theory\npublication venues focusing on functional programming, including the\nInternational Conference on Functional Programming\nJournal of Functional Programming\nSymposium on Trends in Functional Programming\nFunctional programming has been employed in a wide range of industrial applications. For example,\n, which was developed by the\nin the late 1980s, was originally used to implement\nfault-tolerant telecommunications\nbut has since become popular for building a range of applications at companies such as\nÉlectricité de France\n[ 10 ] [ 12 ] [ 117 ] [ 118 ] [ 119 ] Scheme\n, was used as the basis for several applications on early\nand has been applied to problems such as training-\nsimulation software [ 5 ]\n, which was introduced in the mid-1990s, has seen commercial use in areas such as financial analysis,\nverification, industrial\nprogramming and static analysis of\n, though initially intended as a research language,\nhas also been applied in areas such as aerospace systems, hardware design and web programming.\nOther functional programming languages that have seen use in industry include\n[ 18 ] [ 19 ] Wolfram Language\n[ 121 ] Standard ML [ 122 ] [ 123 ]\nScala has been widely used in\nare some of the functional frontend programming languages used in production.\n's Phoenix framework is also used by some relatively popular commercial projects, such as\n(one of the biggest e-commerce platforms in Poland)\n's classified ads platform\nAllegro Lokalnie. [ 130 ]\nFunctional \"platforms\" have been popular in finance for risk analytics (particularly with large investment banks). Risk factors are coded as functions that form interdependent graphs (categories) to measure correlations in market shifts, similar in manner to\noptimizations but also for regulatory frameworks such as\nComprehensive Capital Analysis and Review\n. Given the use of OCaml and\nvariations in finance, these systems are sometimes considered related to a\ncategorical abstract machine\n. Functional programming is heavily influenced by\nteach functional programming.\n[ 131 ] [ 132 ] [ 133 ] [ 134 ]\nSome treat it as an introductory programming concept\nwhile others first teach imperative programming methods.\nOutside of computer science, functional programming is used to teach problem-solving, algebraic and geometric concepts.\nIt has also been used to teach classical mechanics, as in the book\nStructure and Interpretation of Classical Mechanics\nhas been a relatively popular choice for teaching programming for years.\n[ edit ] Computer programming portal Eager evaluation Functional reactive programming Inductive functional programming List of functional programming languages List of functional programming topics Nested function Purely functional programming\nNotes and references\n[ edit ] ^ Hudak, Paul (September 1989). \"Conception, evolution, and application of functional programming languages\" (PDF) . ACM Computing Surveys . 21 (3): 359– 411. doi : 10.1145/72551.72554 . S2CID 207637854 . Archived from the original (PDF) on 2016-01-31 . Retrieved 2013-08-10 . ^ a b Hughes, John (1984). \"Why Functional Programming Matters\" . ^ a b Clinger, Will (1987). \"MultiTasking and MacScheme\" . MacTech . 3 (12) . Retrieved 2008-08-28 . ^ a b Hartheimer, Anne (1987). \"Programming a Text Editor in MacScheme+Toolsmith\" . MacTech . 3 (1). Archived from the original on 2011-06-29 . Retrieved 2008-08-28 . ^ a b Kidd, Eric. Terrorism Response Training in Scheme . CUFP 2007. Archived from the original on 2010-12-21 . Retrieved 2009-08-26 . ^ a b Cleis, Richard. Scheme in Space . CUFP 2006. Archived from the original on 2010-05-27 . Retrieved 2009-08-26 . ^ a b \"Wolfram Language Guide: Functional Programming\" . 2015 . Retrieved 2015-08-24 . ^ \"Functional vs. Procedural Programming Language\" . Department of Applied Math . University of Colorado. Archived from the original on 2007-11-13 . Retrieved 2006-08-28 . ^ \"State-Based Scripting in Uncharted 2\" (PDF) . Archived from the original (PDF) on 2012-12-15 . Retrieved 2011-08-08 . ^ a b \"Who uses Erlang for product development?\" . Frequently asked questions about Erlang . Retrieved 2018-04-27 . ^ a b Armstrong, Joe (June 2007). \"A history of Erlang\". Proceedings of the third ACM SIGPLAN conference on History of programming languages . Third ACM SIGPLAN Conference on History of Programming Languages. San Diego, California. doi : 10.1145/1238844.1238850 . ISBN 9781595937667 . ^ a b Larson, Jim (March 2009). \"Erlang for concurrent programming\" . Communications of the ACM . 52 (3): 48. doi : 10.1145/1467247.1467263 . S2CID 524392 . ^ \"The Elixir Programming Language\" . Retrieved 2021-02-14 . ^ a b Minsky, Yaron; Weeks, Stephen (July 2008). \"Caml Trading — experiences with functional programming on Wall Street\" . Journal of Functional Programming . 18 (4): 553– 564. doi : 10.1017/S095679680800676X . S2CID 30955392 . ^ a b Leroy, Xavier. Some uses of Caml in Industry (PDF) . CUFP 2007. Archived from the original (PDF) on 2011-10-08 . Retrieved 2009-08-26 . ^ a b \"Haskell in industry\" . Haskell Wiki . Retrieved 2009-08-26 . Haskell has a diverse range of use commercially, from aerospace and defense, to finance, to web startups, hardware design firms and lawnmower manufacturers. ^ a b c Hudak, Paul ; Hughes, J.; Jones, S. P.; Wadler, P. (June 2007). A history of Haskell: being lazy with class . Third ACM SIGPLAN Conference on History of Programming Languages. San Diego, California. doi : 10.1145/1238844.1238856 . Retrieved 2013-09-26 . ^ a b Mansell, Howard (2008). Quantitative Finance in F# . CUFP 2008. Archived from the original on 2015-07-08 . Retrieved 2009-08-29 . ^ a b Peake, Alex (2009). The First Substantial Line of Business Application in F# . CUFP 2009. Archived from the original on 2009-10-17 . Retrieved 2009-08-29 . ^ de Moura, Leonardo; Ullrich, Sebastian (July 2021). \"The Lean 4 Theorem Prover and Programming Language\". Lecture Notes in Artificial Intelligence . Conference on Automated Deduction. Vol. 12699. pp. 625– 635. doi : 10.1007/978-3-030-79876-5_37 . ISSN 1611-3349 . ^ Banz, Matt (2017-06-27). \"An introduction to functional programming in JavaScript\" . Opensource.com . Retrieved 2021-01-09 . ^ \"The useR! 2006 conference schedule includes papers on the commercial use of R\" . R-project.org. 2006-06-08 . Retrieved 2011-06-20 . ^ Chambers, John M. (1998). Programming with Data: A Guide to the S Language . Springer Verlag. pp. 67– 70. ISBN 978-0-387-98503-9 . ^ Novatchev, Dimitre. \"The Functional Programming Language XSLT — A proof through examples\" . Retrieved May 27, 2006 . ^ Mertz, David. \"XML Programming Paradigms (part four): Functional Programming approached to XML processing\" . IBM developerWorks . Retrieved May 27, 2006 . ^ Chamberlin, Donald D. ; Boyce, Raymond F. (1974). \"SEQUEL: A structured English query language\". Proceedings of the 1974 ACM SIGFIDET : 249– 264. ^ Functional Programming with C# - Simon Painter - NDC Oslo 2020 , 8 August 2021, archived from the original on 2021-10-30 , retrieved 2021-10-23 ^ a b \"Functional programming - Kotlin Programming Language\" . Kotlin . Retrieved 2019-05-01 . ^ Dominus, Mark J. (2005). Higher-Order Perl . Morgan Kaufmann . ISBN 978-1-55860-701-9 . ^ Holywell, Simon (2014). Functional Programming in PHP . php[architect]. ISBN 9781940111056 . ^ The Cain Gang Ltd. \"Python Metaclasses: Who? Why? When?\" (PDF) . Archived from the original (PDF) on 30 May 2009 . Retrieved 27 June 2009 . ^ \"GopherCon 2020: Dylan Meeus - Functional Programming with Go\" . YouTube . 22 December 2020. ^ \"Functional Language Features: Iterators and Closures - The Rust Programming Language\" . doc.rust-lang.org . Retrieved 2021-01-09 . ^ Vanderbauwhede, Wim (18 July 2020). \"Cleaner code with functional programming\" . Archived from the original on 28 July 2020 . Retrieved 6 October 2020 . ^ \"Effective Scala\" . Scala Wiki . Archived from the original on 2012-06-19 . Retrieved 2012-02-21 . Effective Scala. ^ \"Documentation for package java.util.function since Java 8 (also known as Java 1.8)\" . Retrieved 2021-06-16 . ^ Turing, A. M. (1937). \"Computability and λ-definability\". The Journal of Symbolic Logic . 2 (4). Cambridge University Press: 153– 163. doi : 10.2307/2268280 . JSTOR 2268280 . S2CID 2317046 . ^ Haskell Brooks Curry; Robert Feys (1958). Combinatory Logic . North-Holland Publishing Company . Retrieved 10 February 2013 . ^ Church, A. (1940). \"A Formulation of the Simple Theory of Types\". Journal of Symbolic Logic . 5 (2): 56– 68. doi : 10.2307/2266170 . JSTOR 2266170 . S2CID 15889861 . ^ McCarthy, John (June 1978). \"History of LISP\". The first ACM SIGPLAN conference on History of programming languages - HOPL-1 (PDF) . Los Angeles, CA. pp. 173– 185. doi : 10.1145/800025.808387 . {{ cite book }} : CS1 maint: location missing publisher ( link ) ^ John McCarthy (1960). \"Recursive functions of symbolic expressions and their computation by machine, Part I.\" (PDF) . Communications of the ACM . 3 (4): 184– 195. doi : 10.1145/367177.367199 . S2CID 1489409 . ^ Guy L. Steele; Richard P. Gabriel (February 1996). \"The evolution of Lisp\". History of programming languages---II (PDF) . pp. 233– 330. doi : 10.1145/234286.1057818 . ISBN 978-0-201-89502-5 . S2CID 47047140 . ^ The memoir of Herbert A. Simon (1991), Models of My Life pp.189-190 ISBN 0-465-04640-1 claims that he, Al Newell, and Cliff Shaw are \"...commonly adjudged to be the parents of [the] artificial intelligence [field],\" for writing Logic Theorist , a program that proved theorems from Principia Mathematica automatically. To accomplish this, they had to invent a language and a paradigm that, viewed retrospectively, embeds functional programming. ^ Landin, Peter J. (1964). \"The mechanical evaluation of expressions\" . The Computer Journal . 6 (4). British Computer Society : 308– 320. doi : 10.1093/comjnl/6.4.308 . ^ Diehl, Stephan; Hartel, Pieter; Sestoft, Peter (2000). \"Abstract machines for programming language implementation\". Future Generation Computer Systems . Vol. 16. pp. 739– 751. ^ Landin, Peter J. (February 1965a). \"Correspondence between ALGOL 60 and Church's Lambda-notation: part I\" . Communications of the ACM . 8 (2). Association for Computing Machinery : 89– 101. doi : 10.1145/363744.363749 . S2CID 6505810 . ^ Landin, Peter J. (March 1965b). \"A correspondence between ALGOL 60 and Church's Lambda-notation: part II\" . Communications of the ACM . 8 (3). Association for Computing Machinery : 158– 165. doi : 10.1145/363791.363804 . S2CID 15781851 . ^ Landin, Peter J. (March 1966b). \"The next 700 programming languages\" . Communications of the ACM . 9 (3). Association for Computing Machinery : 157– 166. doi : 10.1145/365230.365257 . S2CID 13409665 . ^ Backus, J. (1978). \"Can programming be liberated from the von Neumann style?: A functional style and its algebra of programs\" . Communications of the ACM . 21 (8): 613– 641. doi : 10.1145/359576.359579 . ^ R.M. Burstall. Design considerations for a functional programming language. Invited paper, Proc. Infotech State of the Art Conf. \"The Software Revolution\", Copenhagen, 45–57 (1977) ^ R.M. Burstall and J. Darlington. A transformation system for developing recursive programs. Journal of the Association for Computing Machinery 24(1):44–67 (1977) ^ R.M. Burstall, D.B. MacQueen and D.T. Sannella. HOPE: an experimental applicative language. Proceedings 1980 LISP Conference, Stanford, 136–143 (1980). ^ \"Make discovering assign() easier!\" . OpenSCAD . Archived from the original on 2023-04-19. ^ Peter Bright (March 13, 2018). \"Developers love trendy new languages but earn more with functional programming\" . Ars Technica . ^ John Leonard (January 24, 2017). \"The stealthy rise of functional programming\" . Computing. ^ Leo Cheung (May 9, 2017). \"Is functional programming better for your startup?\" . InfoWorld . ^ Sean Tull - Monoidal Categories for Formal Concept Analysis. ^ Pountain, Dick. \"Functional Programming Comes of Age\" . Byte (August 1994) . Archived from the original on 2006-08-27 . Retrieved August 31, 2006 . ^ a b \"ISO/IEC JTC 1/SC 22/WG5/N2137 – Fortran 2015 Committee Draft (J3/17-007r2)\" (PDF) . International Organization for Standardization. July 6, 2017. pp. 336– 338. ^ \"Revised^6 Report on the Algorithmic Language Scheme\" . R6rs.org . Retrieved 2013-03-21 . ^ \"Revised^6 Report on the Algorithmic Language Scheme - Rationale\" . R6rs.org . Retrieved 2013-03-21 . ^ Clinger, William (1998). \"Proper tail recursion and space efficiency\". Proceedings of the ACM SIGPLAN 1998 conference on Programming language design and implementation - PLDI '98 . pp. 174– 185. doi : 10.1145/277650.277719 . ISBN 0897919874 . S2CID 16812984 . ^ Baker, Henry (1994). \"CONS Should Not CONS Its Arguments, Part II: Cheney on the M.T.A.\" Archived from the original on 2006-03-03 . Retrieved 2020-04-29 . ^ Turner, D.A. (2004-07-28). \"Total Functional Programming\" . Journal of Universal Computer Science . 10 (7): 751– 768. doi : 10.3217/jucs-010-07-0751 . ^ The Implementation of Functional Programming Languages . Simon Peyton Jones, published by Prentice Hall, 1987 ^ a b Launchbury, John (March 1993). A Natural Semantics for Lazy Evaluation . Symposium on Principles of Programming Languages. Charleston, South Carolina: ACM . pp. 144– 154. doi : 10.1145/158511.158618 . ^ Robert W. Harper (2009). Practical Foundations for Programming Languages (PDF) . Archived from the original (PDF) on 2016-04-07. ^ Huet, Gérard P. (1973). \"The Undecidability of Unification in Third Order Logic\". Information and Control . 22 (3): 257– 267. doi : 10.1016/s0019-9958(73)90301-x . ^ Huet, Gérard (Sep 1976). Resolution d'Equations dans des Langages d'Ordre 1,2,...ω (Ph.D.) (in French). Universite de Paris VII. ^ Huet, Gérard (2002). \"Higher Order Unification 30 years later\" (PDF) . In Carreño, V.; Muñoz, C.; Tahar, S. (eds.). Proceedings, 15th International Conference TPHOL . LNCS. Vol. 2410. Springer. pp. 3– 12. ^ Wells, J. B. (1993). \"Typability and type checking in the second-order lambda-calculus are equivalent and undecidable\". Tech. Rep. 93-011 : 176– 185. CiteSeerX 10.1.1.31.3590 . ^ Leroy, Xavier (17 September 2018). \"The Compcert verified compiler\" . ^ Peyton Jones, Simon; Vytiniotis, Dimitrios; Weirich, Stephanie ; Geoffrey Washburn (April 2006). \"Simple unification-based type inference for GADTs\" . Icfp 2006 : 50– 61. ^ \"OCaml Manual\" . caml.inria.fr . Retrieved 2021-03-08 . ^ \"Algebraic Data Types\" . Scala Documentation . Retrieved 2021-03-08 . ^ Kennedy, Andrew; Russo, Claudio V. (October 2005). Generalized Algebraic Data Types and Object-Oriented Programming (PDF) . OOPSLA. San Diego, California: ACM . doi : 10.1145/1094811.1094814 . ISBN 9781595930316 . Archived from the original on 2006-12-29. ^ Hughes, John. \"Why Functional Programming Matters\" (PDF) . Chalmers University of Technology . ^ Purely functional data structures by Chris Okasaki , Cambridge University Press , 1998, ISBN 0-521-66350-4 ^ L’orange, Jean Niklas. \"polymatheia - Understanding Clojure's Persistent Vector, pt. 1\" . Polymatheia . Retrieved 2018-11-13 . ^ Michael Barr, Charles Well - Category theory for computer science. ^ Newbern, J. \"All About Monads: A comprehensive guide to the theory and practice of monadic programming in Haskell\" . Retrieved 2008-02-14 . ^ \"Thirteen ways of looking at a turtle\" . fF# for fun and profit . Retrieved 2018-11-13 . ^ Hartmanis, Juris; Hemachandra, Lane (1986). \"Complexity classes without machines: On complete languages for UP\". Automata, Languages and Programming . Lecture Notes in Computer Science. Vol. 226. Berlin, Heidelberg: Springer Berlin Heidelberg. pp. 123– 135. doi : 10.1007/3-540-16761-7_62 . ISBN 978-3-540-16761-7 . Retrieved 2024-12-12 . ^ Paulson, Larry C. (28 June 1996). ML for the Working Programmer . Cambridge University Press. ISBN 978-0-521-56543-1 . Retrieved 10 February 2013 . ^ Spiewak, Daniel (26 August 2008). \"Implementing Persistent Vectors in Scala\" . Code Commit . Archived from the original on 23 September 2015 . Retrieved 17 April 2012 . ^ \"Which programs are fastest? | Computer Language Benchmarks Game\" . benchmarksgame.alioth.debian.org. Archived from the original on 2013-05-20 . Retrieved 2011-06-20 . ^ Igor Pechtchanski; Vivek Sarkar (2005). \"Immutability specification and its applications\". Concurrency and Computation: Practice and Experience . 17 ( 5– 6): 639– 662. doi : 10.1002/cpe.853 . S2CID 34527406 . ^ \"An In-Depth Look at Clojure Collections\" . InfoQ . Retrieved 2024-04-29 . ^ \"References and Borrowing - The Rust Programming Language\" . doc.rust-lang.org . Retrieved 2024-04-29 . ^ \"Validating References with Lifetimes - The Rust Programming Language\" . doc.rust-lang.org . Retrieved 2024-04-29 . ^ \"Concurrent Collections (The Java™ Tutorials \u003e Essential Java Classes \u003e Concurrency)\" . docs.oracle.com . Retrieved 2024-04-29 . ^ \"Understanding The Actor Model To Build Non-blocking, High-throughput Distributed Systems - Scaleyourapp\" . scaleyourapp.com . 2023-01-28 . Retrieved 2024-04-29 . ^ Cesarini, Francesco; Thompson, Simon (2009). Erlang programming: a concurrent approach to software development (1st ed.). O'Reilly Media, Inc. (published 2009-06-11). p. 6. ISBN 978-0-596-55585-6 . ^ \"Chapter 25. Profiling and optimization\" . Book.realworldhaskell.org . Retrieved 2011-06-20 . ^ Berthe, Samuel (2024-04-29), samber/lo , retrieved 2024-04-29 ^ \"Go Wiki: Compiler And Runtime Optimizations - The Go Programming Language\" . go.dev . Retrieved 2024-04-29 . ^ \"Comparing Performance: Loops vs. Iterators - The Rust Programming Language\" . doc.rust-lang.org . Retrieved 2024-04-29 . ^ Hartel, Pieter; Henk Muller; Hugh Glaser (March 2004). \"The Functional C experience\" (PDF) . Journal of Functional Programming . 14 (2): 129– 135. doi : 10.1017/S0956796803004817 . S2CID 32346900 . Archived from the original (PDF) on 2011-07-19 . Retrieved 2006-05-28 . ; David Mertz. \"Functional programming in Python, Part 3\" . IBM developerWorks . Archived from the original on 2007-10-16 . Retrieved 2006-09-17 . ( Part 1 , Part 2 ) ^ \"Functions — D Programming Language 2.0\" . Digital Mars. 30 December 2012. ^ \"Lua Unofficial FAQ (uFAQ)\" . ^ \"First-Class Functions in Go - The Go Programming Language\" . golang.org . Retrieved 2021-01-04 . ^ Eich, Brendan (3 April 2008). \"Popularity\" . ^ van Rossum, Guido (2009-04-21). \"Origins of Python's \"Functional\" Features\" . Retrieved 2012-09-27 . ^ \"functools — Higher order functions and operations on callable objects\" . Python Software Foundation. 2011-07-31 . Retrieved 2011-07-31 . ^ Skarsaune, Martin (2008). The SICS Java Port Project Automatic Translation of a Large Object Oriented System from Smalltalk to Java . ^ Gosling, James. \"Closures\" . James Gosling: on the Java Road . Oracle. Archived from the original on 2013-04-14 . Retrieved 11 May 2013 . ^ Williams, Michael (8 April 2013). \"Java SE 8 Lambda Quick Start\" . ^ Bloch, Joshua (2008). \"Item 15: Minimize Mutability\". Effective Java (Second ed.). Addison-Wesley. ISBN 978-0321356680 . ^ \"Object.freeze() - JavaScript | MDN\" . developer.mozilla.org . Retrieved 2021-01-04 . The Object.freeze() method freezes an object. A frozen object can no longer be changed; freezing an object prevents new properties from being added to it, existing properties from being removed, prevents changing the enumerability, configurability, or writability of existing properties, and prevents the values of existing properties from being changed. In addition, freezing an object also prevents its prototype from being changed. freeze() returns the same object that was passed in. ^ Daniel Friedman; William Byrd; Oleg Kiselyov; Jason Hemann (2018). The Reasoned Schemer, Second Edition . The MIT Press. ^ A. Casas, D. Cabeza, M. V. Hermenegildo. A Syntactic Approach to\nCombining Functional Notation, Lazy Evaluation and Higher-Order in\nLP Systems. The 8th International Symposium on Functional and Logic\nProgramming (FLOPS'06), pages 142-162, April 2006. ^ \"How I do my Computing\" . stallman.org . Retrieved 2024-04-29 . ^ \"Helix\" . helix-editor.com . Retrieved 2024-04-29 . ^ Wakeling, David (2007). \"Spreadsheet functional programming\" (PDF) . Journal of Functional Programming . 17 (1): 131– 143. doi : 10.1017/S0956796806006186 . ISSN 0956-7968 . S2CID 29429059 . ^ Peyton Jones, Simon ; Burnett, Margaret ; Blackwell, Alan (March 2003). \"Improving the world's most popular functional language: user-defined functions in Excel\" . Archived from the original on 2005-10-16. ^ Rodger, Richard (11 December 2017). The Tao of Microservices . Manning. ISBN 9781638351733 . ^ Piro, Christopher (2009). Functional Programming at Facebook . CUFP 2009. Archived from the original on 2009-10-17 . Retrieved 2009-08-29 . ^ \"Sim-Diasca: a large-scale discrete event concurrent simulation engine in Erlang\" . November 2011. Archived from the original on 2013-09-17 . Retrieved 2011-11-08 . ^ 1 million is so 2011 Archived 2014-02-19 at the Wayback Machine // WhatsApp blog, 2012-01-06: \"the last important piece of our infrastracture is Erlang\" ^ Momtahan, Lee (2009). Scala at EDF Trading: Implementing a Domain-Specific Language for Derivative Pricing with Scala . CUFP 2009. Archived from the original on 2009-10-17 . Retrieved 2009-08-29 . ^ Graham, Paul (2003). \"Beating the Averages\" . Retrieved 2009-08-29 . ^ Sims, Steve (2006). Building a Startup with Standard ML (PDF) . CUFP 2006 . Retrieved 2009-08-29 . ^ Laurikari, Ville (2007). Functional Programming in Communications Security . CUFP 2007. Archived from the original on 2010-12-21 . Retrieved 2009-08-29 . ^ Lorimer, R. J. (19 January 2009). \"Live Production Clojure Application Announced\" . InfoQ . ^ Bugnion, Pascal (2016). Scala for Data Science (1st ed.). Packt . ISBN 9781785281372 . ^ \"Why developers like ClojureScript\" . StackShare . Retrieved 2024-04-29 . ^ Herrick, Justin (2024-04-29), jah2488/elm-companies , retrieved 2024-04-29 ^ \"Why developers like PureScript\" . StackShare . Retrieved 2024-04-29 . ^ Team, Editorial (2019-01-08). \"ALLEGRO - all you need to know about the best Polish online marketplace\" . E-commerce Germany News . Retrieved 2024-04-29 . ^ \"Websites using Phoenix Framework - Wappalyzer\" . www.wappalyzer.com . Retrieved 2024-04-29 . ^ \"Functional Programming: 2019-2020\" . University of Oxford Department of Computer Science . Retrieved 28 April 2020 . ^ \"Programming I (Haskell)\" . Imperial College London Department of Computing . Retrieved 28 April 2020 . ^ a b \"Computer Science BSc - Modules\" . Retrieved 28 April 2020 . ^ a b Abelson, Hal ; Sussman, Gerald Jay (1985). \"Preface to the Second Edition\" . Structure and Interpretation of Computer Programs (2 ed.). MIT Press. Bibcode : 1985sicp.book.....A . ^ John DeNero (Fall 2019). \"Computer Science 61A, Berkeley\" . Department of Electrical Engineering and Computer Sciences, Berkeley . Retrieved 2020-08-14 . ^ Emmanuel Schanzer of Bootstrap interviewed on the TV show Triangulation on the TWiT.tv network ^ \"Why Scheme for Introductory Programming?\" . home.adelphi.edu . Retrieved 2024-04-29 . ^ Staff, IMACS (2011-06-03). \"What Is Scheme \u0026 Why Is it Beneficial for Students?\" . IMACS – Making Better Thinkers for Life . Retrieved 2024-04-29 .\n[ edit ] Abelson, Hal ; Sussman, Gerald Jay (1985). Structure and Interpretation of Computer Programs . MIT Press. Bibcode : 1985sicp.book.....A .\nCousineau, Guy and Michel Mauny.\nThe Functional Approach to Programming\nCambridge University Press\nCurry, Haskell Brooks and Feys, Robert and Craig, William.\n. Volume I. North-Holland Publishing Company, Amsterdam, 1958.\nCurry, Haskell B. ; Hindley, J. Roger ; Seldin, Jonathan P. (1972). Combinatory Logic . Vol. II. Amsterdam: North Holland. ISBN 978-0-7204-2208-5 .\nDominus, Mark Jason.\nFelleisen, Matthias; Findler, Robert; Flatt, Matthew; Krishnamurthi, Shriram (2018). How to Design Programs . MIT Press.\n. Englewood Cliffs, New Jersey:\nFunctional Programming: Practice and Theory\n. Addison-Wesley, 1990.\nMichaelson, Greg (10 April 2013). An Introduction to Functional Programming Through Lambda Calculus . Courier Corporation. ISBN 978-0-486-28029-5 . O'Sullivan, Brian; Stewart, Don; Goerzen, John (2008). Real World Haskell . O'Reilly.\nPratt, Terrence W. and\nMarvin Victor Zelkowitz\nProgramming Languages: Design and Implementation\n. 3rd ed. Englewood Cliffs, New Jersey:\nFunctional and Logic Programming Languages\n. Vol. 4 of Handbook of Programming Languages. Indianapolis, Indiana:\nMacmillan Technical Publishing\nHaskell: The Craft of Functional Programming\nAddison-Wesley Longman Limited\n[ edit ] Listen to this article\nwas created from a revision of this article dated 25 August 2011\n, and does not reflect subsequent edits.\nMore spoken articles\nFord, Neal. \"Functional thinking\" . Retrieved 2021-11-10 . Akhmechet, Slava (2006-06-19). \"defmacro – Functional Programming For The Rest of Us\" . Retrieved 2013-02-24 .\nFunctional programming in Python\npart 3 v t e Programming paradigms\nComparison by language\nImperative Structured Jackson structures Block-structured Modular Non-structured Procedural Programming in the large and in the small Design by contract Invariant-based Nested function Object-oriented\nObject-based Agent Immutable object Persistent Uniform function call syntax Declarative Functional\nRecursive Anonymous function\nHigher-order Purely functional Total Strict GADTs Dependent types Functional logic Point-free style Expression-oriented Applicative\nConcatenative Function-level\nValue-level Dataflow Flow-based Reactive\nSignals Streams Synchronous Logic Abductive logic Answer set Constraint\nInductive logic Nondeterministic Ontology Probabilistic logic Query DSL Algebraic modeling Array Automata-based\nDifferentiable End-user Grammar-oriented Interface description Language-oriented List comprehension Low-code Modeling Natural language Non-English-based Page description Pipes\nfilters Probabilistic Quantum Scientific Scripting Set-theoretic Simulation Stack-based System Tactile Templating Transformation\nparallel Actor-based Automatic mutual exclusion Choreographic programming Concurrent logic\nConcurrent constraint logic\nConcurrent OO Macroprogramming Multitier programming Organic computing Parallel programming models Partitioned global address space Process-oriented Relativistic programming Service-oriented Structured concurrency Metaprogramming Attribute-oriented Automatic\nDynamic Extensible Generic Homoiconicity Interactive Macro\nMetalinguistic abstraction Multi-stage Program synthesis\nReflective Self-modifying code Symbolic Template Separation of concerns Aspects Components Data-driven Data-oriented Event-driven Features Literate Roles Subjects v t e Types of programming languages\nMachine Assembly Compiled Interpreted Low-level High-level Very high-level Esoteric Generation First Second Third Fourth Fifth Authority control databases\nGermany United States France BnF data Czech Republic Spain Israel\nhttps://en.wikipedia.org/w/index.php?title=Functional_programming\u0026oldid=1305109304\nFunctional programming Programming paradigms Programming language comparisons\nCS1 maint: location missing publisher CS1 French-language sources (fr) Webarchive template wayback links Articles with short description Short description matches Wikidata All articles with unsourced statements Articles with unsourced statements from February 2017 Articles with unsourced statements from July 2018 Articles with unsourced statements from June 2014 Articles with unsourced statements from April 2015 Articles with unsourced statements from August 2022 Articles with hAudio microformats Spoken articles Articles with example C code Articles with example JavaScript code Articles with example Lisp (programming language) code Search Search Functional programming 54 languages Add topic",
          "role": "teacher"
        },
        {
          "content": "Why is this important to understand?",
          "role": "student"
        },
        {
          "content": "Understanding Functional Programming is crucial for building efficient, scalable, and maintainable software systems. It forms the foundation for solving complex computational problems.",
          "role": "teacher"
        },
        {
          "content": "Can you give me a concrete example?",
          "role": "student"
        },
        {
          "content": "For example, Functional Programming can be applied in scenarios where you need to optimize performance, manage complexity, or implement scalable solutions.",
          "role": "teacher"
        }
      ],
      "format": "socratic_dialogue",
      "metadata": {
        "domain": "Programming",
        "source": "https://en.wikipedia.org/wiki/Functional_programming",
        "words": 10268
      },
      "topic": "Functional Programming"
    },
    {
      "conversation": [
        {
          "content": "How do I implement Functional Programming?",
          "role": "user"
        },
        {
          "content": "Here's how to implement Functional Programming:\n\n2. Choose appropriate data structures\n3. Handle edge cases and validation\n4. Optimize for your use case\n5. Test thoroughly with various inputs\n\nBasic structure:\n```\nfunction implementFunctionalProgramming(input) {\n    // Initialize\n    // Process\n    // Return result\n}\n```",
          "role": "assistant"
        }
      ],
      "format": "implementation_guide",
      "metadata": {
        "domain": "Programming",
        "type": "code_focused"
      },
      "topic": "Functional Programming"
    },
    {
      "content": {
        "applications": "application\nproved that the lambda calculus and\nare equivalent models of computation,\nshowing that the lambda calculus is\n. Lambda calculus forms the basis of all functional programming languages. An equivalent theoretical formulation,\nin the 1920s and 1930s.\nChurch later developed a weaker system, the\nsimply typed lambda calculus\n, which extended the lambda calculus by assigning a\nThis forms the basis for statically typed functional programming.\nfunctional programming language,\n, was developed ",
        "considerations": "consider another function such as\nint plusone ( int x ) { return x + 1 ;} is\ntransparent, as it does not implicitly change the input x and thus has no such\nFunctional programs exclusively use this type of function and are therefore referentially transparent.\nPurely functional data structure\nare often represented in a different way to their\nwith constant access and update times is a basic component",
        "key_concepts": [
          "based architectures."
        ],
        "overview": "A comprehensive technical concept with wide applications."
      },
      "format": "conceptual_breakdown",
      "metadata": {
        "domain": "Programming",
        "structure": "hierarchical"
      },
      "topic": "Functional Programming"
    },
    {
      "format": "problem_solving",
      "scenario": {
        "approach": "solve this issue by implementing mechanisms for safe memory sharing between\ndistinguishes itself by its approach to data immutability which involves immutable\nand a concept called\nImmutable data with separation of identity and state and\nschemes can also potentially be more well-suited for\nconcurrent and parallel\nprogramming by the virtue of reducing or eliminating the risk of certain concurrency hazards, since concurrent operations are usually\nand this allows eliminating the need for locks. This",
        "benefits": "benefits of dependently typed programming while avoiding most of its inconvenience.\nGADT's are available in the\nGlasgow Haskell Compiler\nand have been proposed as additions to other languages including Java and C#.\nReferential transparency\nReferential transparency\nFunctional programs do not have ass",
        "problem": "How can Functional Programming solve real-world problems?",
        "tradeoffs": "costly, some functional programming languages, like\nsolve this issue by implementing mechanisms for safe memory sharing between\ndistinguishes itself by its approach to data immutability which involves immutable\nand a concept called\nImmutable data with separation of identity and state and\nschemes can"
      },
      "topic": "Functional Programming"
    },
    {
      "comparison": "Comparison to imperative programming\nFunctional programming is very different from\nimperative programming\n. The most significant differences stem from the fact that functional programming avoids\n, which are used in imperative programming to implement state and I/O. Pure functional programming completely prevents side-effects and provides referential transparency.\nHigher-order functions are rarely used in older imperative programming. A traditional imperative program might use a loop to traverse ",
      "format": "comparative_analysis",
      "topic": "Functional Programming"
    },
    {
      "dialogue": [
        {
          "content": "What is Design Patterns really about at its core?",
          "role": "student"
        },
        {
          "content": "1994 software engineering book\nsoftware design pattern\nmay be in need of reorganization to comply with Wikipedia's layout guidelines\nPlease help by editing the article to make improvements to the overall structure. ( July 2013 ) ( Learn how and when to remove this message ) Design Patterns: Elements of Reusable Object-Oriented Software\nErich Gamma Richard Helm Ralph Johnson John Vlissides\nsoftware engineering\nobject-oriented programming\nISBN 0-201-63361-2 OCLC 31171684 Dewey Decimal\nDesign Patterns: Elements of Reusable Object-Oriented Software\nsoftware engineering\nsoftware design patterns\n. The book was written by\n, with a foreword by\n. The book is divided into two parts, with the first two chapters exploring the capabilities and pitfalls of\nobject-oriented programming\n, and the remaining chapters describing 23 classic\nsoftware design patterns\n. The book includes examples in\nIt has been influential to the field of software engineering and is regarded as an important source for object-oriented design theory and practice. More than 500,000 copies have been sold in English and in 13 other languages.\nThe authors are often referred to as the\n[ 2 ] [ 3 ] [ 4 ] [ 5 ]\nDevelopment and publication history\nThe book started at a birds-of-a-feather session at the 1990\nmeeting, \"Towards an Architecture Handbook\", where Erich Gamma and Richard Helm met and discovered their common interest. They were later joined by Ralph Johnson and John Vlissides.\nThe book was originally published on 21 October 1994, with a 1995 copyright, and was made available to the public at the 1994 OOPSLA meeting.\nmay contain an excessive amount of intricate detail that may only interest a particular audience\nPlease help by spinning off or relocating any relevant information, and removing excessive detail that may be against Wikipedia's inclusion policy . ( October 2020 ) ( Learn how and when to remove this message )\nChapter 1 is a discussion of\ndesign techniques, based on the authors' experience, which they believe would lead to good object-oriented software design, including:\n\"Program to an interface, not an implementation.\" (Gang of Four 1995:18)\nComposition over inheritance\n'.\" (Gang of Four 1995:20)\nThe authors claim the following as advantages of\nover implementation:\nclients remain unaware of the specific types of objects they use, as long as the object adheres to the interface\nclients remain unaware of the classes that implement these objects; clients only know about the abstract class(es) defining the interface\nUse of an interface also leads to\n, which are central features of object-oriented programming.\nThe authors refer to\n, with white-box referring to visibility, because the internals of parent classes are often visible to\n. In contrast, the authors refer to\n(in which objects with well-defined interfaces are used dynamically at runtime by objects obtaining references to other objects) as\nbecause no internal details of composed objects need be visible in the code using them.\nThe authors discuss the tension between inheritance and encapsulation at length and state that in their experience, designers overuse inheritance (Gang of Four 1995:20). The danger is stated as follows:\n\"Because inheritance exposes a\nto details of its parent's implementation, it's often said that 'inheritance breaks encapsulation'\". (Gang of Four 1995:19)\nThey warn that the implementation of a subclass can become so bound up with the implementation of its parent class that any change in the parent's implementation will force the subclass to change. Furthermore, they claim that a way to avoid this is to inherit only from abstract classes—but then, they point out that there is minimal code reuse.\nUsing inheritance is recommended mainly when adding to the functionality of existing components, reusing most of the old code and adding relatively small amounts of new code.\nTo the authors, 'delegation' is an extreme form of object composition that can always be used to replace inheritance. Delegation involves two objects: a 'sender' passes itself to a 'delegate' to let the delegate refer to the sender. Thus the link between two parts of a system are established only at runtime, not at compile-time. The\narticle has more information about delegation.\nThe authors also discuss so-called parameterized types, which are also known as\n). These allow any type to be defined without specifying all the other types it uses—the unspecified types are supplied as 'parameters' at the point of use.\nThe authors admit that delegation and parameterization are very powerful but add a warning:\n\"Dynamic, highly parameterized software is harder to understand and build than more static software.\" (Gang of Four 1995:21)\nThe authors further distinguish between '\n', where one object 'has' or 'is part of' another object (implying that an aggregate object and its owner have identical lifetimes) and acquaintance, where one object merely 'knows of' another object. Sometimes acquaintance is called 'association' or the 'using' relationship. Acquaintance objects may request operations of each other, but they are not responsible for each other. Acquaintance is a weaker relationship than aggregation and suggests much\nbetween objects, which can often be desirable for maximum maintainability in designs.\nThe authors employ the term 'toolkit' where others might today use 'class library', as in C# or Java. In their parlance, toolkits are the object-oriented equivalent of subroutine libraries, whereas a '\n' is a set of cooperating classes that make up a reusable design for a specific class of software. They state that applications are hard to design, toolkits are harder, and frameworks are the hardest to design.\nCreational pattern Creational patterns\nare ones that create objects, rather than having to instantiate objects directly. This gives the program more flexibility in deciding which objects need to be created for a given case.\ngroups object factories that have a common theme.\nconstructs complex objects by separating construction and representation.\ncreates objects without specifying the exact class to create.\ncreates objects by cloning an existing object.\nrestricts object creation for a class to only one instance.\n[ edit ] Structural patterns\nconcern class and object composition. They use inheritance to compose interfaces and define ways to compose objects to obtain new functionality.\nallows classes with incompatible interfaces to work together by wrapping its own interface around that of an already existing class.\ndecouples an abstraction from its implementation so that the two can vary independently.\ncomposes zero-or-more similar objects so that they can be manipulated as one object.\ndynamically adds/overrides behavior in an existing method of an object.\nprovides a simplified interface to a large body of code.\nreduces the cost of creating and manipulating a large number of similar objects.\nprovides a placeholder for another object to control access, reduce cost, and reduce complexity.\nbehavioral design patterns\nare specifically concerned with communication between objects.\nChain of responsibility\ndelegates commands to a chain of processing objects.\ncreates objects that encapsulate actions and parameters.\nimplements a specialized language.\naccesses the elements of an object sequentially without exposing its underlying representation.\nbetween classes by being the only class that has detailed knowledge of their methods.\nprovides the ability to restore an object to its previous state (undo).\nis a publish/subscribe pattern, which allows a number of observer objects to see an event.\nallows an object to alter its behavior when its internal state changes.\nallows one of a family of algorithms to be selected on-the-fly at runtime.\ndefines the skeleton of an algorithm as an abstract class, allowing its subclasses to provide concrete behavior.\nseparates an algorithm from an object structure by moving the hierarchy of methods into one object.\nawarded that year's Programming Languages Achievement Award to the authors, in recognition of the impact of their work \"on programming practice and\nprogramming language\nCriticism has been directed at the concept of\nsoftware design patterns\nspecifically. A primary criticism of\nis that its patterns are simply workarounds for missing features in C++, replacing elegant abstract features with lengthy concrete patterns, essentially becoming a \"human compiler\".\nWhen I see patterns in my programs, I consider it a sign of trouble. The shape of a program should reflect only the problem it needs to solve. Any other regularity in the code is a sign, to me at least, that I'm using abstractions that aren't powerful enough-- often that I'm generating by hand the expansions of some macro that I need to write.\ndemonstrates that 16 out of the 23 patterns in\nare simplified or eliminated by language features in\nRelated observations were made by Hannemann and\nwho implemented several of the 23 design patterns using an\naspect-oriented programming\n) and showed that code-level dependencies were removed from the implementations of 17 of the 23 design patterns and that aspect-oriented programming could simplify the implementations of design patterns.\nIn an interview with InformIT in 2009, Erich Gamma stated that the book authors had a discussion in 2005 on how they would have refactored the book and concluded that they would have recategorized some patterns and added a few additional ones, such as extension object/interface, dependency injection, type object, and null object. Gamma wanted to remove the singleton pattern, but there was no consensus among the authors to do so.\n[ edit ] Software design pattern Enterprise Integration Patterns GRASP (object-oriented design) Pedagogical patterns\n[ edit ] ^ Zehoo, Edmund (26 January 2010). Zehoo, Edmund (ed.). Pro ODP .NET for Oracle Database 11g . Apress. pp. 351– 371. doi : 10.1007/978-1-4302-2821-9_13 – via Springer Link. ^ Hussain, Shahid; Keung, Jacky; Khan, Arif Ali (2017). \"The Effect of Gang-of-Four Design Patterns Usage on Design Quality Attributes\" . 2017 IEEE International Conference on Software Quality, Reliability and Security (QRS) . pp. 263– 273. doi : 10.1109/QRS.2017.37 . ISBN 978-1-5386-0592-9 . S2CID 21343926 . ^ Hunt, John (26 January 2013). Hunt, John (ed.). Scala Design Patterns: Patterns for Practical Reuse and Design . Springer International Publishing. pp. 135– 136. doi : 10.1007/978-3-319-02192-8_16 – via Springer Link. ^ Almadi, Sara H. S.; Hooshyar, Danial; Ahmad, Rodina Binti (26 January 2021). \"Bad Smells of Gang of Four Design Patterns: A Decade Systematic Literature Review\" . Sustainability . 13 (18): 10256. doi : 10.3390/su131810256 . ^ Monteiro, Miguel Pessoa; Fernandes, João M. (26 January 2004). Pitfalls of aspectJ implementations of some of the gang-of-four design patterns . Universidad de Extremadura. ISBN 978-84-688-8889-7 – via repositorium.uminho.pt. ^ Richard Helm ^ \"SIGPLAN FY '05 Annual Report\" (PDF) . ^ Graham, Paul (2002). Revenge of the Nerds . Retrieved 11 August 2012 . ^ Norvig, Peter (1998). Design Patterns in Dynamic Languages . ^ Hannemann, Jan (2002). Design pattern implementation in Java and AspectJ . ^ Gamma, Erich; Helm, Richard; Johnson, Ralph (22 October 2009). \"Design Patterns 15 Years Later: An Interview with Erich Gamma, Richard Helm, and Ralph Johnson\" . InformIT (Interview). Interviewed by Larry O'Brien. Archived from the original on 20 February 2019 . Retrieved 1 September 2019 . v t e Software design patterns Gang of Four patterns Creational Abstract factory Builder Factory method Prototype Singleton Structural Adapter Bridge Composite Decorator Facade Flyweight Proxy Behavioral Chain of responsibility Command Interpreter Iterator Mediator Memento Observer State Strategy Template method Visitor Concurrency patterns Active object Balking Binding properties Double-checked locking Event-based asynchronous Guarded suspension Join Lock Monitor Proactor Reactor Read–write lock Scheduler Scheduled-task pattern Semaphore Thread pool Thread-local storage Architectural patterns Front controller Interceptor MVC MVP MVVM ADR ECS n -tier Specification Publish–subscribe Naked objects Service locator Active record Identity map Data access object Data transfer object Inversion of control Model 2 Broker\nBlackboard Business delegate Composite entity Dependency injection Guard clause Intercepting filter Lazy loading Mock object Null object Object pool Servant Twin Type tunnel Method chaining Delegation\nDesign Patterns Enterprise Integration Patterns\nChristopher Alexander Erich Gamma Ralph Johnson John Vlissides Grady Booch Kent Beck Ward Cunningham Martin Fowler Robert Martin Jim Coplien Douglas Schmidt Linda Rising\nThe Hillside Group Portland Pattern Repository\nAnti-pattern Architectural pattern\nhttps://en.wikipedia.org/w/index.php?title=Design_Patterns\u0026oldid=1303147389\nSoftware engineering books Software design patterns 1994 non-fiction books Addison-Wesley books\nArticles with short description Short description matches Wikidata Wikipedia articles needing reorganization from July 2013 Use dmy dates from January 2024 Use American English from January 2024 All Wikipedia articles written in American English Wikipedia articles that are excessively detailed from October 2020 All articles that are excessively detailed Wikipedia articles with style issues from October 2020 All articles with style issues Search Search Design Patterns 14 languages Add topic",
          "role": "teacher"
        },
        {
          "content": "Why is this important to understand?",
          "role": "student"
        },
        {
          "content": "Understanding Design Patterns is crucial for building efficient, scalable, and maintainable software systems. It forms the foundation for solving complex computational problems.",
          "role": "teacher"
        },
        {
          "content": "Can you give me a concrete example?",
          "role": "student"
        },
        {
          "content": "For example, Design Patterns can be applied in scenarios where you need to optimize performance, manage complexity, or implement scalable solutions.",
          "role": "teacher"
        }
      ],
      "format": "socratic_dialogue",
      "metadata": {
        "domain": "Engineering",
        "source": "https://en.wikipedia.org/wiki/Design_Patterns",
        "words": 2291
      },
      "topic": "Design Patterns"
    },
    {
      "content": {
        "applications": "applications are hard to design, toolkits are harder, and frameworks are the hardest to design.\nCreational pattern Creational patterns\nare ones that create objects, rather than having to instantiate objects directly. This gives the program more flexibility in deciding which objects need to be created for a given case.\ngroups object factories that have a common theme.\nconstructs complex objects by separating construction and representation.\ncreates objects without specifying the exact class to cr",
        "considerations": "consider it a sign of trouble. The shape of a program should reflect only the problem it needs to solve. Any other regularity in the code is a sign, to me at least, that I'm using abstractions that aren't powerful enough-- often that I'm generating by hand the expansions of some macro that I need to write.\ndemonstrates that 16 out of the 23 patterns in\nare simplified or eliminated by language feat",
        "key_concepts": [
          "Core principles and foundations",
          "Implementation strategies",
          "Performance characteristics",
          "Common use cases",
          "Best practices"
        ],
        "overview": "A comprehensive technical concept with wide applications."
      },
      "format": "conceptual_breakdown",
      "metadata": {
        "domain": "Engineering",
        "structure": "hierarchical"
      },
      "topic": "Design Patterns"
    },
    {
      "format": "problem_solving",
      "scenario": {
        "approach": "solve. Any other regularity in the code is a sign, to me at least, that I'm using abstractions that aren't powerful enough-- often that I'm generating by hand the expansions of some macro that I need to write.\ndemonstrates that 16 out of the 23 patterns in\nare simplified or eliminated by language features in\nRelated observations were made by Hannemann and\nwho implemented several of the 23 design patterns using an\naspect-oriented programming\n) and showed that code-level dependencies were removed ",
        "benefits": "advantages of\nover implementation:\nclients remain unaware of the specific types of objects they use, as long as the object adheres to the interface\nclients remain unaware of the classes that implement these objects; clients only know about the abstract class(es) defining the interface\nUse of an inte",
        "problem": "How can Design Patterns solve real-world problems?",
        "tradeoffs": "cost of creating and manipulating a large number of similar objects.\nprovides a placeholder for another object to control access, reduce cost, and reduce complexity.\nbehavioral design patterns\nare specifically concerned with communication between objects.\nChain of responsibility\ndelegates commands t"
      },
      "topic": "Design Patterns"
    },
    {
      "dialogue": [
        {
          "content": "What is SOLID Principles really about at its core?",
          "role": "student"
        },
        {
          "content": "Object-oriented programming design principles\n. For other uses, see\nSolid (disambiguation)\nSingle responsibility Open–closed Liskov substitution Interface segregation Dependency inversion v t e\nsoftware programming\nfor five design principles intended to make\ndesigns more understandable, flexible, and\n. Although the SOLID principles apply to any object-oriented design, they can also form a core philosophy for methodologies such as\nadaptive software development\nSoftware engineer and instructor\nRobert C. Martin [ 2 ] [ 3 ] [ 1 ]\nintroduced the basic principles of SOLID design in his 2000 paper\nDesign Principles and Design Patterns\nacronym was coined around 2004 by Michael Feathers.\nSingle responsibility principle\nsingle-responsibility principle\n) states that \"there should never be more than one reason for a\nIn other words, every class should have only one responsibility.\nMaintainability: When classes have a single, well-defined responsibility, they're easier to understand and modify.\nTestability: It's easier to write unit tests for classes with a single focus.\nFlexibility: Changes to one responsibility don't affect unrelated parts of the system.\nOpen–closed principle\nopen–closed principle\n) states that \"software entities ... should be open for extension, but closed for modification.\"\nExtensibility: New features can be added without modifying existing code.\nStability: Reduces the risk of introducing bugs when making changes.\nFlexibility: Adapts to changing requirements more easily.\nLiskov substitution principle\nLiskov substitution principle\n) states that \"functions that use pointers or references to base classes must be able to use objects of derived classes without knowing it.\"\n[ 9 ] See also design by contract . [ 9 ]\n[ edit ] Polymorphism\n: Enables the use of polymorphic behavior, making code more flexible and reusable.\nReliability: Ensures that subclasses adhere to the contract defined by the superclass.\nPredictability: Guarantees that replacing a superclass object with a subclass object won't break the program.\nInterface segregation principle\ninterface segregation principle\n) states that \"clients should not be forced to depend upon interfaces that they do not use.\"\nDecoupling: Reduces dependencies between classes, making the code more\nFlexibility: Allows for more targeted implementations of interfaces.\nAvoids unnecessary dependencies: Clients don't have to depend on methods they don't use.\nDependency inversion principle\ndependency inversion principle\n) states to depend upon abstractions, [not] concretes.\n[ edit ] Loose coupling\n: Reduces dependencies between modules, making the code more flexible and easier to test.\nFlexibility: Enables changes to implementations without affecting clients.\n: Makes code easier to understand and modify.\n[ edit ] Code reuse GRASP (object-oriented design) Inheritance (object-oriented programming) List of software development philosophies\n[ edit ] ^ a b Metz, Sandi (May 2009). \"SOLID Object-Oriented Design\" . YouTube . Archived from the original on 2021-12-21 . Retrieved 2019-08-13 . Talk given at the 2009 Gotham Ruby Conference. ^ Martin, Robert C. \"Principles Of OOD\" . ButUncleBob.com . Archived from the original on Sep 10, 2014 . Retrieved 2014-07-17 . . (Note the reference to \"the first five principles\", although the acronym is not used in this article.) Dates back to at least 2003. ^ a b Martin, Robert C. (13 Feb 2009). \"Getting a SOLID start\" . Uncle Bob Consulting LLC (Google Sites) . Archived from the original on Sep 17, 2013 . Retrieved 2013-08-19 . ^ a b c d Martin, Robert C. (2000). \"Design Principles and Design Patterns\" (PDF) . objectmentor.com . Archived from the original on 2015-09-06. ^ Martin, Robert (2018). Clean Architecture: A Craftsman's Guide to Software Structure and Design . Pearson. p. 58. ISBN 978-0-13-449416-6 . ^ \"Single Responsibility Principle\" (PDF) . objectmentor.com . Archived from the original on 2 February 2015. ^ a b Martin, Robert C. (2003). Agile Software Development, Principles, Patterns, and Practices . Prentice Hall. p. 95. ISBN 978-0135974445 . ^ \"Open/Closed Principle\" (PDF) . objectmentor.com . Archived from the original on 5 September 2015. ^ a b c \"Liskov Substitution Principle\" (PDF) . objectmentor.com . Archived from the original on 5 September 2015. ^ \"Interface Segregation Principle\" (PDF) . objectmentor.com . 1996. Archived from the original on 5 September 2015. ^ a b \"Dependency Inversion Principle\" (PDF) . objectmentor.com . Archived from the original on 5 September 2015.\nhttps://en.wikipedia.org/w/index.php?title=SOLID\u0026oldid=1303149334\nSoftware design Object-oriented programming Programming principles Computing acronyms Mnemonic acronyms\nCS1: unfit URL Articles with short description Short description is different from Wikidata Search Search SOLID 29 languages Add topic",
          "role": "teacher"
        },
        {
          "content": "Why is this important to understand?",
          "role": "student"
        },
        {
          "content": "Understanding SOLID Principles is crucial for building efficient, scalable, and maintainable software systems. It forms the foundation for solving complex computational problems.",
          "role": "teacher"
        },
        {
          "content": "Can you give me a concrete example?",
          "role": "student"
        },
        {
          "content": "For example, SOLID Principles can be applied in scenarios where you need to optimize performance, manage complexity, or implement scalable solutions.",
          "role": "teacher"
        }
      ],
      "format": "socratic_dialogue",
      "metadata": {
        "domain": "Engineering",
        "source": "https://en.wikipedia.org/wiki/SOLID",
        "words": 872
      },
      "topic": "SOLID Principles"
    },
    {
      "content": {
        "applications": "used in this article.) Dates back to at least 2003. ^ a b Martin, Robert C. (13 Feb 2009). \"Getting a SOLID start\" . Uncle Bob Consulting LLC (Google Sites) . Archived from the original on Sep 17, 2013 . Retrieved 2013-08-19 . ^ a b c d Martin, Robert C. (2000). \"Design Principles and Design Patterns\" (PDF) . objectmentor.com . Archived from the original on 2015-09-06. ^ Martin, Robert (2018). Clean Architecture: A Craftsman's Guide to Software Structure and Design . Pearson. p. 58. ISBN 978-0-1",
        "considerations": "Note the reference to \"the first five principles\", although the acronym is not used in this article.) Dates back to at least 2003. ^ a b Martin, Robert C. (13 Feb 2009). \"Getting a SOLID start\" . Uncle Bob Consulting LLC (Google Sites) . Archived from the original on Sep 17, 2013 . Retrieved 2013-08-19 . ^ a b c d Martin, Robert C. (2000). \"Design Principles and Design Patterns\" (PDF) . objectment",
        "key_concepts": [
          "Core principles and foundations",
          "Implementation strategies",
          "Performance characteristics",
          "Common use cases",
          "Best practices"
        ],
        "overview": "A comprehensive technical concept with wide applications."
      },
      "format": "conceptual_breakdown",
      "metadata": {
        "domain": "Engineering",
        "structure": "hierarchical"
      },
      "topic": "SOLID Principles"
    },
    {
      "format": "problem_solving",
      "scenario": {
        "approach": "SOLID Principles provides systematic approaches to complex problems by breaking them down into manageable components and applying proven techniques.",
        "benefits": "Key benefits include improved performance, better maintainability, and enhanced scalability.",
        "problem": "How can SOLID Principles solve real-world problems?",
        "tradeoffs": "Consider the tradeoffs between complexity and performance, as well as development time versus optimization."
      },
      "topic": "SOLID Principles"
    },
    {
      "dialogue": [
        {
          "content": "What is Merge Sort really about at its core?",
          "role": "student"
        },
        {
          "content": "Divide and conquer sorting algorithm\nAn example of merge sort. First, divide the list into the smallest unit (1 element), then compare each element with the adjacent list to sort and merge the two adjacent lists. Finally, all the elements are sorted and merged.\nArray Worst-case performance O ( n log ⁡ n ) {\\displaystyle O(n\\log n)} Best-case performance Ω ( n log ⁡ n ) {\\displaystyle \\Omega (n\\log n)}\nΩ ( n ) {\\displaystyle \\Omega (n)}\nAverage performance Θ ( n log ⁡ n ) {\\displaystyle \\Theta (n\\log n)} Worst-case space complexity O ( n ) {\\displaystyle O(n)}\nO ( n ) {\\displaystyle O(n)}\nO ( 1 ) {\\displaystyle O(1)}\nauxiliary with linked lists\n(also commonly spelled as\n) is an efficient, general-purpose, and\ncomparison-based sorting algorithm\n. Most implementations of merge sort are\n, which means that the relative order of equal elements is the same between the input and output. Merge sort is a\ndivide-and-conquer algorithm\nthat was invented by\nA detailed description and analysis of bottom-up merge sort appeared in a report by\nand von Neumann as early as 1948.\nConceptually, a merge sort works as follows:\nDivide the unsorted list into\nsub-lists, each containing one element (a list of one element is considered sorted).\nsublists to produce new sorted sublists until there is only one sublist remaining. This will be the sorted list.\nTop-down implementation\ncode using indices for top-down merge sort algorithm that recursively splits the list (called\nin this example) into sublists until sublist size is 1, then merges those sublists to produce a sorted list. The copy back step is avoided with alternating the direction of the merge with each level of recursion (except for an initial one-time copy, that can be avoided too).\nAs a simple example, consider an array with two elements. The elements are copied to\n, then merged back to\n. If there are four elements, when the bottom of the recursion level is reached, single element runs from\n, and then at the next higher level of recursion, those two-element runs are merged to\n. This pattern continues with each level of recursion.\n// Array A[] has the items to sort; array B[] is a work array. void TopDownMergeSort ( A [], B [], n ) { CopyArray ( A , 0 , n , B ); // one time copy of A[] to B[] TopDownSplitMerge ( A , 0 , n , B ); // sort data from B[] into A[] } // Split A[] into 2 runs, sort both runs into B[], merge both runs from B[] to A[] // iBegin is inclusive; iEnd is exclusive (A[iEnd] is not in the set). void TopDownSplitMerge ( B [], iBegin , iEnd , A []) { if ( iEnd - iBegin \u003c= 1 ) // if run size == 1 return ; // consider it sorted // split the run longer than 1 item into halves iMiddle = ( iEnd + iBegin ) / 2 ; // iMiddle = mid point // recursively sort both runs from array A[] into B[] TopDownSplitMerge ( A , iBegin , iMiddle , B ); // sort the left run TopDownSplitMerge ( A , iMiddle , iEnd , B ); // sort the right run // merge the resulting runs from array B[] into A[] TopDownMerge ( B , iBegin , iMiddle , iEnd , A ); } // Left source half is A[ iBegin:iMiddle-1]. // Right source half is A[iMiddle:iEnd-1 ]. // Result is B[ iBegin:iEnd-1 ]. void TopDownMerge ( B [], iBegin , iMiddle , iEnd , A []) { i = iBegin , j = iMiddle ; // While there are elements in the left or right runs... for ( k = iBegin ; k \u003c iEnd ; k ++ ) { // If left run head exists and is \u003c= existing right run head. if ( i \u003c iMiddle \u0026\u0026 ( j \u003e= iEnd || A [ i ] \u003c= A [ j ])) { B [ k ] = A [ i ]; i = i + 1 ; } else { B [ k ] = A [ j ]; j = j + 1 ; } } } void CopyArray ( A [], iBegin , iEnd , B []) { for ( k = iBegin ; k \u003c iEnd ; k ++ ) B [ k ] = A [ k ]; }\nSorting the entire array is accomplished by\nTopDownMergeSort(A, B, length(A))\nBottom-up implementation\nExample C-like code using indices for bottom-up merge sort algorithm which treats the list as an array of\nin this example) of size 1, and iteratively merges sub-lists back and forth between two buffers:\n// array A[] has the items to sort; array B[] is a work array void BottomUpMergeSort ( A [], B [], n ) { // Each 1-element run in A is already \"sorted\". // Make successively longer sorted runs of length 2, 4, 8, 16... until the whole array is sorted. for ( width = 1 ; width \u003c n ; width = 2 * width ) { // Array A is full of runs of length width. for ( i = 0 ; i \u003c n ; i = i + 2 * width ) { // Merge two runs: A[i:i+width-1] and A[i+width:i+2*width-1] to B[] // or copy A[i:n-1] to B[] ( if (i+width \u003e= n) ) BottomUpMerge ( A , i , min ( i + width , n ), min ( i + 2 * width , n ), B ); } // Now work array B is full of runs of length 2*width. // Copy array B to array A for the next iteration. // A more efficient implementation would swap the roles of A and B. CopyArray ( B , A , n ); // Now array A is full of runs of length 2*width. } } // Left run is A[iLeft :iRight-1]. // Right run is A[iRight:iEnd-1 ]. void BottomUpMerge ( A [], iLeft , iRight , iEnd , B []) { i = iLeft , j = iRight ; // While there are elements in the left or right runs... for ( k = iLeft ; k \u003c iEnd ; k ++ ) { // If left run head exists and is \u003c= existing right run head. if ( i \u003c iRight \u0026\u0026 ( j \u003e= iEnd || A [ i ] \u003c= A [ j ])) { B [ k ] = A [ i ]; i = i + 1 ; } else { B [ k ] = A [ j ]; j = j + 1 ; } } } void CopyArray ( B [], A [], n ) { for ( i = 0 ; i \u003c n ; i ++ ) A [ i ] = B [ i ]; }\nTop-down implementation using lists\nfor top-down merge sort algorithm which recursively divides the input list into smaller sublists until the sublists are trivially sorted, and then merges the sublists while returning up the call chain.\nBase case. A list of zero or one elements is sorted, by definition. if\nRecursive case. First, divide the list into equal-sized sublists\nconsisting of the first half and second half of the list.\nThis assumes lists start at index 0. var\nRecursively sort both sublists.\nleft := merge_sort(left)\nright := merge_sort(right)\n// Then merge the now-sorted sublists.\nIn this example, the\nfunction merges the left and right sublists.\nresult := empty list\nfirst(left) ≤ first(right)\nappend first(left) to result\nappend first(right) to result\nright := rest(right)\nEither left or right may have elements left; consume them.\n(Only one of the following loops will actually be entered.) while\nappend first(left) to result\nappend first(right) to result\nright := rest(right)\nBottom-up implementation using lists\nfor bottom-up merge sort algorithm which uses a small fixed size array of references to nodes, where\nis either a reference to a list of size 2\nis a reference or pointer to a node. The\nfunction would be similar to the one shown in the top-down merge lists example, it merges two already sorted lists, and handles empty lists. In this case,\nfor its input parameters and return value.\n// return if empty list\narray[32]; initially all nil\n// merge nodes into array\nnext := result.next;\n(i = 0; (i \u003c 32) \u0026\u0026 (array[i] ≠ nil); i += 1)\nresult := merge(array[i], result)\n// do not go past end of array\n// merge array into single list\n(i = 0; i \u003c 32; i += 1)\nresult := merge(array[i], result)\nTop-down implementation in a declarative style\n-like pseudocode, showing how merge sort can be implemented in such a language using constructs and ideas from\nfunctional programming\nmergeSort :: Ord a =\u003e [ a ] -\u003e [ a ] mergeSort [] = [] mergeSort [ x ] = [ x ] mergeSort xs = merge ( mergeSort l , mergeSort r ) where ( l , r ) = splitAt ( length xs ` div ` 2 ) xs merge :: Ord a =\u003e ([ a ], [ a ]) -\u003e [ a ] merge ( [] , xs ) = xs merge ( xs , [] ) = xs merge ( x : xs , y : ys ) | x \u003c= y = x : merge ( xs , y : ys ) | otherwise = y : merge ( x : xs , ys )\n[ edit ] A recursive merge sort algorithm used to sort an array of 7 integer values. These are the steps a human would take to emulate merge sort (top-down).\nobjects, merge sort has an\nworst-case performance\n) comparisons. If the running time (number of comparisons) of merge sort for a list of length\nrecurrence relation T\nfollows from the definition of the algorithm (apply the algorithm to two lists of half the size of the original list, and add the\nsteps taken to merge the resulting two lists).\nThe closed form follows from the\nmaster theorem for divide-and-conquer recurrences\nThe number of comparisons made by merge sort in the worst case is given by the\n. These numbers are equal to or slightly smaller than (\n+ 1), which is between (\nMerge sort's best case takes about half as many iterations as its worst case.\nand a randomly ordered input list, merge sort's expected (average) number of comparisons approaches\nfewer than the worst case, where\nα = − 1 + ∑ k = 0 ∞ 1 2 k + 1 ≈ 0.2645. {\\displaystyle \\alpha =-1+\\sum _{k=0}^{\\infty }{\\frac {1}{2^{k}+1}}\\approx 0.2645.}\ncase, merge sort uses approximately 39% fewer comparisons than\ncase, and in terms of moves, merge sort's worst case complexity is\n) - the same complexity as quicksort's best case.\nMerge sort is more efficient than quicksort for some types of lists if the data to be sorted can only be efficiently accessed sequentially, and is thus popular in languages such as\n, where sequentially accessed data structures are very common. Unlike some (efficient) implementations of quicksort, merge sort is a stable sort.\nMerge sort's most common implementation does not sort in place;\ntherefore, the memory size of the input must be allocated for the sorted output to be stored in (see below for variations that need only\nA natural merge sort is similar to a bottom-up merge sort except that any naturally occurring\n(sorted sequences) in the input are exploited. Both monotonic and bitonic (alternating up/down) runs may be exploited, with lists (or equivalently tapes or files) being convenient data structures (used as\nIn the bottom-up merge sort, the starting point assumes each run is one item long. In practice, random input data will have many short runs that just happen to be sorted. In the typical case, the natural merge sort may not need as many passes because there are fewer runs to merge. In the best case, the input is already sorted (i.e., is one run), so the natural merge sort need only make one pass through the data. In many practical cases, long natural runs are present, and for that reason natural merge sort is exploited as the key component of\nStart : 3 4 2 1 7 5 8 9 0 6\nSelect runs : (3 4)(2)(1 7)(5 8 9)(0 6)\nMerge : (2 3 4)(1 5 7 8 9)(0 6)\nMerge : (1 2 3 4 5 7 8 9)(0 6)\nMerge : (0 1 2 3 4 5 6 7 8 9)\nFormally, the natural merge sort is said to be\nR u n s ( L ) {\\displaystyle {\\mathtt {Runs}}(L)}\nis the number of runs in\nTournament replacement selection sorts\nare used to gather the initial runs for external sorting algorithms.\nPing-pong merge sort\nInstead of merging two blocks at a time, a ping-pong merge merges four blocks at a time. The four sorted blocks are merged simultaneously to auxiliary space into two sorted blocks, then the two sorted blocks are merged back to main memory. Doing so omits the copy operation and reduces the total number of moves by half. An early public domain implementation of a four-at-once merge was by WikiSort in 2014, the method was later that year described as an optimization for\nand named a ping-pong merge.\nQuadsort implemented the method in 2020 and named it a quad merge.\nOne drawback of merge sort, when implemented on arrays, is its\nworking memory requirement. Several methods to reduce memory or make merge sort fully\nhave been suggested:\nsuggested an alternative version of merge sort that uses constant additional space.\npresent an algorithm that requires a constant amount of working memory: enough storage space to hold one element of the input array, and additional space to hold\npointers into the input array. They achieve an\ntime bound with small constants, but their algorithm is not stable.\nSeveral attempts have been made at producing an\nalgorithm that can be combined with a standard (top-down or bottom-up) merge sort to produce an in-place merge sort. In this case, the notion of \"in-place\" can be relaxed to mean \"taking logarithmic stack space\", because standard merge sort requires that amount of space for its own stack usage. It was shown by Geffert\nthat in-place, stable merging is possible in\ntime using a constant amount of scratch space, but their algorithm is complicated and has high constant factors: merging arrays of length\n5 n + 12 m + o ( m )\nThis high constant factor and complicated in-place algorithm was made simpler and easier to understand. Bing-Chao Huang and Michael A. Langston\npresented a straightforward linear time algorithm\npractical in-place merge\nto merge a sorted list using fixed amount of additional space. They both have used the work of Kronrod and others. It merges in linear time and constant extra space. The algorithm takes little more average time than standard merge sort algorithms, free to exploit\ntemporary extra memory cells, by less than a factor of two. Though the algorithm is much faster in a practical way, it is unstable for some lists. But using similar concepts, they have been able to solve this problem. Other in-place algorithms include SymMerge, which takes\nO (( n + m ) log ( n + m ))\ntime in total and is stable.\nPlugging such an algorithm into merge sort increases its complexity to the non-\nMany applications of\nuse a form of merge sorting where the input gets split up to a higher number of sublists, ideally to a number for which merging them still makes the currently processed set of\nfit into main memory.\nA modern stable, linear, and in-place merge variant is\n, which creates a section of unique values to use as swap space.\nThe space overhead can be reduced to\nby using binary searches and rotations.\nThis method is employed by the C++ STL library and quadsort.\nAn alternative to reduce the copying into multiple lists is to associate a new field of information with each key (the elements in\nare called keys). This field will be used to link the keys and any associated information together in a sorted list (a key and its related information is called a record). Then the merging of the sorted lists proceeds by changing the link values; no records need to be moved at all. A field which contains only a link will generally be smaller than an entire record so less space will also be used. This is a standard sorting technique, not restricted to merge sort.\nA simple way to reduce the space overhead to\nas a combined structure, copy only the\ninto temporary space, and to direct the\nroutine to place the merged output into\n. With this version it is better to allocate the temporary space outside the\nroutine, so that only one allocation is needed. The excessive copying mentioned previously is also mitigated, since the last pair of lines before the\nin the pseudo code above) become superfluous.\nUse with tape drives\n[ edit ] Merge sort type algorithms allowed large data sets to be sorted on early computers that had small random access memories by modern standards. Records were stored on magnetic tape and processed on banks of magnetic tape drives, such as these IBM 729s .\nmerge sort is practical to run using\ndrives when the data to be sorted is too large to fit into\nexplains how merge sort is implemented with disk drives. A typical tape drive sort uses four tape drives. All I/O is sequential (except for rewinds at the end of each pass). A minimal implementation can get by with just two record buffers and a few program variables.\nNaming the four tape drives as A, B, C, D, with the original data on A, and using only two record buffers, the algorithm is similar to\nthe bottom-up implementation\n, using pairs of tape drives instead of arrays in memory. The basic algorithm can be described as follows:\nMerge pairs of records from A; writing two-record sublists alternately to C and D.\nMerge two-record sublists from C and D into four-record sublists; writing these alternately to A and B.\nMerge four-record sublists from A and B into eight-record sublists; writing these alternately to C and D\nRepeat until you have one list containing all the data, sorted—in log\nInstead of starting with very short runs, usually a\nis used, where the initial pass will read many records into memory, do an internal sort to create a long run, and then distribute those long runs onto the output set. The step avoids many early passes. For example, an internal sort of 1024 records will save nine passes. The internal sort is often large because it has such a benefit. In fact, there are techniques that can make the initial runs longer than the available internal memory. One of them, the Knuth's 'snowplow' (based on a\n), generates runs twice as long (on average) as a size of memory used.\nWith some overhead, the above algorithm can be modified to use three tapes.\n) running time can also be achieved using two\nand a queue, or three stacks. In the other direction, using\n) items in memory), we can reduce the number of tape operations in\nA more sophisticated merge sort that optimizes tape (and disk) drive usage is the\npolyphase merge sort\nOptimizing merge sort\n[ edit ] Tiled merge sort applied to an array of random integers. The horizontal axis is the array index and the vertical axis is the integer.\nOn modern computers,\nlocality of reference\ncan be of paramount importance in\nsoftware optimization\n, because multilevel\n-aware versions of the merge sort algorithm, whose operations have been specifically chosen to minimize the movement of pages in and out of a machine's memory cache, have been proposed. For example, the\nalgorithm stops partitioning subarrays when subarrays of size S are reached, where S is the number of data items fitting into a CPU's cache. Each of these subarrays is sorted with an in-place sorting algorithm such as\n, to discourage memory swaps, and normal merge sort is then completed in the standard recursive fashion. This algorithm has demonstrated better performance\non machines that benefit from cache optimization. (\nLaMarca \u0026 Ladner 1997\nMerge sort parallelizes well due to the use of the\nmethod. Several different parallel variants of the algorithm have been developed over the years. Some parallel merge sort algorithms are strongly related to the sequential top-down merge algorithm while others have a different general structure and use the\nMerge sort with parallel recursion\nThe sequential merge sort procedure can be described in two phases, the divide phase and the merge phase. The first consists of many recursive calls that repeatedly perform the same division process until the subsequences are trivially sorted (containing one or no element). An intuitive approach is the parallelization of those recursive calls.\nFollowing pseudocode describes the merge sort with parallel recursion using the\nSort elements lo through hi (exclusive) of array A. algorithm\nmergesort(A, lo, hi)\nTwo or more elements.\nmid := ⌊(lo + hi) / 2⌋\nmergesort(A, lo, mid)\nmergesort(A, mid, hi)\nmerge(A, lo, mid, hi)\nThis algorithm is the trivial modification of the sequential version and does not parallelize well. Therefore, its speedup is not very impressive. It has a\nΘ ( n ) {\\displaystyle \\Theta (n)}\n, which is only an improvement of\nΘ ( log ⁡ n ) {\\displaystyle \\Theta (\\log n)}\ncompared to the sequential version (see\nIntroduction to Algorithms\n). This is mainly due to the sequential merge method, as it is the bottleneck of the parallel executions.\nMerge sort with parallel merging\nMerge algorithm § Parallel merge\nBetter parallelism can be achieved by using a parallel\npresent a binary variant that merges two sorted sub-sequences into one sorted output sequence.\nIn one of the sequences (the longer one if unequal length), the element of the middle index is selected. Its position in the other sequence is determined in such a way that this sequence would remain sorted if this element were inserted at this position. Thus, one knows how many other elements from both sequences are smaller and the position of the selected element in the output sequence can be calculated. For the partial sequences of the smaller and larger elements created in this way, the merge algorithm is again executed in parallel until the base case of the recursion is reached.\nThe following pseudocode shows the modified parallel merge sort method using the parallel merge algorithm (adopted from Cormen et al.).\nparallelMergesort(A, lo, hi, B, off)\nlet T[1..len] be a new array\nmid := ⌊(lo + hi) / 2⌋\nmid' := mid - lo + 1\nparallelMergesort(A, lo, mid, T, 1)\nparallelMergesort(A, mid + 1, hi, T, mid' + 1)\nparallelMerge(T, 1, mid', mid' + 1, len, B, off)\nIn order to analyze a\nfor the worst case span, the recursive calls of parallelMergesort have to be incorporated only once due to their parallel execution, obtaining\nT ∞ sort ( n ) = T ∞ sort ( n 2 ) + T ∞ merge ( n ) = T ∞ sort ( n 2 ) + Θ ( log ⁡ ( n ) 2 ) . {\\displaystyle T_{\\infty }^{\\text{sort}}(n)=T_{\\infty }^{\\text{sort}}\\left({\\frac {n}{2}}\\right)+T_{\\infty }^{\\text{merge}}(n)=T_{\\infty }^{\\text{sort}}\\left({\\frac {n}{2}}\\right)+\\Theta \\left(\\log(n)^{2}\\right).}\nFor detailed information about the complexity of the parallel merge procedure, see\nThe solution of this recurrence is given by\nT ∞ sort = Θ ( log ⁡ ( n ) 3 ) . {\\displaystyle T_{\\infty }^{\\text{sort}}=\\Theta \\left(\\log(n)^{3}\\right).}\nThis parallel merge algorithm reaches a parallelism of\nΘ ( n ( log ⁡ n ) 2 ) {\\textstyle \\Theta \\left({\\frac {n}{(\\log n)^{2}}}\\right)}\n, which is much higher than the parallelism of the previous algorithm. Such a sort can perform well in practice when combined with a fast stable sequential sort, such as\n, and a fast sequential merge as a base case for merging small arrays.\nParallel multiway merge sort\nIt seems arbitrary to restrict the merge sort algorithms to a binary merge method, since there are usually p \u003e 2 processors available. A better approach may be to use a\nmethod, a generalization of binary merge, in which\nsorted sequences are merged. This merge variant is well suited to describe a sorting algorithm on a\n[ edit ] The parallel multiway mergesort process on four processors t 0 {\\displaystyle t_{0}} to t 3 {\\displaystyle t_{3}} .\nGiven an unsorted sequence of\nelements, the goal is to sort the sequence with\n. These elements are distributed equally among all processors and sorted locally using a sequential\n. Hence, the sequence consists of sorted sequences\nS 1 , . . . , S p {\\displaystyle S_{1},...,S_{p}}\n⌈ n p ⌉ {\\textstyle \\lceil {\\frac {n}{p}}\\rceil }\n. For simplification let\n| S i | = n p {\\textstyle \\left\\vert S_{i}\\right\\vert ={\\frac {n}{p}}}\ni = 1 , . . . , p {\\displaystyle i=1,...,p}\nThese sequences will be used to perform a multisequence selection/splitter selection. For\nj = 1 , . . . , p {\\displaystyle j=1,...,p}\n, the algorithm determines splitter elements\nv j {\\displaystyle v_{j}}\nk = j n p {\\textstyle k=j{\\frac {n}{p}}}\n. Then the corresponding positions of\nv 1 , . . . , v p {\\displaystyle v_{1},...,v_{p}}\nS i {\\displaystyle S_{i}}\nS i {\\displaystyle S_{i}}\nare further partitioned into\nS i , 1 , . . . , S i , p {\\displaystyle S_{i,1},...,S_{i,p}}\nS i , j := { x ∈ S i | r a n k ( v j − 1 ) \u003c r a n k ( x ) ≤ r a n k ( v j ) } {\\textstyle S_{i,j}:=\\{x\\in S_{i}|rank(v_{j-1})\u003crank(x)\\leq rank(v_{j})\\}}\nFurthermore, the elements of\nS 1 , i , . . . , S p , i {\\displaystyle S_{1,i},...,S_{p,i}}\nare assigned to processor\n, means all elements between rank\n( i − 1 ) n p {\\textstyle (i-1){\\frac {n}{p}}}\ni n p {\\textstyle i{\\frac {n}{p}}}\n, which are distributed over all\nS i {\\displaystyle S_{i}}\n. Thus, each processor receives a sequence of sorted sequences. The fact that the rank\nof the splitter elements\nv i {\\displaystyle v_{i}}\nwas chosen globally, provides two important properties: On the one hand,\nwas chosen so that each processor can still operate on\nn / p {\\textstyle n/p}\nelements after assignment. The algorithm is perfectly\n. On the other hand, all elements on processor\nare less than or equal to all elements on processor\ni + 1 {\\displaystyle i+1}\n. Hence, each processor performs the\nlocally and thus obtains a sorted sequence from its sub-sequences. Because of the second property, no further\n-way-merge has to be performed, the results only have to be put together in the order of the processor number.\nMulti-sequence selection\nIn its simplest form, given\nS 1 , . . . , S p {\\displaystyle S_{1},...,S_{p}}\ndistributed evenly on\nprocessors and a rank\n, the task is to find an element\nin the union of the sequences. Hence, this can be used to divide each\nS i {\\displaystyle S_{i}}\nin two parts at a splitter index\nl i {\\displaystyle l_{i}}\n, where the lower part contains only elements which are smaller than\n, while the elements bigger than\nare located in the upper part.\nsequential algorithm\nreturns the indices of the splits in each sequence, e.g. the indices\nl i {\\displaystyle l_{i}}\nS i {\\displaystyle S_{i}}\nS i [ l i ] {\\displaystyle S_{i}[l_{i}]}\nhas a global rank less than\nr a n k ( S i [ l i + 1 ] ) ≥ k {\\displaystyle \\mathrm {rank} \\left(S_{i}[l_{i}+1]\\right)\\geq k}\nmsSelect(S : Array of sorted Sequences [S_1,..,S_p], k : int)\n(l_i, r_i) = (0, |S_i|-1)\nthere exists i: l_i \u003c r_i\nin S_j[l_j], .., S_j[r_j], chose random j uniformly\nv := pickPivot(S, l, r)\nm_i = binarySearch(v, S_i[l_i, r_i]) // sequentially\nm_1 + ... + m_p \u003e= k\n// m_1+ ... + m_p is the global rank of v\nr := m // vector assignment\nFor the complexity analysis the\nmodel is chosen. If the data is evenly distributed over all\n, the p-fold execution of the\nmethod has a running time of\nO ( p log ⁡ ( n / p ) ) {\\displaystyle {\\mathcal {O}}\\left(p\\log \\left(n/p\\right)\\right)}\n. The expected recursion depth is\nO ( log ⁡ ( ∑ i | S i | ) ) = O ( log ⁡ ( n ) ) {\\displaystyle {\\mathcal {O}}\\left(\\log \\left(\\textstyle \\sum _{i}|S_{i}|\\right)\\right)={\\mathcal {O}}(\\log(n))}\n. Thus the overall expected running time is\nO ( p log ⁡ ( n / p ) log ⁡ ( n ) ) {\\displaystyle {\\mathcal {O}}\\left(p\\log(n/p)\\log(n)\\right)}\nApplied on the parallel multiway merge sort, this algorithm has to be invoked in parallel such that all splitter elements of rank\ni n p {\\textstyle i{\\frac {n}{p}}}\ni = 1 , . . , p {\\displaystyle i=1,..,p}\nare found simultaneously. These splitter elements can then be used to partition each sequence in\nparts, with the same total running time of\nO ( p log ⁡ ( n / p ) log ⁡ ( n ) ) {\\displaystyle {\\mathcal {O}}\\left(p\\,\\log(n/p)\\log(n)\\right)}\nBelow, the complete pseudocode of the parallel multiway merge sort algorithm is given. We assume that there is a barrier synchronization before and after the multisequence selection such that every processor can determine the splitting elements and the sequence partition properly.\n* d: Unsorted Array of Elements\n* n: Number of Elements\n* p: Number of Processors\n* return Sorted Array\nparallelMultiwayMergesort(d : Array, n : int, p : int)\nArray[0, n] // the output array\n// each processor in parallel\nS_i := d[(i-1) * n/p, i * n/p] // Sequence of length n/p\nsort(S_i) // sort locally\nv_i := msSelect([S_1,...,S_p], i * n/p) // element with global rank i * n/p\n(S_i,1, ..., S_i,p) := sequence_partitioning(si, v_1, ..., v_p) // split s_i into subsequences\no[(i-1) * n/p, i * n/p] := kWayMerge(s_1,i, ..., s_p,i) // merge and assign to output array\nFirstly, each processor sorts the assigned\nn / p {\\displaystyle n/p}\nelements locally using a sorting algorithm with complexity\nO ( n / p log ⁡ ( n / p ) ) {\\displaystyle {\\mathcal {O}}\\left(n/p\\;\\log(n/p)\\right)}\n. After that, the splitter elements have to be calculated in time\nO ( p log ⁡ ( n / p ) log ⁡ ( n ) ) {\\displaystyle {\\mathcal {O}}\\left(p\\,\\log(n/p)\\log(n)\\right)}\n. Finally, each group of\nsplits have to be merged in parallel by each processor with a running time of\nO ( log ⁡ ( p ) n / p ) {\\displaystyle {\\mathcal {O}}(\\log(p)\\;n/p)}\np-way merge algorithm\n. Thus, the overall running time is given by\nO ( n p log ⁡ ( n p ) + p log ⁡ ( n p ) log ⁡ ( n ) + n p log ⁡ ( p ) ) {\\displaystyle {\\mathcal {O}}\\left({\\frac {n}{p}}\\log \\left({\\frac {n}{p}}\\right)+p\\log \\left({\\frac {n}{p}}\\right)\\log(n)+{\\frac {n}{p}}\\log(p)\\right)}\nPractical adaption and application\nThe multiway merge sort algorithm is very scalable through its high parallelization capability, which allows the use of many processors. This makes the algorithm a viable candidate for sorting large amounts of data, such as those processed in\n. Also, since in such systems memory is usually not a limiting resource, the disadvantage of\nof merge sort is negligible. However, other factors become important in such systems, which are not taken into account when modelling on a\n. Here, the following aspects need to be considered:\n, when the data does not fit into the processors cache, or the communication overhead of exchanging data between processors, which could become a bottleneck when the data can no longer be accessed via the shared memory.\net al. have presented in their paper a\nbulk synchronous parallel\nalgorithm for multilevel multiway mergesort, which divides\np ′ {\\displaystyle p'}\n. All processors sort locally first. Unlike single level multiway mergesort, these sequences are then partitioned into\nparts and assigned to the appropriate processor groups. These steps are repeated recursively in those groups. This reduces communication and especially avoids problems with many small messages. The hierarchical structure of the underlying real network can be used to define the processor groups (e.g.\nMerge sort was one of the first sorting algorithms where optimal speed up was achieved, with Richard Cole using a clever subsampling algorithm to ensure\nOther sophisticated parallel sorting algorithms can achieve the same or better time bounds with a lower constant. For example, in 1991 David Powers described a parallelized\n) that can operate in\nCRCW parallel random-access machine\nprocessors by performing partitioning implicitly.\nPowers further shows that a pipelined version of Batcher's\n) time on a butterfly\nis in practice actually faster than his\n) sorts on a PRAM, and he provides detailed discussion of the hidden overheads in comparison, radix and parallel sorting.\nComparison with other sort algorithms\nhas the same time bounds as merge sort, it requires only Θ(1) auxiliary space instead of merge sort's Θ(\n). On typical modern architectures, efficient\nimplementations generally outperform merge sort for sorting RAM-based arrays.\nQuicksorts are preferred when the data size to be sorted is lesser, since the space complexity for quicksort is O(log\n), it helps in utilizing cache locality better than merge sort (with space complexity O(n)).\nOn the other hand, merge sort is a stable sort and is more efficient at handling slow-to-access sequential media. Merge sort is often the best choice for sorting a\n: in this situation it is relatively easy to implement a merge sort in such a way that it requires only Θ(1) extra space, and the slow random-access performance of a linked list makes some other algorithms (such as quicksort) perform poorly, and others (such as heapsort) completely impossible.\n5.8, merge sort is its default sorting algorithm (it was quicksort in previous versions of Perl).\nmethods use merge sort or a tuned quicksort depending on the datatypes and for implementation efficiency switch to\nwhen fewer than seven array elements are being sorted.\nkernel uses merge sort for its linked lists.\n, a tuned hybrid of merge sort and insertion sort is used in variety of software platforms and languages including the Java and Android platforms\nsince version 2.3; since version 3.11, Timsort's merge policy was updated to\n[ edit ] ^ Skiena (2008 , p. 122) ^ Goodrich, Michael T.; Tamassia, Roberto; Goldwasser, Michael H. (2013). \"Chapter 12 - Sorting and Selection\". Data structures and algorithms in Python (1st ed.). Hoboken [NJ]: Wiley. pp. 538– 549. ISBN 978-1-118-29027-9 . ^ Knuth (1998 , p. 158) ^ Katajainen, Jyrki; Träff, Jesper Larsson (March 1997). \"Algorithms and Complexity\". Proceedings of the 3rd Italian Conference on Algorithms and Complexity . Italian Conference on Algorithms and Complexity. Lecture Notes in Computer Science. Vol. 1203. Rome. pp. 217– 228. CiteSeerX 10.1.1.86.3154 . doi : 10.1007/3-540-62592-5_74 . ISBN 978-3-540-62592-6 . ^ Cormen et al. (2009 , p. 36) ^ The worst case number given here does not agree with that given in Knuth 's Art of Computer Programming , Vol 3 . The discrepancy is due to Knuth analyzing a variant implementation of merge sort that is slightly sub-optimal ^ a b Jayalakshmi, N. (2007). Data structure using C++ . Firewall Media. ISBN 978-81-318-0020-1 . OCLC 849900742 . ^ Cormen et al. (2009 , p. 151) ^ Powers, David M. W.; McMahon, Graham B. (1983). \"A compendium of interesting prolog programs\". DCS Technical Report 8313 (Report). Department of Computer Science, University of New South Wales. ^ \"WikiSort. Fast and stable sort algorithm that uses O(1) memory. Public domain\" . GitHub . 14 Apr 2014. ^ Chandramouli, Badrish; Goldstein, Jonathan (2014). Patience is a Virtue: Revisiting Merge and Sort on Modern Processors (PDF) . SIGMOD/PODS. ^ a b \"Quadsort is a branchless stable adaptive merge sort\" . GitHub . 8 Jun 2022. ^ Katajainen, Pasanen \u0026 Teuhola (1996) ^ Geffert, Viliam; Katajainen, Jyrki; Pasanen, Tomi (2000). \"Asymptotically efficient in-place merging\" . Theoretical Computer Science . 237 ( 1– 2): 159– 181. doi : 10.1016/S0304-3975(98)00162-5 . ^ Huang, Bing-Chao; Langston, Michael A. (March 1988). \"Practical In-Place Merging\" . Communications of the ACM . 31 (3): 348– 352. doi : 10.1145/42392.42403 . S2CID 4841909 . ^ Kim, Pok-Son; Kutzner, Arne (2004). \"Stable Minimum Storage Merging by Symmetric Comparisons\". Algorithms – ESA 2004 . European Symp. Algorithms. Lecture Notes in Computer Science. Vol. 3221. pp. 714– 723. CiteSeerX 10.1.1.102.4612 . doi : 10.1007/978-3-540-30140-0_63 . ISBN 978-3-540-23025-0 . ^ Kim, Pok-Son; Kutzner, Arne (1 Sep 2003). \"A New Method for Efficient in-Place Merging\" . Proceedings of the Korean Institute of Intelligent Systems Conference : 392– 394. ^ Ferragina, Paolo (2009–2019), \"5. Sorting Atomic Items\" (PDF) , The magic of Algorithms! , p. 5-4, archived (PDF) from the original on 2021-05-12 ^ a b Cormen et al. (2009 , pp. 797–805) ^ Victor J. Duvanenko \"Parallel Merge Sort\" Dr. Dobb's Journal \u0026 blog [1] and GitHub repo C++ implementation [2] ^ Peter Sanders; Johannes Singler (2008). \"Lecture Parallel algorithms \" (PDF) . Retrieved 2020-05-02 . ^ a b Axtmann, Michael; Bingmann, Timo; Sanders, Peter; Schulz, Christian (2015). \"Practical Massively Parallel Sorting\" . Proceedings of the 27th ACM symposium on Parallelism in Algorithms and Architectures . pp. 13– 23. doi : 10.1145/2755573.2755595 . ISBN 9781450335881 . S2CID 18249978 . ^ Peter Sanders (2019). \"Lecture Parallel algorithms \" (PDF) . Retrieved 2020-05-02 . ^ Cole, Richard (August 1988). \"Parallel merge sort\". SIAM J. Comput . 17 (4): 770– 785. CiteSeerX 10.1.1.464.7118 . doi : 10.1137/0217049 . S2CID 2416667 . ^ Powers, David M. W. (1991). \"Parallelized Quicksort and Radixsort with Optimal Speedup\" . Proceedings of International Conference on Parallel Computing Technologies, Novosibirsk . Archived from the original on 2007-05-25. ^ Powers, David M. W. (January 1995). Parallel Unification: Practical Complexity (PDF) . Australasian Computer Architecture Workshop Flinders University. ^ a b Oladipupo, Esau Taiwo; Abikoye, Oluwakemi Christianah (2020). \"Comparison of quicksort and mergesort\" . Third International Conference on Computing and Network Communications (CoCoNet 2019) . 2020 (2020): 9 . Retrieved 2024-01-20 – via Elsevier Science Direct. ^ \"Sort – Perl 5 version 8.8 documentation\" . Retrieved 2020-08-23 . ^ coleenp (22 Feb 2019). \"src/java.base/share/classes/java/util/Arrays.java @ 53904:9c3fe09f69bc\" . OpenJDK . ^ linux kernel /lib/list_sort.c ^ University of Liverpool (2022-12-12). \"Computer scientists improve Python sorting function\" . Tech Xplore . Retrieved 2024-05-08 . ^ James, Mike (2022-12-21). \"Python Now Uses Powersort\" . i-programmer.info . Retrieved 2024-05-08 .\n[ edit ] Cormen, Thomas H. ; Leiserson, Charles E. ; Rivest, Ronald L. ; Stein, Clifford (2009) [1990]. Introduction to Algorithms (3rd ed.). MIT Press and McGraw-Hill. ISBN 0-262-03384-4 . Katajainen, Jyrki; Pasanen, Tomi; Teuhola, Jukka (1996). \"Practical in-place mergesort\" . Nordic Journal of Computing . 3 (1): 27– 40. CiteSeerX 10.1.1.22.8523 . ISSN 1236-6064 . Archived from the original on 2011-08-07 . Retrieved 2009-04-04 .\nPractical In-Place Mergesort\n[3] Knuth, Donald (1998). \"Section 5.2.4: Sorting by Merging\". Sorting and Searching . The Art of Computer Programming . Vol. 3 (2nd ed.). Addison-Wesley. pp. 158– 168. ISBN 0-201-89685-0 . Kronrod, M. A. (1969). \"Optimal ordering algorithm without operational field\". Soviet Mathematics - Doklady . 10 : 744. LaMarca, A.; Ladner, R. E. (1997). \"The influence of caches on the performance of sorting\". Proc. 8th Ann. ACM-SIAM Symp. On Discrete Algorithms (SODA97) : 370– 379. CiteSeerX 10.1.1.31.1153 . Skiena, Steven S. (2008). \"4.5: Mergesort: Sorting by Divide-and-Conquer\". The Algorithm Design Manual (2nd ed.). Springer. pp. 120– 125. ISBN 978-1-84800-069-8 . Sun Microsystems. \"Arrays API (Java SE 6)\" . Retrieved 2007-11-19 . Oracle Corp. \"Arrays (Java SE 10 \u0026 JDK 10)\" . Retrieved 2018-07-23 .\nAlgorithm implementation\nhas a page on the topic of:\nMerge sort Animated Sorting Algorithms: Merge Sort\n(archived 6 March 2015) – graphical demonstration\nOpen Data Structures - Section 11.1.1 - Merge Sort\nPat Morin v t e Sorting algorithms\nComputational complexity theory Big O notation Total order Lists Inplacement Stability Comparison sort Adaptive sort Sorting network Integer sorting X + Y sorting Transdichotomous model Quantum sort\nBubble sort Cocktail shaker sort Odd–even sort Comb sort Gnome sort Proportion extend sort Quicksort Selection sorts Selection sort Heapsort Smoothsort Cartesian tree sort Tournament sort Cycle sort Weak-heap sort\nInsertion sort Shellsort Splaysort Tree sort Library sort Patience sorting\nMerge sort Cascade merge sort Oscillating merge sort Polyphase merge sort\nAmerican flag sort Bead sort Bucket sort Burstsort Counting sort Interpolation sort Pigeonhole sort Proxmap sort Radix sort Flashsort Concurrent\nBitonic sorter Batcher odd–even mergesort Pairwise sorting network Samplesort Hybrid sorts Block merge sort Introsort Kirkpatrick–Reisch sort Merge-insertion sort Powersort Timsort Spreadsort\nTopological sorting Pre-topological order Pancake sorting Spaghetti sort\nStooge sort Slowsort Bogosort Stalin sort\nhttps://en.wikipedia.org/w/index.php?title=Merge_sort\u0026oldid=1303350280\nComparison sorts Stable sorts Divide-and-conquer algorithms\nArticles with short description Short description is different from Wikidata All articles needing examples Articles needing examples from August 2016 Webarchive template wayback links Articles with example pseudocode Search Search Merge sort 47 languages Add topic",
          "role": "teacher"
        },
        {
          "content": "Why is this important to understand?",
          "role": "student"
        },
        {
          "content": "Understanding Merge Sort is crucial for building efficient, scalable, and maintainable software systems. It forms the foundation for solving complex computational problems.",
          "role": "teacher"
        },
        {
          "content": "Can you give me a concrete example?",
          "role": "student"
        },
        {
          "content": "For example, Merge Sort can be applied in scenarios where you need to optimize performance, manage complexity, or implement scalable solutions.",
          "role": "teacher"
        }
      ],
      "format": "socratic_dialogue",
      "metadata": {
        "domain": "Algorithms",
        "source": "https://en.wikipedia.org/wiki/Merge_sort",
        "words": 7867
      },
      "topic": "Merge Sort"
    },
    {
      "conversation": [
        {
          "content": "How do I implement Merge Sort?",
          "role": "user"
        },
        {
          "content": "Here's how to implement Merge Sort:\n\n1. Understand the core algorithm:\n• , then merged back to\n• , and then at the next higher level of recursion, those two-element runs are merged to\n• consisting of the first half and second half of the list.\n• first(left) ≤ first(right)\n• append first(left) to result\n\n2. Choose appropriate data structures\n3. Handle edge cases and validation\n4. Optimize for your use case\n5. Test thoroughly with various inputs\n\nBasic structure:\n```\nfunction implementMergeSort(input) {\n    // Initialize\n    // Process\n    // Return result\n}\n```",
          "role": "assistant"
        }
      ],
      "format": "implementation_guide",
      "metadata": {
        "domain": "Algorithms",
        "type": "code_focused"
      },
      "topic": "Merge Sort"
    },
    {
      "content": {
        "applications": "applications of\nuse a form of merge sorting where the input gets split up to a higher number of sublists, ideally to a number for which merging them still makes the currently processed set of\nfit into main memory.\nA modern stable, linear, and in-place merge variant is\n, which creates a section of unique values to use as swap space.\nThe space overhead can be reduced to\nby using binary searches and rotations.\nThis method is employed by the C++ STL library and quadsort.\nAn alternative to reduce the",
        "considerations": "considered sorted).\nsublists to produce new sorted sublists until there is only one sublist remaining. This will be the sorted list.\nTop-down implementation\ncode using indices for top-down merge sort algorithm that recursively splits the list (called\nin this example) into sublists until sublist size is 1, then merges those sublists to produce a sorted list. The copy back step is avoided with alter",
        "key_concepts": [
          "like pseudocode, showing how merge sort can be implemented in such a language using constructs and ideas from",
          "aware versions of the merge sort algorithm, whose operations have been specifically chosen to minimize the movement of pages in and out of a machine's memory cache, have been proposed. For example, the",
          "way-merge has to be performed, the results only have to be put together in the order of the processor number.",
          "d: Unsorted Array of Elements",
          "n: Number of Elements"
        ],
        "overview": "A comprehensive technical concept with wide applications."
      },
      "format": "conceptual_breakdown",
      "metadata": {
        "domain": "Algorithms",
        "structure": "hierarchical"
      },
      "topic": "Merge Sort"
    },
    {
      "format": "problem_solving",
      "scenario": {
        "approach": "solve this problem. Other in-place algorithms include SymMerge, which takes\nO (( n + m ) log ( n + m ))\ntime in total and is stable.\nPlugging such an algorithm into merge sort increases its complexity to the non-\nMany applications of\nuse a form of merge sorting where the input gets split up to a higher number of sublists, ideally to a number for which merging them still makes the currently processed set of\nfit into main memory.\nA modern stable, linear, and in-place merge variant is\n, which creat",
        "benefits": "benefit. In fact, there are techniques that can make the initial runs longer than the available internal memory. One of them, the Knuth's 'snowplow' (based on a\n), generates runs twice as long (on average) as a size of memory used.\nWith some overhead, the above algorithm can be modified to use three",
        "problem": "How can Merge Sort solve real-world problems?",
        "tradeoffs": "disadvantage of\nof merge sort is negligible. However, other factors become important in such systems, which are not taken into account when modelling on a\n. Here, the following aspects need to be considered:\n, when the data does not fit into the processors cache, or the communication overhead of exc"
      },
      "topic": "Merge Sort"
    },
    {
      "comparison": "compare each element with the adjacent list to sort and merge the two adjacent lists. Finally, all the elements are sorted and merged.\nArray Worst-case performance O ( n log ⁡ n ) {\\displaystyle O(n\\log n)} Best-case performance Ω ( n log ⁡ n ) {\\displaystyle \\Omega (n\\log n)}\nΩ ( n ) {\\displaystyle \\Omega (n)}\nAverage performance Θ ( n log ⁡ n ) {\\displaystyle \\Theta (n\\log n)} Worst-case space complexity O ( n ) {\\displaystyle O(n)}\nO ( n ) {\\displaystyle O(n)}\nO ( 1 ) {\\displaystyle ",
      "format": "comparative_analysis",
      "topic": "Merge Sort"
    },
    {
      "dialogue": [
        {
          "content": "What is Dijkstra's Algorithm really about at its core?",
          "role": "student"
        },
        {
          "content": "Algorithm for finding shortest paths\nNot to be confused with\nDykstra's projection algorithm\nDijkstra's algorithm\nDijkstra's algorithm to find the shortest path between\n. It picks the unvisited vertex with the lowest distance, calculates the distance through it to each unvisited neighbor, and updates the neighbor's distance if smaller. Mark visited (set to red) when done with neighbors.\nSearch algorithm Greedy algorithm Dynamic programming [ 1 ]\n[ 2 ] [ 3 ] Worst-case performance Θ ( | E | + | V | log ⁡ | V | ) {\\displaystyle \\Theta (|E|+|V|\\log |V|)} [ 3 ] Dijkstra's algorithm\n/ ˈ d aɪ k s t r ə z / DYKE -strəz\n, which may represent, for example, a\n. It was conceived by computer scientist\nin 1956 and published three years later.\nDijkstra's algorithm finds the shortest path from a given source node to every other node.\nIt can be used to find the shortest path to a specific destination node, by terminating the algorithm after determining the shortest path to the destination node. For example, if the nodes of the graph represent cities, and the costs of edges represent the distances between pairs of cities connected by a direct road, then Dijkstra's algorithm can be used to find the shortest route between one city and all other cities. A common application of shortest path algorithms is network\n(Intermediate System to Intermediate System) and\n(Open Shortest Path First). It is also employed as a\nin algorithms such as\nThe algorithm uses a\nmin-priority queue data structure\nfor selecting the shortest paths known so far. Before more advanced priority queue structures were discovered, Dijkstra's original algorithm ran in\nΘ ( | V | 2 ) {\\displaystyle \\Theta (|V|^{2})} time\n| V | {\\displaystyle |V|}\nis the number of nodes.\n[ 8 ] [ 9 ] Fredman \u0026 Tarjan 1984\npriority queue to optimize the running time complexity to\nΘ ( | E | + | V | log ⁡ | V | ) {\\displaystyle \\Theta (|E|+|V|\\log |V|)}\nthe fastest known single-source\nshortest-path algorithm\nwith unbounded non-negative weights. However, specialized cases (such as bounded/integer weights, directed acyclic graphs etc.) can be\n. If preprocessing is allowed, algorithms such as\ncontraction hierarchies\nDijkstra's algorithm is commonly used on graphs where the edge weights are positive integers or real numbers. It can be generalized to any graph where the edge weights are\n, provided the subsequent labels (a subsequent label is produced when traversing an edge) are\nIn many fields, particularly\nartificial intelligence\n, Dijkstra's algorithm or a variant offers a\nand is formulated as an instance of the more general idea of\nWhat is the shortest way to travel from\n, in general: from given city to given city.\nIt is the algorithm for the shortest path\n, which I designed in about twenty minutes. One morning I was shopping in\nwith my young fiancée, and tired, we sat down on the café terrace to drink a cup of coffee and I was just thinking about whether I could do this, and I then designed the algorithm for the shortest path. As I said, it was a twenty-minute invention. In fact, it was published in '59, three years later. The publication is still readable, it is, in fact, quite nice. One of the reasons that it is so nice was that I designed it without pencil and paper. I learned later that one of the advantages of designing without pencil and paper is that you are almost forced to avoid all avoidable complexities. Eventually, that algorithm became to my great amazement, one of the cornerstones of my fame.\n— Edsger Dijkstra, in an interview with Philip L. Frana, Communications of the ACM, 2001\nDijkstra thought about the shortest path problem while working as a programmer at the\nMathematical Center in Amsterdam\nin 1956. He wanted to demonstrate the capabilities of the new ARMAC computer.\nHis objective was to choose a problem and a computer solution that non-computing people could understand. He designed the shortest path algorithm and later implemented it for ARMAC for a slightly simplified transportation map of 64 cities in the Netherlands (he limited it to 64, so that 6 bits would be sufficient to encode the city number).\nA year later, he came across another problem advanced by hardware engineers working on the institute's next computer: minimize the amount of wire needed to connect the pins on the machine's back panel. As a solution, he re-discovered\nPrim's minimal spanning tree algorithm\n, and also rediscovered by\nDijkstra published the algorithm in 1959, two years after Prim and 29 years after Jarník.\n[ edit ] Illustration of Dijkstra's algorithm finding a path from a start node (lower left, red) to a target node (upper right, green) in a robot motion planning problem. Open nodes represent the \"tentative\" set (aka set of \"unvisited\" nodes). Filled nodes are the visited ones, with color representing the distance: the redder, the closer (to the start node). Nodes in all the different directions are explored uniformly, appearing more-or-less as a circular wavefront as Dijkstra's algorithm uses a heuristic of picking the shortest known path so far.\nThe algorithm requires a starting node, and computes the shortest distance from that starting node to each other node. Dijkstra's algorithm starts with infinite distances and tries to improve them step by step:\nof all unvisited nodes: the unvisited set.\nAssign to every node a distance from start value: for the starting node, it is zero, and for all other nodes, it is infinity, since initially no path is known to these nodes. During execution, the distance of a node\nis the length of the shortest path discovered so far between the starting node and\nFrom the unvisited set, select the current node to be the one with the smallest (finite) distance; initially, this is the starting node (distance zero). If the unvisited set is empty, or contains only nodes with infinite distance (which are unreachable), then the algorithm terminates by skipping to step 6. If the only concern is the path to a target node, the algorithm terminates once the current node is the target node. Otherwise, the algorithm continues.\nFor the current node, consider all of its unvisited neighbors and update their distances through the current node; compare the newly calculated distance to the one currently assigned to the neighbor and assign the smaller one to it. For example, if the current node\nis marked with a distance of 6, and the edge connecting it with its neighbor\nhas length 2, then the distance to\nis 6 + 2 = 8. If B was previously marked with a distance greater than 8, then update it to 8 (the path to B through A is shorter). Otherwise, keep its current distance (the path to B through A is not the shortest).\nAfter considering all of the current node's unvisited neighbors, the current node is removed from the unvisited set. Thus a visited node is never rechecked, which is correct because the distance recorded on the current node is minimal (as ensured in step 3), and thus final. Repeat from step 3.\nOnce the loop exits (steps 3–5), every visited node contains its shortest distance from the starting node.\nNote: For ease of understanding, this discussion uses the terms intersection, road and map – however, in formal terminology these terms are vertex, edge and graph, respectively.\nThe shortest path between two\non a city map can be found by this algorithm using pencil and paper. Every intersection is listed on a separate line: one is the starting point and is labeled (given a distance of) 0. Every other intersection is initially labeled with a distance of infinity. This is done to note that no path to these intersections has yet been established. At each iteration one intersection becomes the current intersection. For the first iteration, this is the starting point.\nFrom the current intersection, the distance to every\n(directly-connected) intersection is assessed by summing the label (value) of the current intersection and the distance to the neighbor and then\nthe neighbor with the lesser of that sum and the neighbor's existing label. I.e., the neighbor is relabeled if the path to it through the current intersection is shorter than previously assessed paths. If so, mark the road to the neighbor with an arrow pointing to it, and erase any other arrow that points to it. After the distances to each of the current intersection's neighbors have been assessed, the current intersection is marked as visited. The unvisited intersection with the smallest label becomes the current intersection and the process repeats until all nodes with labels less than the destination's label have been visited.\nOnce no unvisited nodes remain with a label smaller than the destination's label, the remaining arrows show the shortest path.\nis an array that contains the current distances from the\nto other vertices, i.e.\nis the current distance from the source to the vertex\narray contains pointers to previous-hop nodes on the shortest path from source to the given vertex (equivalently, it is the\nthe source). The code\nu ← vertex in Q with min dist[u]\n, searches for the vertex\nGraph.Edges( u , v )\nreturns the length of the edge joining (i.e. the distance between) the two neighbor-nodes\non line 14 is the length of the path from the\nnode to the neighbor node\nif it were to go through\n. If this path is shorter than the current shortest path recorded for\n, then the distance of\n[ 7 ] A demo of Dijkstra's algorithm based on Euclidean distance. Red lines are the shortest path covering, i.e., connecting u and prev[ u ]. Blue lines indicate where relaxing happens, i.e., connecting v with a node u in Q , which gives a shorter path from the source to v .\nwith minimum dist[v]\nTo find the shortest path between vertices\n, the search terminates after line 10 if\n. The shortest path from\ncan be obtained by reverse iteration:\n// Proceed if the vertex is reachable\n// Construct the shortest path with a stack S\n// Push the vertex onto the stack\n// Traverse from target to source\nis the list of vertices constituting one of the shortest paths from\n, or the empty sequence if no path exists.\nA more general problem is to find all the shortest paths between\n(there might be several of the same length). Then instead of storing only a single node in each entry of\nall nodes satisfying the relaxation condition can be stored. For example, if both\nand they lie on different shortest paths through\n(because the edge cost is the same in both cases), then both\n. When the algorithm completes,\ndata structure describes a graph that is a subset of the original graph with some edges removed. Its key property is that if the algorithm was run with some starting node, then every path from that node to any other node in the new graph is the shortest path between those nodes graph, and all paths of that length from the original graph are present in the new graph. Then to actually find all these shortest paths between two given nodes, a path finding algorithm on the new graph, such as\nUsing a priority queue\nA min-priority queue is an abstract data type that provides 3 basic operations:\n. As mentioned earlier, using such a data structure can lead to faster computing times than using a basic queue. Notably,\nFibonacci heap [ 19 ]\noffer optimal implementations for those 3 operations. As the algorithm is slightly different in appearance, it is mentioned here, in pseudocode as well:\n2 Q ← Queue storing vertex priority\n// associated priority equals dist[·]\n// Unknown distance from source to v\n11 Q.add_with_priority(v, INFINITY)\n// Remove and return best vertex\n// Go through all v neighbors of u\nInstead of filling the priority queue with all nodes in the initialization phase, it is possible to initialize it to contain only\nYet another alternative is to add nodes unconditionally to the priority queue and to instead check after extraction (\nu ← Q .extract_min()\n) that it isn't revisiting, or that no shorter connection was found yet in the\nblock. This can be done by additionally extracting the associated priority\nfrom the queue and only processing further\nwhile Q is not empty\nThese alternatives can use entirely array-based priority queues without decrease-key functionality, which have been found to achieve even faster computing times in practice. However, the difference in performance was found to be narrower for denser graphs.\nof Dijkstra's algorithm,\nmathematical induction\ncan be used on the number of visited nodes.\n[ 22 ] Invariant hypothesis\n: For each visited node\nis the shortest distance from\n, and for each unvisited node\nis the shortest distance from\nwhen traveling via visited nodes only, or infinity if no such path exists. (Note: we do not assume\nis the actual shortest distance for unvisited nodes, while\nis the actual shortest distance)\nThe base case is when there is just one visited node,\n. Its distance is defined to be zero, which is the shortest distance, since negative weights are not allowed. Hence, the hypothesis holds.\nAssuming that the hypothesis holds for\nvisited nodes, to show it holds for\nk + 1 {\\displaystyle k+1}\nbe the next visited node, i.e. the node with minimum\nis the shortest distance from\nThe proof is by contradiction. If a shorter path were available, then this shorter path either contains another unvisited node or not.\nIn the former case, let\nbe the first unvisited node on this shorter path. By induction, the shortest paths from\nthrough visited nodes only have costs\nrespectively. This means the cost of going from\nhas the cost of at least\n+ the minimal cost of going from\n. As the edge costs are positive, the minimal cost of going from\nis a positive number. However,\nbecause otherwise w would have been picked by the priority queue instead of u. This is a contradiction, since it has already been established that\n+ a positive number \u003c\nIn the latter case, let\nbe the last but one node on the shortest path. That means\ndist[w] + Graph.Edges[w,u] \u003c dist[u]\n. That is a contradiction because by the time\nis visited, it should have set\ndist[w] + Graph.Edges[w,u]\nFor all other visited nodes\nis already known to be the shortest distance from\nalready, because of the inductive hypothesis, and these values are unchanged.\n, it is still true that for each unvisited node\nis the shortest distance from\nusing visited nodes only. Any shorter path that did not use\n, would already have been found, and if a shorter path used\nit would have been updated when processing\nAfter all nodes are visited, the shortest path from\nconsists only of visited nodes. Therefore,\nis the shortest distance.\nBounds of the running time of Dijkstra's algorithm on a graph with edges\ncan be expressed as a function of the number of edges, denoted\n| E | {\\displaystyle |E|}\n, and the number of vertices, denoted\n| V | {\\displaystyle |V|}\n. The complexity bound depends mainly on the data structure used to represent the set\n. In the following, upper bounds can be simplified because\n| E | {\\displaystyle |E|}\nO ( | V | 2 ) {\\displaystyle O(|V|^{2})}\nfor any simple graph, but that simplification disregards the fact that in some problems, other upper bounds on\n| E | {\\displaystyle |E|}\nFor any data structure for the vertex set\n, the running time is:\n[ 2 ] Θ ( | E | ⋅ T d k + | V | ⋅ T e m ) , {\\displaystyle \\Theta (|E|\\cdot T_{\\mathrm {dk} }+|V|\\cdot T_{\\mathrm {em} }),}\nT d k {\\displaystyle T_{\\mathrm {dk} }}\nT e m {\\displaystyle T_{\\mathrm {em} }}\nare the complexities of the\nThe simplest version of Dijkstra's algorithm stores the vertex set\nas a linked list or array, and edges as an\n. In this case, extract-minimum is simply a linear search through all vertices in\n, so the running time is\nΘ ( | E | + | V | 2 ) = Θ ( | V | 2 ) {\\displaystyle \\Theta (|E|+|V|^{2})=\\Theta (|V|^{2})}\n, that is, graphs with far fewer than\n| V | 2 {\\displaystyle |V|^{2}}\nedges, Dijkstra's algorithm can be implemented more efficiently by storing the graph in the form of adjacency lists and using a\nself-balancing binary search tree\nor a priority heap as a\nto implement extracting minimum efficiently. To perform decrease-key steps in a binary heap efficiently, it is necessary to use an auxiliary data structure that maps each vertex to its position in the heap, and to update this structure as the priority queue\nchanges. With a self-balancing binary search tree or binary heap, the algorithm requires\nΘ ( ( | E | + | V | ) log ⁡ | V | ) {\\displaystyle \\Theta ((|E|+|V|)\\log |V|)}\ntime in the worst case; for connected graphs this time bound can be simplified to\nΘ ( | E | log ⁡ | V | ) {\\displaystyle \\Theta (|E|\\log |V|)}\nΘ ( | E | + | V | log ⁡ | V | ) . {\\displaystyle \\Theta (|E|+|V|\\log |V|).}\nWhen using binary heaps, the\ntime complexity is lower than the worst-case: assuming edge costs are drawn independently from a common\nprobability distribution\n, the expected number of\noperations is bounded by\nΘ ( | V | log ⁡ ( | E | / | V | ) ) {\\displaystyle \\Theta (|V|\\log(|E|/|V|))}\n, giving a total running time of\n[ 7 ] : 199–200 O ( | E | + | V | log ⁡ | E | | V | log ⁡ | V | ) . {\\displaystyle O\\left(|E|+|V|\\log {\\frac {|E|}{|V|}}\\log |V|\\right).}\nPractical optimizations and infinite graphs\nIn common presentations of Dijkstra's algorithm, initially all nodes are entered into the priority queue. This is, however, not necessary: the algorithm can start with a priority queue that contains only one item, and insert new items as they are discovered (instead of doing a decrease-key, check whether the key is in the queue; if it is, decrease its key, otherwise insert it).\nThis variant has the same worst-case bounds as the common variant, but maintains a smaller priority queue in practice, speeding up queue operations.\nMoreover, not inserting all nodes in a graph makes it possible to extend the algorithm to find the shortest path from a single source to the closest of a set of target nodes on infinite graphs or those too large to represent in memory. The resulting algorithm is called\n(UCS) in the artificial intelligence literature\n[ 12 ] [ 23 ] [ 24 ]\nand can be expressed in pseudocode as\nuniform_cost_search(start)\nfrontier ← priority queue containing node only\nexpanded ← empty set\nnode ← frontier.pop()\nnode is a goal state\nis not in expanded and not in frontier\nis in frontier with higher cost\nreplace existing node with\nIts complexity can be expressed in an alternative way for very large graphs: when\nis the length of the shortest path from the start node to any node satisfying the \"goal\" predicate, each edge has cost at least\n, and the number of neighbors per node is bounded by\n, then the algorithm's worst-case time and space complexity are both in\nO ( b 1+⌊ C * ⁄ ε ⌋ )\nFurther optimizations for the single-target case include\nvariants, goal-directed variants such as the\n§ Related problems and algorithms\n), graph pruning to determine which nodes are likely to form the middle segment of shortest paths (reach-based routing), and hierarchical decompositions of the input graph that reduce\nrouting to connecting\nto their respective \"\n\" followed by shortest-path computation between these transit nodes using a \"highway\".\nCombinations of such techniques may be needed for optimal practical performance on specific problems.\nOptimality for comparison-sorting by distance\nAs well as simply computing distances and paths, Dijkstra's algorithm can be used to sort vertices by their distances from a given starting vertex.\nIn 2023, Haeupler, Rozhoň, Tětek, Hladík, and\n(one of the inventors of the 1984 heap), proved that, for this sorting problem on a positively-weighted directed graph, a version of Dijkstra's algorithm with a special heap data structure has a runtime and number of comparisons that is within a constant factor of optimal among\nalgorithms for the same sorting problem on the same graph and starting vertex but with variable edge weights. To achieve this, they use a comparison-based heap whose cost of returning/removing the minimum element from the heap is logarithmic in the number of elements inserted after it rather than in the number of elements in the heap.\nSpecialized variants\nWhen arc weights are small integers (bounded by a parameter\n), specialized queues can be used for increased speed. The first algorithm of this type was Dial's algorithm\nfor graphs with positive integer edge weights, which uses a\nto obtain a running time\nO ( | E | + | V | C ) {\\displaystyle O(|E|+|V|C)}\nas the priority queue brings the complexity to\nO ( | E | + | V | log ⁡ C / log ⁡ log ⁡ | V | C ) {\\displaystyle O(|E|+|V|\\log C/\\log \\log |V|C)}\nAnother interesting variant based on a combination of a new\nand the well-known Fibonacci heap runs in time\nO ( | E | + | V | log ⁡ C ) {\\displaystyle O(|E|+|V|{\\sqrt {\\log C}})}\nFinally, the best algorithms in this special case run in\nO ( | E | log ⁡ log ⁡ | V | ) {\\displaystyle O(|E|\\log \\log |V|)} [ 31 ]\nO ( | E | + | V | min { ( log ⁡ | V | ) 1 / 3 + ε , ( log ⁡ C ) 1 / 4 + ε } ) {\\displaystyle O(|E|+|V|\\min\\{(\\log |V|)^{1/3+\\varepsilon },(\\log C)^{1/4+\\varepsilon }\\})}\nRelated problems and algorithms\nDijkstra's original algorithm can be extended with modifications. For example, sometimes it is desirable to present solutions which are less than mathematically optimal. To obtain a ranked list of less-than-optimal solutions, the optimal solution is first calculated. A single edge appearing in the optimal solution is removed from the graph, and the optimum solution to this new graph is calculated. Each edge of the original solution is suppressed in turn and a new shortest-path calculated. The secondary solutions are then ranked and presented after the first optimal solution.\nDijkstra's algorithm is usually the working principle behind\nlink-state routing protocols\nare the most common.\nUnlike Dijkstra's algorithm, the\nBellman–Ford algorithm\ncan be used on graphs with negative edge weights, as long as the graph contains no\nreachable from the source vertex\n. The presence of such cycles means that no shortest path can be found, since the label becomes lower each time the cycle is traversed. (This statement assumes that a \"path\" is allowed to repeat vertices. In\nthat is normally not allowed. In\ntheoretical computer science\nit often is allowed.) It is possible to adapt Dijkstra's algorithm to handle negative weights by combining it with the Bellman-Ford algorithm (to remove negative edges and detect negative cycles):\nis a generalization of Dijkstra's algorithm that reduces the size of the subgraph that must be explored, if additional information is available that provides a lower bound on the distance to the target.\nThe process that underlies Dijkstra's algorithm is similar to the\n. Prim's purpose is to find a\nminimum spanning tree\nthat connects all nodes in the graph; Dijkstra is concerned with only two nodes. Prim's does not evaluate the total weight of the path from the starting node, only the individual edges.\nBreadth-first search\ncan be viewed as a special-case of Dijkstra's algorithm on unweighted graphs, where the priority queue degenerates into a\nfast marching method\ncan be viewed as a continuous version of Dijkstra's algorithm which computes the geodesic distance on a triangle mesh.\nDynamic programming perspective\npoint of view, Dijkstra's algorithm is a successive approximation scheme that solves the dynamic programming functional equation for the shortest path problem by the\n[ 33 ] [ 34 ] [ 35 ]\nIn fact, Dijkstra's explanation of the logic behind the algorithm:\nFind the path of minimum total length between two given nodes\nWe use the fact that, if\nis a node on the minimal path from\n, knowledge of the latter implies the knowledge of the minimal path from\nis a paraphrasing of\nBellman's Principle of Optimality\nin the context of the shortest path problem.\n[ edit ] A* search algorithm Bellman–Ford algorithm Euclidean shortest path Floyd–Warshall algorithm Johnson's algorithm Longest path problem Parallel all-pairs shortest path algorithm\n[ edit ] ^ Controversial, see Moshe Sniedovich (2006). \"Dijkstra's algorithm revisited: the dynamic programming connexion\" . Control and Cybernetics . 35 : 599– 620. and below part . ^ a b Cormen et al. 2001 . ^ a b Fredman \u0026 Tarjan 1987 . ^ Richards, Hamilton. \"Edsger Wybe Dijkstra\" . A.M. Turing Award . Association for Computing Machinery . Retrieved 16 October 2017 . At the Mathematical Centre a major project was building the ARMAC computer. For its official inauguration in 1956, Dijkstra devised a program to solve a problem interesting to a nontechnical audience: Given a network of roads connecting cities, what is the shortest route between two designated cities? ^ a b c Frana, Phil (August 2010). \"An Interview with Edsger W. Dijkstra\". Communications of the ACM . 53 (8): 41– 47. doi : 10.1145/1787234.1787249 . S2CID 27009702 . ^ Dijkstra, E. W. (1959). \"A note on two problems in connexion with graphs\" (PDF) . Numerische Mathematik . 1 : 269– 271. CiteSeerX 10.1.1.165.7577 . doi : 10.1007/BF01386390 . S2CID 123284777 . ^ a b c d e Mehlhorn, Kurt ; Sanders, Peter (2008). \"Chapter 10. Shortest Paths\" (PDF) . Algorithms and Data Structures: The Basic Toolbox . Springer. doi : 10.1007/978-3-540-77978-0 . ISBN 978-3-540-77977-3 . ^ Schrijver, Alexander (2012). \"On the history of the shortest path problem\" (PDF) . Optimization Stories . Documenta Mathematica Series. Vol. 6. pp. 155– 167. doi : 10.4171/dms/6/19 . ISBN 978-3-936609-58-5 . ^ Leyzorek et al. 1957 . ^ Szcześniak, Ireneusz; Jajszczyk, Andrzej; Woźna-Szcześniak, Bożena (2019). \"Generic Dijkstra for optical networks\". Journal of Optical Communications and Networking . 11 (11): 568– 577. arXiv : 1810.04481 . doi : 10.1364/JOCN.11.000568 . S2CID 52958911 . ^ Szcześniak, Ireneusz; Woźna-Szcześniak, Bożena (2023), \"Generic Dijkstra: Correctness and tractability\", NOMS 2023-2023 IEEE/IFIP Network Operations and Management Symposium , pp. 1– 7, arXiv : 2204.13547 , doi : 10.1109/NOMS56928.2023.10154322 , ISBN 978-1-6654-7716-1 , S2CID 248427020 ^ a b c Felner, Ariel (2011). Position Paper: Dijkstra's Algorithm versus Uniform Cost Search or a Case Against Dijkstra's Algorithm . Proc. 4th Int'l Symp. on Combinatorial Search. Archived from the original on 18 February 2020 . Retrieved 12 February 2015 . In a route-finding problem, Felner finds that the queue can be a factor 500–600 smaller, taking some 40% of the running time. ^ \"ARMAC\" . Unsung Heroes in Dutch Computing History . 2007. Archived from the original on 13 November 2013. ^ Dijkstra, Edsger W., Reflections on \"A note on two problems in connexion with graphs (PDF) ^ Tarjan, Robert Endre (1983), Data Structures and Network Algorithms , CBMS_NSF Regional Conference Series in Applied Mathematics, vol. 44, Society for Industrial and Applied Mathematics, p. 75, The third classical minimum spanning tree algorithm was discovered by Jarník and rediscovered by Prim and Dikstra; it is commonly known as Prim's algorithm. ^ Prim, R.C. (1957). \"Shortest connection networks and some generalizations\" (PDF) . Bell System Technical Journal . 36 (6): 1389– 1401. Bibcode : 1957BSTJ...36.1389P . doi : 10.1002/j.1538-7305.1957.tb01515.x . Archived from the original (PDF) on 18 July 2017 . Retrieved 18 July 2017 . ^ V. Jarník: O jistém problému minimálním [About a certain minimal problem], Práce Moravské Přírodovědecké Společnosti, 6, 1930, pp. 57–63. (in Czech) ^ Gass, Saul; Fu, Michael (2013). \"Dijkstra's Algorithm\". In Gass, Saul I; Fu, Michael C (eds.). Encyclopedia of Operations Research and Management Science . Vol. 1. Springer. doi : 10.1007/978-1-4419-1153-7 . ISBN 978-1-4419-1137-7 – via Springer Link. ^ Fredman \u0026 Tarjan 1984 . ^ Observe that p \u003c dist[ u ] cannot ever hold because of the update dist[ v ] ← alt when updating the queue. See https://cs.stackexchange.com/questions/118388/dijkstra-without-decrease-key for discussion. ^ Chen, M.; Chowdhury, R. A.; Ramachandran, V.; Roche, D. L.; Tong, L. (2007). Priority Queues and Dijkstra's Algorithm – UTCS Technical Report TR-07-54 – 12 October 2007 (PDF) . Austin, Texas: The University of Texas at Austin, Department of Computer Sciences. ^ Cormen, Thomas H. ; Leiserson, Charles E. ; Rivest, Ronald L. ; Stein, Clifford (2022) [1990]. \"22\". Introduction to Algorithms (4th ed.). MIT Press and McGraw-Hill. pp. 622– 623. ISBN 0-262-04630-X . ^ a b Russell, Stuart ; Norvig, Peter (2009) [1995]. Artificial Intelligence: A Modern Approach (3rd ed.). Prentice Hall. pp. 75, 81. ISBN 978-0-13-604259-4 . ^ Sometimes also least-cost-first search : Nau, Dana S. (1983). \"Expert computer systems\" (PDF) . Computer . 16 (2). IEEE: 63– 85. doi : 10.1109/mc.1983.1654302 . S2CID 7301753 . ^ Wagner, Dorothea; Willhalm, Thomas (2007). Speed-up techniques for shortest-path computations . STACS. pp. 23– 36. ^ Bauer, Reinhard; Delling, Daniel; Sanders, Peter; Schieferdecker, Dennis; Schultes, Dominik; Wagner, Dorothea (2010). \"Combining hierarchical and goal-directed speed-up techniques for Dijkstra's algorithm\" . ACM Journal of Experimental Algorithmics . 15 : 2.1. doi : 10.1145/1671970.1671976 . S2CID 1661292 . ^ Haeupler, Bernhard; Hladík, Richard; Rozhoň, Václav; Tarjan, Robert; Tětek, Jakub (28 October 2024). \"Universal Optimality of Dijkstra via Beyond-Worst-Case Heaps\". arXiv : 2311.11793 [ cs.DS ]. ^ Brubaker, Ben (25 October 2024). \"Computer Scientists Establish the Best Way to Traverse a Graph\" . Quanta Magazine . Retrieved 9 December 2024 . ^ Dial 1969 . ^ a b Ahuja et al. 1990 . ^ Thorup 2000 . ^ Raman 1997 . ^ Sniedovich, M. (2006). \"Dijkstra's algorithm revisited: the dynamic programming connexion\" (PDF) . Journal of Control and Cybernetics . 35 (3): 599– 620. Online version of the paper with interactive computational modules. ^ Denardo, E.V. (2003). Dynamic Programming: Models and Applications . Mineola, NY: Dover Publications . ISBN 978-0-486-42810-9 . ^ Sniedovich, M. (2010). Dynamic Programming: Foundations and Principles . Francis \u0026 Taylor . ISBN 978-0-8247-4099-3 . ^ Dijkstra 1959 , p. 270.\n[ edit ] Cormen, Thomas H. ; Leiserson, Charles E. ; Rivest, Ronald L. ; Stein, Clifford (2001). \"Section 24.3: Dijkstra's algorithm\". Introduction to Algorithms (Second ed.). MIT Press and McGraw–Hill . pp. 595– 601. ISBN 0-262-03293-7 . Dial, Robert B. (1969). \"Algorithm 360: Shortest-path forest with topological ordering [H]\" . Communications of the ACM . 12 (11): 632– 633. doi : 10.1145/363269.363610 . S2CID 6754003 . Fredman, Michael Lawrence ; Tarjan, Robert E. (1984). Fibonacci heaps and their uses in improved network optimization algorithms . 25th Annual Symposium on Foundations of Computer Science. IEEE . pp. 338– 346. doi : 10.1109/SFCS.1984.715934 . Fredman, Michael Lawrence ; Tarjan, Robert E. (1987). \"Fibonacci heaps and their uses in improved network optimization algorithms\" . Journal of the Association for Computing Machinery . 34 (3): 596– 615. doi : 10.1145/28869.28874 . S2CID 7904683 . Zhan, F. Benjamin; Noon, Charles E. (February 1998). \"Shortest Path Algorithms: An Evaluation Using Real Road Networks\". Transportation Science . 32 (1): 65– 73. doi : 10.1287/trsc.32.1.65 . S2CID 14986297 . Leyzorek, M.; Gray, R. S.; Johnson, A. A.; Ladew, W. C.; Meaker, Jr., S. R.; Petry, R. M.; Seitz, R. N. (1957). Investigation of Model Techniques – First Annual Report – 6 June 1956 – 1 July 1957 – A Study of Model Techniques for Communication Systems . Cleveland, Ohio: Case Institute of Technology. Knuth, D.E. (1977). \"A Generalization of Dijkstra's Algorithm\". Information Processing Letters . 6 (1): 1– 5. doi : 10.1016/0020-0190(77)90002-3 . Ahuja, Ravindra K.; Mehlhorn, Kurt; Orlin, James B.; Tarjan, Robert E. (April 1990). \"Faster Algorithms for the Shortest Path Problem\" (PDF) . Journal of the ACM . 37 (2): 213– 223. doi : 10.1145/77600.77615 . hdl : 1721.1/47994 . S2CID 5499589 . Raman, Rajeev (1997). \"Recent results on the single-source shortest paths problem\" . SIGACT News . 28 (2): 81– 87. doi : 10.1145/261342.261352 . S2CID 18031586 . Thorup, Mikkel (2000). \"On RAM priority Queues\". SIAM Journal on Computing . 30 (1): 86– 109. doi : 10.1137/S0097539795288246 . S2CID 5221089 . Thorup, Mikkel (1999). \"Undirected single-source shortest paths with positive integer weights in linear time\" . Journal of the ACM . 46 (3): 362– 394. doi : 10.1145/316542.316548 . S2CID 207654795 .\nWikimedia Commons has media related to\nDijkstra's algorithm\nOral history interview with Edsger W. Dijkstra\nCharles Babbage Institute\n, University of Minnesota, Minneapolis\nImplementation of Dijkstra's algorithm using TDD\n, The Clean Code Blog\nv t e Edsger Dijkstra Works A Primer of ALGOL 60 Programming\nStructured Programming\nA Discipline of Programming\nA Method of Programming\nPredicate Calculus and Program Semantics\nSelected Writings on Computing: A Personal Perspective\nA Note on Two Problems in Connexion with Graphs Cooperating Sequential Processes Solution of a Problem in Concurrent Programming Control The Structure of the 'THE'-Multiprogramming System Go To Statement Considered Harmful Notes on Structured Programming The Humble Programmer Programming Considered as a Human Activity How Do We Tell Truths That Might Hurt? On the Role of Scientific Thought Self-stabilizing Systems in Spite of Distributed Control On the Cruelty of Really Teaching Computer Science Selected papers EWD manuscripts Main research areas Theoretical computing science Software engineering Systems science Algorithm design\nDijkstra's algorithm\nConcurrent computing Distributed computing Formal methods Programming methodology Programming language research Program design\ndevelopment Software architecture Philosophy of computer programming and computing science\nShlomi Dolev Per Brinch Hansen Tony Hoare Ole-Johan Dahl Leslie Lamport David Parnas Jaap A. Zonneveld Carel S. Scholten Adriaan van Wijngaarden Niklaus Wirth Wikiquote v t e Graph\ntraversal algorithms\nSearch α–β pruning A* IDA* LPA* SMA* Best-first search Beam search Bidirectional search Breadth-first search Lexicographic Parallel B* Depth-first search Iterative deepening D* Fringe search Jump point search Monte Carlo tree search SSS* Shortest path Bellman–Ford Dijkstra's Floyd–Warshall Johnson's Shortest path faster Yen's Minimum spanning tree Borůvka's Kruskal's Prim's Reverse-delete List of graph search algorithms v t e Optimization\nheuristics Unconstrained nonlinear Functions Golden-section search Powell's method Line search Nelder–Mead method Successive parabolic interpolation Gradients Convergence Trust region Wolfe conditions Quasi–Newton Berndt–Hall–Hall–Hausman Broyden–Fletcher–Goldfarb–Shanno\nL-BFGS Davidon–Fletcher–Powell Symmetric rank-one (SR1) Other methods Conjugate gradient Gauss–Newton Gradient Mirror Levenberg–Marquardt Powell's dog leg method Truncated Newton Hessians Newton's method Optimization computes maxima and minima. Constrained nonlinear\nBarrier methods Penalty methods\nAugmented Lagrangian methods Sequential quadratic programming Successive linear programming Convex optimization Convex minimization Cutting-plane method Reduced gradient (Frank–Wolfe) Subgradient method Linear\nquadratic Interior point Affine scaling Ellipsoid algorithm of Khachiyan Projective algorithm of Karmarkar Basis- exchange Simplex algorithm of Dantzig Revised simplex algorithm Criss-cross algorithm Principal pivoting algorithm of Lemke Active-set method Combinatorial\nApproximation algorithm Dynamic programming Greedy algorithm Integer programming Branch and bound\ncut Graph algorithms Minimum spanning tree Borůvka Prim Kruskal Shortest path Bellman–Ford SPFA Dijkstra Floyd–Warshall Network flows Dinic Edmonds–Karp Ford–Fulkerson Push–relabel maximum flow Metaheuristics Evolutionary algorithm Hill climbing Local search Parallel metaheuristics Simulated annealing Spiral optimization algorithm Tabu search Software\nhttps://en.wikipedia.org/w/index.php?title=Dijkstra%27s_algorithm\u0026oldid=1301664509\nEdsger W. Dijkstra 1959 in computing Graph algorithms Search algorithms Greedy algorithms Routing algorithms Combinatorial optimization Dutch inventions Graph distance\nArticles with short description Short description is different from Wikidata Use dmy dates from December 2019 Commons category link is on Wikidata Articles with example pseudocode Search Search Dijkstra's algorithm 49 languages Add topic",
          "role": "teacher"
        },
        {
          "content": "Why is this important to understand?",
          "role": "student"
        },
        {
          "content": "Understanding Dijkstra's Algorithm is crucial for building efficient, scalable, and maintainable software systems. It forms the foundation for solving complex computational problems.",
          "role": "teacher"
        },
        {
          "content": "Can you give me a concrete example?",
          "role": "student"
        },
        {
          "content": "For example, Dijkstra's Algorithm can be applied in scenarios where you need to optimize performance, manage complexity, or implement scalable solutions.",
          "role": "teacher"
        }
      ],
      "format": "socratic_dialogue",
      "metadata": {
        "domain": "Algorithms",
        "source": "https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm",
        "words": 6898
      },
      "topic": "Dijkstra's Algorithm"
    },
    {
      "conversation": [
        {
          "content": "How do I implement Dijkstra's Algorithm?",
          "role": "user"
        },
        {
          "content": "Here's how to implement Dijkstra's Algorithm:\n\n1. Understand the core algorithm:\n• has length 2, then the distance to\n• Once the loop exits (steps 3–5), every visited node contains its shortest distance from the starting node.\n• (directly-connected) intersection is assessed by summing the label (value) of the current intersection and the distance to the neighbor and then\n• , then the distance of\n• (because the edge cost is the same in both cases), then both\n\n2. Choose appropriate data structures\n3. Handle edge cases and validation\n4. Optimize for your use case\n5. Test thoroughly with various inputs\n\nBasic structure:\n```\nfunction implementDijkstra'sAlgorithm(input) {\n    // Initialize\n    // Process\n    // Return result\n}\n```",
          "role": "assistant"
        }
      ],
      "format": "implementation_guide",
      "metadata": {
        "domain": "Algorithms",
        "type": "code_focused"
      },
      "topic": "Dijkstra's Algorithm"
    },
    {
      "content": {
        "applications": "application of shortest path algorithms is network\n(Intermediate System to Intermediate System) and\n(Open Shortest Path First). It is also employed as a\nin algorithms such as\nThe algorithm uses a\nmin-priority queue data structure\nfor selecting the shortest paths known so far. Before more advanced priority queue structures were discovered, Dijkstra's original algorithm ran in\nΘ ( | V | 2 ) {\\displaystyle \\Theta (|V|^{2})} time\n| V | {\\displaystyle |V|}\nis the number of nodes.\n[ 8 ] [ 9 ] Fredman",
        "considerations": "consider all of its unvisited neighbors and update their distances through the current node; compare the newly calculated distance to the one currently assigned to the neighbor and assign the smaller one to it. For example, if the current node\nis marked with a distance of 6, and the edge connecting it with its neighbor\nhas length 2, then the distance to\nis 6 + 2 = 8. If B was previously marked wit",
        "key_concepts": [
          "Core principles and foundations",
          "Implementation strategies",
          "Performance characteristics",
          "Common use cases",
          "Best practices"
        ],
        "overview": "A comprehensive technical concept with wide applications."
      },
      "format": "conceptual_breakdown",
      "metadata": {
        "domain": "Algorithms",
        "structure": "hierarchical"
      },
      "topic": "Dijkstra's Algorithm"
    },
    {
      "format": "problem_solving",
      "scenario": {
        "approach": "solves the dynamic programming functional equation for the shortest path problem by the\n[ 33 ] [ 34 ] [ 35 ]\nIn fact, Dijkstra's explanation of the logic behind the algorithm:\nFind the path of minimum total length between two given nodes\nWe use the fact that, if\nis a node on the minimal path from\n, knowledge of the latter implies the knowledge of the minimal path from\nis a paraphrasing of\nBellman's Principle of Optimality\nin the context of the shortest path problem.\n[ edit ] A* search algorithm ",
        "benefits": "advantages of designing without pencil and paper is that you are almost forced to avoid all avoidable complexities. Eventually, that algorithm became to my great amazement, one of the cornerstones of my fame.\n— Edsger Dijkstra, in an interview with Philip L. Frana, Communications of the ACM, 2001\n",
        "problem": "How can Dijkstra's Algorithm solve real-world problems?",
        "tradeoffs": "costs of edges represent the distances between pairs of cities connected by a direct road, then Dijkstra's algorithm can be used to find the shortest route between one city and all other cities. A common application of shortest path algorithms is network\n(Intermediate System to Intermediate System) "
      },
      "topic": "Dijkstra's Algorithm"
    },
    {
      "comparison": "compare the newly calculated distance to the one currently assigned to the neighbor and assign the smaller one to it. For example, if the current node\nis marked with a distance of 6, and the edge connecting it with its neighbor\nhas length 2, then the distance to\nis 6 + 2 = 8. If B was previously marked with a distance greater than 8, then update it to 8 (the path to B through A is shorter). Otherwise, keep its current distance (the path to B through A is not the shortest).\nAfter considering all ",
      "format": "comparative_analysis",
      "topic": "Dijkstra's Algorithm"
    },
    {
      "dialogue": [
        {
          "content": "What is A* Search really about at its core?",
          "role": "student"
        },
        {
          "content": "Algorithm used for pathfinding and graph traversal\n\"A Star\" redirects here. For other uses, see\nAstar (disambiguation)\nGraph Worst-case performance O ( | E | log ⁡ | V | ) = O ( b d ) {\\displaystyle O(|E|\\log |V|)=O(b^{d})} Worst-case space complexity O ( | V | ) = O ( b d ) {\\displaystyle O(|V|)=O(b^{d})} A*\n(pronounced \"A-star\") is a\npathfinding algorithm\nthat is used in many fields of\ndue to its completeness, optimality, and optimal efficiency.\nand a goal node, the algorithm finds the\n(with respect to the given weights) from source to goal.\nOne major practical drawback is its\nO ( b d ) {\\displaystyle O(b^{d})} space complexity\nis the depth of the shallowest solution (the length of the shortest path from the source node to any given goal node) and\n(the maximum number of successors for any given state), as it stores all generated nodes in memory. Thus, in practical\ntravel-routing systems\n, it is generally outperformed by algorithms that can pre-process the graph to attain better performance,\nas well as by memory-bounded approaches; however, A* is still the best solution in many cases.\nof Stanford Research Institute (now\n) first published the algorithm in 1968.\nIt can be seen as an extension of\nDijkstra's algorithm\n. A* achieves better performance by using\nto guide its search.\nCompared to Dijkstra's algorithm, the A* algorithm only finds the shortest path from a specified source to a specified goal, and not the shortest-path tree from a specified source to all possible goals. This is a necessary\nfor using a specific-goal-directed\n. For Dijkstra's algorithm, since the entire shortest-path tree is generated, every node is a goal, and there can be no specific-goal-directed heuristic.\n[ edit ] A* was invented by researchers working on Shakey the Robot's path planning.\nA* was created as part of\n, which had the aim of building a mobile robot that could plan its own actions. Nils Nilsson originally proposed using the\nfor Shakey's path planning.\nGraph Traverser is guided by a heuristic function\n, the estimated distance from node\nto the goal node: it entirely ignores\n, the distance from the start node to\n. Bertram Raphael suggested using the sum,\nPeter Hart invented the concepts we now call\nof heuristic functions. A* was originally designed for finding least-cost paths when the cost of a path is the sum of its costs, but it has been shown that A* can be used to find optimal paths for any problem satisfying the conditions of a cost algebra.\nThe original 1968 A* paper\ncontained a theorem stating that no A*-like algorithm\ncould expand fewer nodes than A* if the heuristic function is consistent and A*'s tie-breaking rule is suitably chosen. A \"correction\" was published a few years later\nclaiming that consistency was not required, but this was shown to be false in 1985 in Dechter and Pearl's definitive study of A*'s optimality (now called optimal efficiency), which gave an example of A* with a heuristic that was admissible but not consistent expanding arbitrarily more nodes than an alternative A*-like algorithm.\n[ edit ] A* pathfinding algorithm navigating around a randomly-generated maze Illustration of A* search for finding a path between two points on a graph. From left to right, a heuristic that prefers points closer to the goal is used increasingly.\ninformed search algorithm\n, meaning that it is formulated in terms of\n: starting from a specific starting\nof a graph, it aims to find a path to the given goal node having the smallest cost (least distance travelled, shortest time, etc.). It does this by maintaining a\nof paths originating at the start node and extending those paths one edge at a time until the goal node is reached.\nAt each iteration of its main loop, A* needs to determine which of its paths to extend. It does so based on the cost of the path and an estimate of the cost required to extend the path all the way to the goal. Specifically, A* selects the path that minimizes\nf ( n ) = g ( n ) + h ( n ) {\\displaystyle f(n)=g(n)+h(n)}\nis the next node on the path,\nis the cost of the path from the start node to\nfunction that estimates the cost of the cheapest path from\nto the goal. The heuristic function is problem-specific. If the heuristic function is\n– meaning that it never overestimates the actual cost to get to the goal – A* is guaranteed to return a least-cost path from start to goal.\nTypical implementations of A* use a\nto perform the repeated selection of minimum (estimated) cost nodes to expand. This priority queue is known as the\n. At each step of the algorithm, the node with the lowest\nvalue is removed from the queue, the\nvalues of its neighbors are updated accordingly, and these neighbors are added to the queue. The algorithm continues until a removed node (thus the node with the lowest\nvalue out of all fringe nodes) is a goal node.\nvalue of that goal is then also the cost of the shortest path, since\nat the goal is zero in an admissible heuristic.\nThe algorithm described so far only gives the length of the shortest path. To find the actual sequence of steps, the algorithm can be easily revised so that each node on the path keeps track of its predecessor. After this algorithm is run, the ending node will point to its predecessor, and so on, until some node's predecessor is the start node.\nAs an example, when searching for the shortest route on a map,\nstraight-line distance\nto the goal, since that is physically the smallest possible distance between any two points. For a grid map from a video game, using the\nbecomes better depending on the set of movements available (4-way or 8-way).\nsatisfies the additional condition\nh ( x ) ≤ d ( x , y ) + h ( y )\ndenotes the length of that edge), then\nmonotone, or consistent\n. With a consistent heuristic, A* is guaranteed to find an optimal path without processing any node more than once and A* is equivalent to running\nDijkstra's algorithm\nreduced cost d' ( x , y ) = d ( x , y ) + h ( y ) − h ( x )\ndescribes the algorithm:\nfunction reconstruct_path ( cameFrom , current ) total_path := {current} while current in cameFrom . Keys : current := cameFrom [ current ] total_path . prepend ( current ) return total_path // A* finds a path from start to goal. // h is the heuristic function. h(n) estimates the cost to reach goal from node n. function A_Star ( start , goal , h ) // The set of discovered nodes that may need to be (re-)expanded. // Initially, only the start node is known. // This is usually implemented as a min-heap or priority queue rather than a hash-set. openSet := {start} // For node n, cameFrom[n] is the node immediately preceding it on the cheapest path from the start // to n currently known. cameFrom := an empty map // For node n, gScore[n] is the currently known cost of the cheapest path from start to n. gScore := map with default value of Infinity gScore [ start ] := 0 // For node n, fScore[n] := gScore[n] + h(n). fScore[n] represents our current best guess as to // how cheap a path could be from start to finish if it goes through n. fScore := map with default value of Infinity fScore [ start ] := h ( start ) while openSet is not empty // This operation can occur in O(Log(N)) time if openSet is a min-heap or a priority queue current := the node in openSet having the lowest fScore [] value if current = goal return reconstruct_path ( cameFrom , current ) openSet . Remove ( current ) for each neighbor of current // d(current,neighbor) is the weight of the edge from current to neighbor // tentative_gScore is the distance from start to the neighbor through current tentative_gScore := gScore [ current ] + d ( current , neighbor ) if tentative_gScore \u003c gScore [ neighbor ] // This path to neighbor is better than any previous one. Record it! cameFrom [ neighbor ] := current gScore [ neighbor ] := tentative_gScore fScore [ neighbor ] := tentative_gScore + h ( neighbor ) if neighbor not in openSet openSet . add ( neighbor ) // Open set is empty but goal was never reached return failure Remark:\nIn this pseudocode, if a node is reached by one path, removed from openSet, and subsequently reached by a cheaper path, it will be added to openSet again. This is essential to guarantee that the path returned is optimal if the heuristic function is\n. If the heuristic is consistent, when a node is removed from openSet the path to it is guaranteed to be optimal so the test ‘\ntentative_gScore \u003c gScore[neighbor]\n’ will always fail if the node is reached again. The pseudocode implemented here is sometimes called the\nThis is in contrast with the version without the ‘\ntentative_gScore \u003c gScore[neighbor]\n’ test to add nodes back to openSet, which is sometimes called the\nversion of A* and require a consistent heuristic to guarantee optimality.\nIllustration of A* search for finding path from a start node to a goal node in a robot motion planning problem. The empty circles represent the nodes in the open set , i.e., those that remain to be explored, and the filled ones are in the closed set. Color on each closed node indicates the distance from the goal: the greener, the closer. One can first see the A* moving in a straight line in the direction of the goal, then when hitting the obstacle, it explores alternative routes through the nodes from the open set.\nDijkstra's algorithm\nAn example of an A* algorithm in action where nodes are cities connected with roads and h(x) is the straight-line distance to the target point:\ngreen: start; blue: goal; orange: visited\nThe A* algorithm has real-world applications. In this example, edges are railroads and h(x) is the\ngreat-circle distance\n(the shortest possible distance on a sphere) to the target. The algorithm is searching for a path between Washington, D.C., and Los Angeles.\nImplementation details\nThere are a number of simple optimizations or implementation details that can significantly affect the performance of an A* implementation. The first detail to note is that the way the priority queue handles ties can have a significant effect on performance in some situations. If ties are broken so the queue behaves in a\nmanner, A* will behave like\namong equal cost paths (avoiding exploring more than one equally optimal solution).\nWhen a path is required at the end of the search, it is common to keep with each node a reference to that node's parent. At the end of the search, these references can be used to recover the optimal path. If these references are being kept then it can be important that the same node doesn't appear in the priority queue more than once (each entry corresponding to a different path to the node, and each with a different cost). A standard approach here is to check if a node about to be added already appears in the priority queue. If it does, then the priority and parent pointers are changed to correspond to the lower-cost path. A standard\nbased priority queue does not directly support the operation of searching for one of its elements, but it can be augmented with a\nthat maps elements to their position in the heap, allowing this decrease-priority operation to be performed in logarithmic time. Alternatively, a\ncan perform the same decrease-priority operations in constant\n[ edit ] Dijkstra's algorithm\n, as another example of a uniform-cost search algorithm, can be viewed as a special case of A* where\n⁠ h ( x ) = 0 {\\displaystyle h(x)=0} ⁠\ncan be implemented using A* by considering that there is a global counter\ninitialized with a very large value. Every time we process a node we assign\nto all of its newly discovered neighbors. After every single assignment, we decrease the counter\nby one. Thus the earlier a node is discovered, the higher its\n⁠ h ( x ) {\\displaystyle h(x)} ⁠\nvalue. Both Dijkstra's algorithm and depth-first search can be implemented more efficiently without including an\n⁠ h ( x ) {\\displaystyle h(x)} ⁠\nTermination and completeness\nOn finite graphs with non-negative edge weights A* is guaranteed to terminate and is\n, i.e. it will always find a solution (a path from start to goal) if one exists. On infinite graphs with a finite branching factor and edge costs that are bounded away from zero (\nd ( x , y ) \u003e ε \u003e 0 {\\textstyle d(x,y)\u003e\\varepsilon \u003e0}\nε {\\displaystyle \\varepsilon }\n), A* is guaranteed to terminate only if there exists a solution.\nA search algorithm is said to be\nif it is guaranteed to return an optimal solution. If the heuristic function used by A* is\n, then A* is admissible. An intuitive \"proof\" of this is as follows:\nif it has been visited and is not in the open set. We\na node when we remove it from the open set. A basic property of the A* algorithm, which we'll sketch a proof of below, is that when\n⁠ n {\\displaystyle n} ⁠\n⁠ f ( n ) {\\displaystyle f(n)} ⁠\nis an optimistic estimate (lower bound) of the true distance from the start to the goal. So when the goal node,\n⁠ g {\\displaystyle g} ⁠\n⁠ f ( g ) {\\displaystyle f(g)} ⁠\nis no more than the true distance. On the other hand, it is no less than the true distance, since it is the length of a path to the goal plus a heuristic term.\nNow we'll see that whenever a node\n⁠ n {\\displaystyle n} ⁠\n⁠ f ( n ) {\\displaystyle f(n)} ⁠\nis an optimistic estimate. It is enough to see that whenever the open set is not empty, it has at least one node\n⁠ n {\\displaystyle n} ⁠\non an optimal path to the goal for which\n⁠ g ( n ) {\\displaystyle g(n)} ⁠\nis the true distance from start, since in that case\n⁠ g ( n ) {\\displaystyle g(n)} ⁠\n⁠ h ( n ) {\\displaystyle h(n)} ⁠\nunderestimates the distance to goal, and therefore so does the smaller value chosen for the closed vertex. Let\n⁠ P {\\displaystyle P} ⁠\nbe an optimal path from the start to the goal. Let\n⁠ p {\\displaystyle p} ⁠\nbe the last closed node on\n⁠ P {\\displaystyle P} ⁠\n⁠ g ( p ) {\\displaystyle g(p)} ⁠\nis the true distance from the start to the goal (the start is one such vertex). The next node in\n⁠ P {\\displaystyle P} ⁠\n⁠ g {\\displaystyle g} ⁠\nvalue, since it was updated when\n⁠ p {\\displaystyle p} ⁠\nwas closed, and it is open since it is not closed.\nOptimality and consistency\nAlgorithm A is optimally efficient with respect to a set of alternative algorithms\non a set of problems\nif for every problem P in\nand every algorithm A′ in\n, the set of nodes expanded by A in solving P is a subset (possibly equal) of the set of nodes expanded by A′ in solving P. The definitive study of the optimal efficiency of A* is due to Rina Dechter and Judea Pearl.\nThey considered a variety of definitions of\nin combination with A*'s heuristic being merely admissible or being both\nand admissible. The most interesting positive result they proved is that A*, with a consistent heuristic, is optimally efficient with respect to all admissible A*-like search algorithms on all \"non-pathological\" search problems. Roughly speaking, their notion of the non-pathological problem is what we now mean by \"up to tie-breaking\". This result does not hold if A*'s heuristic is admissible but not consistent. In that case, Dechter and Pearl showed there exist admissible A*-like algorithms that can expand arbitrarily fewer nodes than A* on some non-pathological problems.\nOptimal efficiency is about the\nof nodes expanded, not the\nof node expansions (the number of iterations of A*'s main loop). When the heuristic being used is admissible but not consistent, it is possible for a node to be expanded by A* many times, an exponential number of times in the worst case.\nIn such circumstances, Dijkstra's algorithm could outperform A* by a large margin. However, more recent research found that this pathological case only occurs in certain contrived situations where the edge weight of the search graph is exponential in the size of the graph and that certain inconsistent (but admissible) heuristics can lead to a reduced number of node expansions in A* searches.\n[ edit ] A* search that uses a heuristic that is 5.0(=ε) times a consistent heuristic , and obtains a suboptimal path\nWhile the admissibility criterion guarantees an optimal solution path, it also means that A* must examine all equally meritorious paths to find the optimal path. To compute approximate shortest paths, it is possible to speed up the search at the expense of optimality by relaxing the admissibility criterion. Oftentimes we want to bound this relaxation, so that we can guarantee that the solution path is no worse than (1 +\n) times the optimal solution path. This new guarantee is referred to as\nThere are a number of\n-admissible algorithms:\nWeighted A*/Static Weighting's.\n) is an admissible heuristic function, in the weighted version of the A* search one uses\nh w ( n ) = ε h a ( n )\nas the heuristic function, and perform the A* search as usual (which eventually happens faster than using\nsince fewer nodes are expanded). The path hence found by the search algorithm can have a cost of at most\ntimes that of the least cost path in the graph.\nConvex Upward/Downward Parabola (XUP/XDP).\nModification to the cost function in weighted A* to push optimality toward the start or goal. XDP gives paths which are near optimal close to the start, and XUP paths are near-optimal close to the goal. Both yield\nϵ {\\displaystyle \\epsilon }\n-optimal paths overall.\nf XDP ( n ) = 1 2 ϵ [ g ( n ) + ( 2 ϵ − 1 ) + ( g ( n ) − h ( n ) ) 2 + 4 ϵ g ( n ) h ( n ) ] {\\displaystyle f_{\\text{XDP}}(n)={\\frac {1}{2\\epsilon }}\\left[\\ g(n)+(2\\epsilon -1)+{\\sqrt {(g(n)-h(n))^{2}+4\\epsilon g(n)h(n)}}\\ \\right]}\nf XUP ( n ) = 1 2 ϵ [ g ( n ) + h ( n ) + ( g ( n ) + h ( n ) ) 2 + 4 ϵ ( ϵ − 1 ) h ( n ) 2 ] {\\displaystyle f_{\\text{XUP}}(n)={\\frac {1}{2\\epsilon }}\\left[\\ g(n)+h(n)+{\\sqrt {(g(n)+h(n))^{2}+4\\epsilon (\\epsilon -1)h(n)^{2}}}\\ \\right]}\nPiecewise Upward/Downward Curve (pwXU/pwXD).\nSimilar to XUP/XDP but with piecewise functions instead of parabola. Solution paths are also\nϵ {\\displaystyle \\epsilon }\nf pwXD ( n ) = { g ( n ) + h ( n ) , if h ( n ) \u003e g ( n ) g ( n ) + ( 2 ϵ − 1 ) h ( n ) / ϵ , if h ( n ) ≤ g ( n ) {\\displaystyle f_{\\text{pwXD}}(n)={\\begin{cases}g(n)+h(n),\u0026{\\text{if }}h(n)\u003eg(n)\\\\g(n)+(2\\epsilon -1)h(n)/\\epsilon ,\u0026{\\text{if }}h(n)\\leq g(n)\\end{cases}}} f pwXU ( n ) = { g ( n ) / ( 2 ϵ − 1 ) + h ( n ) , if g ( n ) \u003c ( 2 ϵ − 1 ) h ( n ) ( g ( n ) + h ( n ) ) / ϵ , if g ( n ) ≥ ( 2 ϵ − 1 ) h ( n ) {\\displaystyle f_{\\text{pwXU}}(n)={\\begin{cases}g(n)/(2\\epsilon -1)+h(n),\u0026{\\text{if }}g(n)\u003c(2\\epsilon -1)h(n)\\\\(g(n)+h(n))/\\epsilon ,\u0026{\\text{if }}g(n)\\geq (2\\epsilon -1)h(n)\\end{cases}}}\nuses the cost function\n⁠ f ( n ) = g ( n ) + ( 1 + ε w ( n ) ) h ( n ) {\\displaystyle f(n)=g(n)+(1+\\varepsilon w(n))h(n)} ⁠\nw ( n ) = { 1 − d ( n ) N d ( n ) ≤ N 0 otherwise {\\displaystyle w(n)={\\begin{cases}1-{\\frac {d(n)}{N}}\u0026d(n)\\leq N\\\\0\u0026{\\text{otherwise}}\\end{cases}}}\n⁠ d ( n ) {\\displaystyle d(n)} ⁠\nis the depth of the search and\nis the anticipated length of the solution path.\nSampled Dynamic Weighting\nuses sampling of nodes to better estimate and debias the heuristic error.\nA ε ∗ {\\displaystyle A_{\\varepsilon }^{*}}\nuses two heuristic functions. The first is the FOCAL list, which is used to select candidate nodes, and the second\nis used to select the most promising node from the FOCAL list.\nselects nodes with the function\n⁠ A f ( n ) + B h F ( n ) {\\displaystyle Af(n)+Bh_{F}(n)} ⁠\nare constants. If no nodes can be selected, the algorithm will backtrack with the function\n⁠ C f ( n ) + D h F ( n ) {\\displaystyle Cf(n)+Dh_{F}(n)} ⁠\nattempts to promote depth-first exploitation by preferring recently expanded nodes. AlphA* uses the cost function\nf α ( n ) = ( 1 + w α ( n ) ) f ( n ) {\\displaystyle f_{\\alpha }(n)=(1+w_{\\alpha }(n))f(n)}\nw α ( n ) = { λ g ( π ( n ) ) ≤ g ( n ~ ) Λ otherwise {\\displaystyle w_{\\alpha }(n)={\\begin{cases}\\lambda \u0026g(\\pi (n))\\leq g({\\tilde {n}})\\\\\\Lambda \u0026{\\text{otherwise}}\\end{cases}}}\nλ ≤ Λ {\\displaystyle \\lambda \\leq \\Lambda }\nis the most recently expanded node.\nAs a heuristic search algorithm, the performance of A* is heavily influenced by the quality of the heuristic function\nh ( n ) {\\textstyle h(n)}\n. If the heuristic closely approximates the true cost to the goal, A* can significantly reduce the number of node expansions. On the other hand, a poor heuristic can lead to many unnecessary expansions.\nIn the worst case, A* expands all nodes\nf ( n ) = g ( n ) + h ( n ) ≤ C ∗ {\\textstyle f(n)=g(n)+h(n)\\leq C^{*}}\nC ∗ {\\textstyle C^{*}}\nis the cost of the optimal goal node.\nWhy can’t it be worse\nSuppose there is a node\nN ′ {\\textstyle N'}\nin the open list with\nf ( N ′ ) \u003e C ∗ {\\textstyle f(N')\u003eC^{*}}\n, and it's the next node to be expanded. Since the goal node has\nf ( g o a l ) = g ( g o a l ) + h ( g o a l ) = g ( g o a l ) = C ∗ {\\textstyle f(goal)=g(goal)+h(goal)=g(goal)=C^{*}}\nf ( N ′ ) \u003e C ∗ {\\textstyle f(N')\u003eC^{*}}\n, the goal node will have a lower f-value and will be expanded before\nN ′ {\\textstyle N'}\n. Therefore, A* never expands nodes with\nf ( n ) \u003e C ∗ {\\textstyle f(n)\u003eC^{*}}\nWhy can’t it be better\nAssume there exists an optimal algorithm that expands fewer nodes than\nC ∗ {\\textstyle C^{*}}\nin the worst case using the same heuristic. That means there must be some node\nN ′ {\\textstyle N'}\nf ( N ′ ) \u003c C ∗ {\\textstyle f(N')\u003cC^{*}}\n, yet the algorithm chooses not to expand it.\nNow consider a modified graph where a new edge of cost\nε {\\textstyle \\varepsilon }\nε \u003e 0 {\\textstyle \\varepsilon \u003e0}\nN ′ {\\textstyle N'}\nf ( N ′ ) + ε \u003c C ∗ {\\textstyle f(N')+\\varepsilon \u003cC^{*}}\n, then the new optimal path goes through\nN ′ {\\textstyle N'}\n. However, since the algorithm still avoids expanding\nN ′ {\\textstyle N'}\n, it will miss the new optimal path, violating its optimality.\nTherefore, no optimal algorithm including A* could expand fewer nodes than\nC ∗ {\\textstyle C^{*}}\nMathematical Notation\nThe worst-case complexity of A* is often described as\nO ( b d ) {\\textstyle O(b^{d})}\nis the branching factor and\nis the depth of the shallowest goal. While this gives a rough intuition, it does not precisely capture the actual behavior of A*.\nA more accurate bound considers the number of nodes with\nf ( n ) ≤ C ∗ {\\textstyle f(n)\\leq C^{*}}\nε {\\displaystyle \\varepsilon }\nis the smallest possible difference in\n-cost between distinct nodes, then A* may expand up to:\nO ( C ∗ ε ) {\\displaystyle O\\left({\\frac {C^{*}}{\\varepsilon }}\\right)}\nThis represents both the time and space complexity in the worst case.\nof A* is roughly the same as that of all other graph search algorithms, as it keeps all generated nodes in memory.\nIn practice, this turns out to be the biggest drawback of the A* search, leading to the development of memory-bounded heuristic searches, such as\nIterative deepening A*\n, memory-bounded A*, and\nA* is often used for the common\nproblem in applications such as video games, but was originally designed as a general graph traversal algorithm.\nIt finds applications in diverse problems, including the problem of\nOther cases include an Informational search with online learning.\nRelations to other algorithms\nWhat sets A* apart from a\nbest-first search algorithm is that it takes the cost/distance already traveled,\nSome common variants of\nDijkstra's algorithm\ncan be viewed as a special case of A* where the heuristic\nh ( n ) = 0 {\\displaystyle h(n)=0}\nin turn, both Dijkstra and A* are special cases of\nA* itself is a special case of a generalization of\nexcept that beam search maintains a limit on the numbers of paths that it has to explore.\n[ edit ] Anytime A* [ 32 ] Block A* D* Field D* Fringe Fringe Saving A* (FSA*) Generalized Adaptive A* (GAA*) Incremental heuristic search\n[ 33 ] Iterative deepening A* (IDA*) Jump point search Lifelong Planning A* (LPA*)\nNew Bidirectional A* (NBA*)\n[ 34 ] Simplified Memory bounded A* (SMA*) Theta*\nA* can also be adapted to a\nbidirectional search\nalgorithm, but special care needs to be taken for the stopping criterion.\n[ edit ] Any-angle path planning\n, search for paths that are not limited to moving along graph edges but rather can take on any angle\nBreadth-first search Depth-first search Dijkstra's algorithm\n– Algorithm for finding shortest paths\n[ edit ] ^ \"A*-like\" means the algorithm searches by extending paths originating at the start node one edge at a time, just as A* does. This excludes, for example, algorithms that search backward from the goal or in both directions simultaneously. In addition, the algorithms covered by this theorem must be admissible, and \"not more informed\" than A*. ^ Goal nodes may be passed over multiple times if there remain other nodes with lower f values, as they may lead to a shorter path to a goal.\n[ edit ] ^ a b c Russell, Stuart J. ; Norvig, Peter (2018). Artificial intelligence a modern approach (4th ed.). Boston: Pearson. ISBN 978-0134610993 . OCLC 1021874142 . ^ Delling, D.; Sanders, P. ; Schultes, D.; Wagner, D. (2009). \"Engineering Route Planning Algorithms\". Algorithmics of Large and Complex Networks: Design, Analysis, and Simulation . Lecture Notes in Computer Science. Vol. 5515. Springer. pp. 117– 139. doi : 10.1007/978-3-642-02094-0_7 . ISBN 978-3-642-02093-3 . ^ Zeng, W.; Church, R. L. (2009). \"Finding shortest paths on real road networks: the case for A*\" . International Journal of Geographical Information Science . 23 (4): 531– 543. Bibcode : 2009IJGIS..23..531Z . doi : 10.1080/13658810801949850 . S2CID 14833639 . ^ a b c Hart, P. E. ; Nilsson, N.J. ; Raphael, B. (1968). \"A Formal Basis for the Heuristic Determination of Minimum Cost Paths\". IEEE Transactions on Systems Science and Cybernetics . 4 (2): 100– 7. doi : 10.1109/TSSC.1968.300136 . ^ Doran, J. E.; Michie, D. (1966-09-20). \"Experiments with the Graph Traverser program\". Proc. R. Soc. Lond. A . 294 (1437): 235– 259. Bibcode : 1966RSPSA.294..235D . doi : 10.1098/rspa.1966.0205 . S2CID 21698093 . ^ Nilsson, Nils J. (2009-10-30). The Quest for Artificial Intelligence (PDF) . Cambridge: Cambridge University Press. ISBN 9780521122931 . One of the first problems we considered was how to plan a sequence of 'way points' that Shakey could use in navigating from place to place. […] Shakey's navigation problem is a search problem, similar to ones I have mentioned earlier. ^ Nilsson, Nils J. (2009-10-30). The Quest for Artificial Intelligence (PDF) . Cambridge: Cambridge University Press. ISBN 9780521122931 . Bertram Raphael, who was directing work on Shakey at that time, observed that a better value for the score would be the sum of the distance traveled so far from the initial position plus my heuristic estimate of how far the robot had to go. ^ Edelkamp, Stefan; Jabbar, Shahid; Lluch-Lafuente, Alberto (2005). \"Cost-Algebraic Heuristic Search\". Proceedings of the Twentieth National Conference on Artificial Intelligence (AAAI) (PDF) . pp. 1362– 1367. ISBN 978-1-57735-236-5 . ^ Hart, Peter E.; Nilsson, Nils J. ; Raphael, Bertram (1972-12-01). \"Correction to 'A Formal Basis for the Heuristic Determination of Minimum Cost Paths' \" (PDF) . ACM SIGART Bulletin (37): 28– 29. doi : 10.1145/1056777.1056779 . S2CID 6386648 . ^ a b Dechter, Rina; Judea Pearl (1985). \"Generalized best-first search strategies and the optimality of A*\" . Journal of the ACM . 32 (3): 505– 536. doi : 10.1145/3828.3830 . S2CID 2092415 . ^ Nannicini, Giacomo; Delling, Daniel; Schultes, Dominik; Liberti, Leo (2012). \"Bidirectional A* search on time-dependent road networks\" (PDF) . Networks . 59 (2): 240– 251. doi : 10.1002/NET.20438 . ^ Russell, Stuart J. ; Norvig, Peter (2009). Artificial intelligence: A modern approach (3rd ed.). Boston: Pearson. p. 95. ISBN 978-0136042594 . ^ a b De Smith, Michael John; Goodchild, Michael F.; Longley, Paul (2007), Geospatial Analysis: A Comprehensive Guide to Principles, Techniques and Software Tools , Troubadour Publishing Ltd, p. 344, ISBN 9781905886609 . ^ a b Hetland, Magnus Lie (2010), Python Algorithms: Mastering Basic Algorithms in the Python Language , Apress, p. 214, ISBN 9781430232377 , archived from the original on 15 February 2022 . ^ Martelli, Alberto (1977). \"On the Complexity of Admissible Search Algorithms\". Artificial Intelligence . 8 (1): 1– 13. doi : 10.1016/0004-3702(77)90002-9 . ^ Felner, Ariel; Uzi Zahavi (2011). \"Inconsistent heuristics in theory and practice\" . Artificial Intelligence . 175 ( 9– 10): 1570– 1603. doi : 10.1016/j.artint.2011.02.001 . ^ Zhang, Zhifu; N. R. Sturtevant (2009). Using Inconsistent Heuristics on A* Search . Twenty-First International Joint Conference on Artificial Intelligence. pp. 634– 639. ^ Pohl, Ira (1970). \"First results on the effect of error in heuristic search\". Machine Intelligence 5 . Edinburgh University Press: 219– 236. ISBN 978-0-85224-176-9 . OCLC 1067280266 . ^ Pearl, Judea (1984). Heuristics: Intelligent Search Strategies for Computer Problem Solving . Addison-Wesley. ISBN 978-0-201-05594-8 . ^ Chen, Jingwei; Sturtevant, Nathan R. (2019). \"Conditions for Avoiding Node Re-expansions in Bounded Suboptimal Search\" . Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence . International Joint Conferences on Artificial Intelligence Organization: 1220– 1226. ^ Chen, Jingwei; Sturtevant, Nathan R. (2021-05-18). \"Necessary and Sufficient Conditions for Avoiding Reopenings in Best First Suboptimal Search with General Bounding Functions\" . Proceedings of the AAAI Conference on Artificial Intelligence . 35 (5): 3688– 3696. doi : 10.1609/aaai.v35i5.16485 . ISSN 2374-3468 . ^ Pohl, Ira (August 1973). \"The avoidance of (relative) catastrophe, heuristic competence, genuine dynamic weighting and computational issues in heuristic problem solving\" (PDF) . Proceedings of the Third International Joint Conference on Artificial Intelligence (IJCAI-73) . Vol. 3. California, USA. pp. 11– 17. ^ Köll, Andreas; Hermann Kaindl (August 1992). \"A new approach to dynamic weighting\" . Proceedings of the Tenth European Conference on Artificial Intelligence (ECAI-92) . Vienna, Austria: Wiley. pp. 16– 17. ISBN 978-0-471-93608-4 . ^ Pearl, Judea; Jin H. Kim (1982). \"Studies in semi-admissible heuristics\". IEEE Transactions on Pattern Analysis and Machine Intelligence . 4 (4): 392– 399. doi : 10.1109/TPAMI.1982.4767270 . PMID 21869053 . S2CID 3176931 . ^ Ghallab, Malik; Dennis Allard (August 1983). \" A ε – an efficient near admissible heuristic search algorithm\" (PDF) . Proceedings of the Eighth International Joint Conference on Artificial Intelligence (IJCAI-83) . Vol. 2. Karlsruhe, Germany. pp. 789– 791. Archived from the original (PDF) on 2014-08-06. ^ Reese, Bjørn (1999). AlphA*: An ε -admissible heuristic search algorithm (Report). Institute for Production Technology, University of Southern Denmark. Archived from the original on 2016-01-31 . Retrieved 2014-11-05 . ^ Klein, Dan; Manning, Christopher D. (2003). \"A* parsing: fast exact Viterbi parse selection\" (PDF) . Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics . pp. 119– 126. doi : 10.3115/1073445.1073461 . ^ Kagan E.; Ben-Gal I. (2014). \"A Group-Testing Algorithm with Online Informational Learning\" (PDF) . IIE Transactions . 46 (2): 164– 184. doi : 10.1080/0740817X.2013.803639 . S2CID 18588494 . Archived from the original (PDF) on 2016-11-05 . Retrieved 2016-02-12 . ^ Ferguson, Dave; Likhachev, Maxim; Stentz, Anthony (2005). \"A Guide to Heuristic-based Path Planning\" (PDF) . Proceedings of the international workshop on planning under uncertainty for autonomous systems, international conference on automated planning and scheduling (ICAPS) . pp. 9– 18. Archived (PDF) from the original on 2016-06-29. ^ Nau, Dana S.; Kumar, Vipin; Kanal, Laveen (1984). \"General branch and bound, and its relation to A∗ and AO∗\" (PDF) . Artificial Intelligence . 23 (1): 29– 58. doi : 10.1016/0004-3702(84)90004-3 . Archived (PDF) from the original on 2012-10-04. ^ \"Variants of A*\" . theory.stanford.edu . Retrieved 2023-06-09 . ^ Hansen, Eric A.; Zhou, Rong (2007). \"Anytime Heuristic Search\" . Journal of Artificial Intelligence Research . 28 : 267– 297. arXiv : 1110.2737 . doi : 10.1613/jair.2096 . S2CID 9832874 . ^ Fareh, Raouf; Baziyad, Mohammed; Rahman, Mohammad H.; Rabie, Tamer; Bettayeb, Maamar (2019-05-14). \"Investigating Reduced Path Planning Strategy for Differential Wheeled Mobile Robot\" . Robotica . 38 (2): 235– 255. doi : 10.1017/S0263574719000572 . ISSN 0263-5747 . S2CID 181849209 . ^ Pijls, Wim; Post, Henk. Yet another bidirectional algorithm for shortest paths (PDF) (Technical report). Econometric Institute, Erasmus University Rotterdam. EI 2009-10. Archived (PDF) from the original on 2014-06-11. ^ Goldberg, Andrew V.; Harrelson, Chris; Kaplan, Haim; Werneck, Renato F. \"Efficient Point-to-Point Shortest Path Algorithms\" (PDF) . Princeton University . Archived (PDF) from the original on 18 May 2022.\n[ edit ] Nilsson, N. J. (1980). Principles of Artificial Intelligence . Palo Alto, California: Tioga Publishing Company. ISBN 978-0-935382-01-3 .\nVariation on A* called\nHierarchical Path-Finding A* (HPA*) Brian Grinstead. \"A* Search Algorithm in JavaScript (Updated)\" . Archived from the original on 15 February 2020 . Retrieved 8 February 2021 . v t e Graph\ntraversal algorithms\nSearch α–β pruning A* IDA* LPA* SMA* Best-first search Beam search Bidirectional search Breadth-first search Lexicographic Parallel B* Depth-first search Iterative deepening D* Fringe search Jump point search Monte Carlo tree search SSS* Shortest path Bellman–Ford Dijkstra's Floyd–Warshall Johnson's Shortest path faster Yen's Minimum spanning tree Borůvka's Kruskal's Prim's Reverse-delete List of graph search algorithms\nhttps://en.wikipedia.org/w/index.php?title=A*_search_algorithm\u0026oldid=1296432332\nGraph algorithms Routing algorithms Search algorithms Heuristic algorithms Combinatorial optimization Game artificial intelligence Greedy algorithms Graph distance\nArticles with short description Short description matches Wikidata Articles with example pseudocode Search Search A* search algorithm 30 languages Add topic",
          "role": "teacher"
        },
        {
          "content": "Why is this important to understand?",
          "role": "student"
        },
        {
          "content": "Understanding A* Search is crucial for building efficient, scalable, and maintainable software systems. It forms the foundation for solving complex computational problems.",
          "role": "teacher"
        },
        {
          "content": "Can you give me a concrete example?",
          "role": "student"
        },
        {
          "content": "For example, A* Search can be applied in scenarios where you need to optimize performance, manage complexity, or implement scalable solutions.",
          "role": "teacher"
        }
      ],
      "format": "socratic_dialogue",
      "metadata": {
        "domain": "Algorithms",
        "source": "https://en.wikipedia.org/wiki/A*_search_algorithm",
        "words": 6444
      },
      "topic": "A* Search"
    },
    {
      "conversation": [
        {
          "content": "How do I implement A* Search?",
          "role": "user"
        },
        {
          "content": "Here's how to implement A* Search:\n\n1. Understand the core algorithm:\n• ) first published the algorithm in 1968.\n• . At each step of the algorithm, the node with the lowest\n• value of that goal is then also the cost of the shortest path, since\n• denotes the length of that edge), then\n• value. Both Dijkstra's algorithm and depth-first search can be implemented more efficiently without including an\n\n2. Choose appropriate data structures\n3. Handle edge cases and validation\n4. Optimize for your use case\n5. Test thoroughly with various inputs\n\nBasic structure:\n```\nfunction implementA*Search(input) {\n    // Initialize\n    // Process\n    // Return result\n}\n```",
          "role": "assistant"
        }
      ],
      "format": "implementation_guide",
      "metadata": {
        "domain": "Algorithms",
        "type": "code_focused"
      },
      "topic": "A* Search"
    },
    {
      "content": {
        "applications": "applications. In this example, edges are railroads and h(x) is the\ngreat-circle distance\n(the shortest possible distance on a sphere) to the target. The algorithm is searching for a path between Washington, D.C., and Los Angeles.\nImplementation details\nThere are a number of simple optimizations or implementation details that can significantly affect the performance of an A* implementation. The first detail to note is that the way the priority queue handles ties can have a significant effect on p",
        "considerations": "considering that there is a global counter\ninitialized with a very large value. Every time we process a node we assign\nto all of its newly discovered neighbors. After every single assignment, we decrease the counter\nby one. Thus the earlier a node is discovered, the higher its\n⁠ h ( x ) {\\displaystyle h(x)} ⁠\nvalue. Both Dijkstra's algorithm and depth-first search can be implemented more effic",
        "key_concepts": [
          "admissible algorithms:",
          "optimal paths overall.",
          "cost between distinct nodes, then A* may expand up to:"
        ],
        "overview": "A comprehensive technical concept with wide applications."
      },
      "format": "conceptual_breakdown",
      "metadata": {
        "domain": "Algorithms",
        "structure": "hierarchical"
      },
      "topic": "A* Search"
    },
    {
      "format": "problem_solving",
      "scenario": {
        "approach": "solving P is a subset (possibly equal) of the set of nodes expanded by A′ in solving P. The definitive study of the optimal efficiency of A* is due to Rina Dechter and Judea Pearl.\nThey considered a variety of definitions of\nin combination with A*'s heuristic being merely admissible or being both\nand admissible. The most interesting positive result they proved is that A*, with a consistent heuristic, is optimally efficient with respect to all admissible A*-like search algorithms on all \"non-pa",
        "benefits": "efficiently without including an\n⁠ h ( x ) {\\displaystyle h(x)} ⁠\nTermination and completeness\nOn finite graphs with non-negative edge weights A* is guaranteed to terminate and is\n, i.e. it will always find a solution (a path from start to goal) if one exists. On infinite graphs with a finite br",
        "problem": "How can A* Search solve real-world problems?",
        "tradeoffs": "cost paths when the cost of a path is the sum of its costs, but it has been shown that A* can be used to find optimal paths for any problem satisfying the conditions of a cost algebra.\nThe original 1968 A* paper\ncontained a theorem stating that no A*-like algorithm\ncould expand fewer nodes than A* i"
      },
      "topic": "A* Search"
    },
    {
      "comparison": "A* Search offers unique advantages compared to alternative approaches, particularly in terms of efficiency and scalability.",
      "format": "comparative_analysis",
      "topic": "A* Search"
    },
    {
      "dialogue": [
        {
          "content": "What is Blockchain really about at its core?",
          "role": "student"
        },
        {
          "content": "Distributed data store for digital transactions\nBlock chain (disambiguation)\nwith growing lists of\n) that are securely linked together via\ncryptographic hashes\n[ 1 ] [ 2 ] [ 3 ] [ 4 ]\nEach block contains a cryptographic hash of the previous block, a\n, and transaction data (generally represented as a\nare represented by leaves). Since each block contains information about the previous block, they effectively form a\ndata structure), with each additional block linking to the ones before it. Consequently, blockchain transactions are resistant to alteration because, once recorded, the data in any given block cannot be changed retroactively without altering all subsequent blocks and obtaining network consensus to accept these changes.\nBlockchains are typically managed by a\ncomputer network for use as a public\n, where nodes collectively adhere to a\nconsensus algorithm protocol\nto add and validate new transaction blocks. Although blockchain records are not unalterable, since\nare possible, blockchains may be considered\nand exemplify a distributed computing system with high\nByzantine fault tolerance\nA blockchain was created by a person (or group of people) using the name (or\nin 2008 to serve as the public\nbitcoin cryptocurrency\ntransactions, based on previous work by\nThe implementation of the blockchain within bitcoin made it the first digital currency to solve the\nproblem without the need for a trusted authority or central\ndesign has inspired other applications\nand blockchains that are readable by the public and are widely used by\n. The blockchain may be considered a type of\nPrivate blockchains have been proposed for business use.\ncalled the marketing of such privatized blockchains without a proper security model \"\nhowever, others have argued that permissioned blockchains, if carefully designed, may be more decentralized and therefore more secure in practice than permissionless ones.\nCryptographer David Chaum\nfirst proposed a blockchain-like protocol in his 1982 dissertation \"Computer Systems Established, Maintained, and Trusted by Mutually Suspicious Groups\".\nFurther work on a cryptographically secured chain of blocks was described in 1991 by\nThey wanted to implement a system wherein document\ncould not be tampered with. In 1992, Haber, Stornetta, and\ninto the design, which improved its efficiency by allowing several document certificates to be collected into one block.\nUnder their company Surety, their document certificate hashes have been published in\nevery week since 1995.\nThe first decentralized blockchain was conceptualized by a person (or group of people) known as\nin 2008. Nakamoto improved the design in an important way using a\nblocks without requiring them to be signed by a trusted party and introducing a difficulty parameter to stabilize the rate at which blocks are added to the chain.\nThe design was implemented the following year by Nakamoto as a core component of the\ncryptocurrency bitcoin\n, where it serves as the public ledger for all transactions on the network.\nIn August 2014, the bitcoin blockchain file size, containing records of all transactions that have occurred on the network, reached 20 GB (\nIn January 2015, the size had grown to almost 30 GB, and from January 2016 to January 2017, the bitcoin blockchain grew from 50 GB to 100 GB in size. The ledger size had exceeded 200 GB by early 2020.\nwere used separately in Satoshi Nakamoto's original paper, but were eventually popularized as a single word,\n, an application of the\ndiffusion of innovations\ntheory suggests that blockchains attained a 13.5% adoption rate within financial services in 2016, therefore reaching the\nIndustry trade groups joined to create the Global Blockchain Forum in 2016, an initiative of the\nChamber of Digital Commerce\nfound that only 1% of\nindicated any kind of blockchain adoption within their organisations, and only 8% of CIOs were in the short-term \"planning or [looking at] active experimentation with blockchain\".\nFor the year 2019 Gartner reported 5% of CIOs believed blockchain technology was a 'game-changer' for their business.\nStructure and design\nBlockchain formation. The main chain (black) consists of the longest series of blocks from the genesis block (green) to the current block. Orphan blocks (purple) exist outside of the main chain.\n, and often public, digital ledger consisting of records called\nthat are used to record transactions across many computers so that any involved block cannot be altered retroactively, without the alteration of all subsequent blocks.\nThis allows the participants to verify and audit transactions independently and relatively inexpensively.\nA blockchain database is managed autonomously using a\nnetwork and a distributed timestamping server. They are\ncollective self-interests\nSuch a design facilitates\nwhere participants' uncertainty regarding data security is marginal. The use of a blockchain removes the characteristic of infinite\n. It confirms that each unit of value was transferred only once, solving the long-standing problem of\n. A blockchain has been described as a\nvalue-exchange protocol\nA blockchain can maintain\nbecause, when properly set up to detail the exchange agreement, it provides a record that compels\noffer and acceptance\nLogically, a blockchain can be seen as consisting of several layers:\ninfrastructure (hardware)\n(node discovery, information propagation\ndata (blocks, transactions)\ndecentralized applications\nBlocks hold batches of valid\nthat are hashed and encoded into a\nEach block includes the\nof the prior block in the blockchain, linking the two. The linked blocks form a chain.\nprocess confirms the integrity of the previous block, all the way back to the initial block, which is known as the\nTo assure the integrity of a block and the data contained in it, the block is usually\nSometimes separate blocks can be produced concurrently, creating a temporary\n. In addition to a secure\nhistory, any blockchain has a specified algorithm for scoring different versions of the history so that one with a higher score can be selected over others. Blocks not selected for inclusion in the chain are called orphan blocks.\nPeers supporting the database have different versions of the history from time to time. They keep only the highest-scoring version of the database known to them. Whenever a peer receives a higher-scoring version (usually the old version with a single new block added) they extend or overwrite their own database and retransmit the improvement to their peers. There is never an absolute guarantee that any particular entry will remain in the best version of history forever. Blockchains are typically built to add the score of new blocks onto old blocks and are given incentives to extend with new blocks rather than overwrite old blocks. Therefore, the probability of an entry becoming superseded decreases exponentially\nas more blocks are built on top of it, eventually becoming very low.\n[ 3 ] [ 30 ] : ch. 08 [ 31 ]\nFor example, bitcoin uses a\nproof-of-work system\n, where the chain with the most cumulative proof-of-work is considered the valid one by the network. There are a number of methods that can be used to demonstrate a sufficient level of\n. Within a blockchain the computation is carried out redundantly rather than in the traditional segregated and\nis the average time it takes for the network to generate one extra block in the blockchain. By the time of block completion, the included data becomes verifiable. In cryptocurrency, this is practically when the transaction takes place, so a shorter block time means faster transactions. The block time for\nis set to between 14 and 15 seconds, while for bitcoin it is on average 10 minutes.\nThis section is an excerpt from\nFork (blockchain) § Hard fork\nis a change to the blockchain protocol that is not backward compatible and requires all users to upgrade their software in order to continue participating in the network. In a hard fork, the network splits into two separate versions: one that follows the new rules and one that follows the old rules.\nFor example, Ethereum was hard forked in 2016 to \"make whole\" the investors in\n, which had been hacked by exploiting a vulnerability in its code. In this case, the fork resulted in a split creating Ethereum and\ncommunity was asked to consider a hard fork that would have led to a rollback of the blockchain records to mitigate the effects of a theft of 50 million NXT from a major\ncryptocurrency exchange\n. The hard fork proposal was rejected, and some of the funds were recovered after negotiations and ransom payment. Alternatively, to prevent a permanent split, a majority of nodes using the new software may return to the old rules, as was the case of bitcoin split on 12 March 2013.\nA more recent hard-fork example is of Bitcoin in 2017, which resulted in a split creating\nThe network split was mainly due to a disagreement in how to increase the transactions per second to accommodate for demand.\nBy storing data across its\npeer-to-peer network\n, the blockchain eliminates some risks that come with data being held centrally.\nThe decentralized blockchain may use\nad hoc message passing\ndistributed networking\nIn a so-called \"51% attack\" a central entity gains control of more than half of a network and can then manipulate that specific blockchain record at will, allowing\nBlockchain security methods include the use of\npublic-key cryptography\n(a long, random-looking string of numbers) is an address on the blockchain. Value tokens sent across the network are recorded as belonging to that address. A\nis like a password that gives its owner access to their digital assets or the means to otherwise interact with the various capabilities that blockchains now support. Data stored on the blockchain is generally considered incorruptible.\nin a decentralized system has a copy of the blockchain.\nis maintained by massive database\n. No centralized \"official\" copy exists and no user is \"trusted\" more than any other.\nTransactions are broadcast to the network using the software. Messages are delivered on a\nbasis. Early blockchains rely on energy-intensive mining nodes to validate transactions,\nadd them to the block they are building, and then\nthe completed block to other nodes.\nBlockchains use various time-stamping schemes, such as\n, to serialize changes.\nLater consensus methods include\nThe growth of a decentralized blockchain is accompanied by the risk of\nbecause the computer resources required to process larger amounts of data become more expensive.\nFinality is the level of confidence that the well-formed block recently appended to the blockchain will not be revoked in the future (is \"finalized\") and thus can be trusted. Most distributed blockchain protocols, whether\n, cannot guarantee the finality of a freshly committed block, and instead rely on \"probabilistic finality\": as the block goes deeper into a blockchain, it is less likely to be altered or reverted by a newly found consensus.\n[ 43 ] Byzantine fault tolerance\n-based proof-of-stake protocols purport to provide so called \"absolute finality\": a randomly chosen\nproposes a block, the rest of validators vote on it, and, if a supermajority decision approves it, the block is irreversibly committed into the blockchain.\nA modification of this method, an \"economic finality\", is used in practical protocols, like the Casper protocol used in\n: validators which sign two different blocks at the same position in the blockchain are subject to \"slashing\", where their leveraged stake is forfeited.\nOpen blockchains are more\nthan some traditional ownership records, which, while open to the public, still require physical access to view. Because all early blockchains were permissionless, controversy has arisen over the blockchain definition. An issue in this ongoing debate is whether a private system with verifiers tasked and authorized (permissioned) by a central authority should be considered a blockchain.\n[ 44 ] [ 45 ] [ 46 ] [ 47 ] [ 48 ]\nProponents of permissioned or private chains argue that the term \"blockchain\" may be applied to any\nthat batches data into time-stamped blocks. These blockchains serve as a distributed version of\nmultiversion concurrency control\n(MVCC) in databases.\nJust as MVCC prevents two transactions from concurrently modifying a single object in a database, blockchains prevent two transactions from spending the same single output in a blockchain.\nOpponents say that permissioned systems resemble traditional corporate databases, not supporting decentralized data verification, and that such systems are not hardened against operator tampering and revision.\nsaid that \"many in-house blockchain solutions will be nothing more than cumbersome databases,\" and \"without a clear security model, proprietary blockchains should be eyed with suspicion.\"\nPermissionless (public) blockchain\nAn advantage to an open, permissionless, or public, blockchain network is that guarding against bad actors is not required and no\nThis means that applications can be added to the network without the approval or trust of others, using the blockchain as a\nBitcoin and other cryptocurrencies currently secure their blockchain by requiring new entries to include proof of work. To prolong the blockchain, bitcoin uses\npuzzles. While Hashcash was designed in 1997 by\n, the original idea was first proposed by\nand Eli Ponyatovski in their 1992 paper \"Pricing via Processing or Combatting Junk Mail\".\ninvestment for blockchain-related projects was weakening in the US but increasing in China.\nBitcoin and many other cryptocurrencies use open (public) blockchains. As of April 2018\n, bitcoin has the highest\nmarket capitalization\nPermissioned (private) blockchain\nPermissioned blockchains use an access control layer to govern who has access to the network.\nIt has been argued that permissioned blockchains can guarantee a certain level of decentralization, if carefully designed, as opposed to permissionless blockchains, which are often centralized in practice.\nDisadvantages of permissioned blockchain\nNikolai Hampton argued in\nthat \"There is also no need for a '51 percent' attack on a private blockchain, as the private blockchain (most likely) already controls 100 percent of all block creation resources. If you could attack or damage the blockchain creation tools on a private corporate server, you could effectively control 100 percent of their network and alter transactions however you wished.\"\nThis has a set of particularly profound adverse implications during a\n2008 financial crisis\n, where politically powerful actors may make decisions that favor some groups at the expense of others,\nand \"the bitcoin blockchain is protected by the massive group mining effort. It's unlikely that any private blockchain will try to protect records using\nof computing power — it's time-consuming and expensive.\"\nHe also said, \"Within a private blockchain there is also no 'race'; there's no incentive to use more power or discover blocks faster than competitors. This means that many in-house blockchain solutions will be nothing more than cumbersome databases.\"\nanalysis of public blockchains\nhas become increasingly important with the popularity of\nA blockchain, if it is public, provides access to anyone to observe and analyse the chain data, given the know-how. The process of understanding and accessing the flow of crypto has been an issue for many cryptocurrencies, crypto exchanges and banks.\nThe reason for this is accusations of blockchain-enabled cryptocurrencies enabling illicit\ntrading of drugs, weapons, money laundering, etc.\nA common belief has been that cryptocurrency is private and untraceable, thus leading many actors to use it for illegal purposes. This is changing now that specialised tech companies provide blockchain tracking services, making crypto exchanges, law-enforcement and banks more aware of what is happening with crypto funds and\n-crypto exchanges. The development, some argue, has led criminals to prioritise the use of new cryptos such as\n[ 59 ] [ 60 ] [ 61 ]\nsubmitted a proposal to the\nInternational Organization for Standardization\nto consider developing standards to support blockchain technology. This proposal resulted in the creation of ISO Technical Committee 307, Blockchain and Distributed Ledger Technologies.\nThe technical committee has working groups relating to blockchain terminology, reference architecture, security and privacy, identity, smart contracts, governance and interoperability for blockchain and DLT, as well as standards specific to industry sectors and generic government requirements.\n[ 63 ] [ non-primary source needed ]\nMore than 50 countries are participating in the standardization process together with external liaisons such as the\nSociety for Worldwide Interbank Financial Telecommunication\nInternational Federation of Surveyors\nInternational Telecommunication Union\nUnited Nations Economic Commission for Europe\nMany other national standards bodies and open standards bodies are also working on blockchain standards.\nNational Institute of Standards and Technology [ 65 ]\nEuropean Committee for Electrotechnical Standardization [ 66 ]\nInstitute of Electrical and Electronics Engineers [ 67 ]\n(IEEE), the Organization for the Advancement of Structured Information Standards (\n), and some individual participants in the\nInternet Engineering Task Force [ 68 ]\nCentralized blockchain\nAlthough most of blockchain implementation are decentralized and distributed,\nlaunched a centralized blockchain table feature in\n. The Blockchain Table in\nis a centralized blockchain which provide immutable feature. Compared to decentralized blockchains, centralized blockchains normally can provide a higher throughput and lower latency of transactions than consensus-based distributed blockchains.\nCurrently, there are at least four types of blockchain networks — public blockchains, private blockchains,\nblockchains and hybrid blockchains.\nA public blockchain has absolutely no access restrictions. Anyone with an\nto it as well as become a validator (i.e., participate in the execution of a\n[ 71 ] [ self-published source? ]\nUsually, such networks offer\nfor those who secure them and utilize some type of a\nSome of the largest, most known public blockchains are the bitcoin blockchain and the Ethereum blockchain.\nA private blockchain is permissioned.\nOne cannot join it unless invited by the network administrators. Participant and validator access is\n. To distinguish between open blockchains and other peer-to-peer decentralized database applications that are not open ad-hoc compute clusters, the terminology\n(DLT) is normally used for private blockchains.\nA hybrid blockchain has a combination of centralized and decentralized features.\nThe exact workings of the chain can vary based on which portions of centralization and decentralization are used.\nA sidechain is a designation for a blockchain ledger that runs in parallel to a primary blockchain.\nEntries from the primary blockchain (where said entries typically represent\n) can be linked to and from the sidechain; this allows the sidechain to otherwise operate independently of the primary blockchain (e.g., by using an alternate means of record keeping, alternate\n[ 75 ] [ better source needed ]\nConsortium blockchain\nA consortium blockchain is a type of blockchain that combines elements of both public and private blockchains. In a consortium blockchain, a group of organizations come together to create and operate the blockchain, rather than a single entity. The consortium members jointly manage the blockchain network and are responsible for validating transactions. Consortium blockchains are permissioned, meaning that only certain individuals or organizations are allowed to participate in the network. This allows for greater control over who can access the blockchain and helps to ensure that sensitive information is kept confidential.\nConsortium blockchains are commonly used in industries where multiple organizations need to collaborate on a common goal, such as supply chain management or financial services. One advantage of consortium blockchains is that they can be more efficient and scalable than public blockchains, as the number of nodes required to validate transactions is typically smaller. Additionally, consortium blockchains can provide greater security and reliability than private blockchains, as the consortium members work together to maintain the network. Some examples of consortium blockchains include\nBlockchain technology can be integrated into multiple areas. The primary use of blockchains is as a\n; there were also a few other operational products that had matured from\nAs of 2016, some businesses have been testing the technology and conducting low-level implementation to gauge blockchain's effects on organizational efficiency in their\nBlockchain is seen as a pivotal technological advancement of the 21st century, with the ability to impact organizations at strategic, operational, and market levels.\nIn 2019, it was estimated that around $2.9 billion were invested in blockchain technology, which represents an 89% increase from the year prior. Additionally, the International Data Corp estimated that corporate investment into blockchain technology would reach $12.4 billion by 2022.\nFurthermore, According to\nPricewaterhouseCoopers\n(PwC), the second-largest professional services network in the world, blockchain technology has the potential to generate an annual business value of more than $3 trillion by 2030. PwC's estimate is further augmented by a 2018 study that they have conducted, in which PwC surveyed 600 business executives and determined that 84% have at least some exposure to utilizing blockchain technology, which indicates a significant demand and interest in blockchain technology.\nradio and podcast series\nFifty Things That Made the Modern Economy\nidentified blockchain as a technology that would have far-reaching consequences for economics and society. The economist and\njournalist and broadcaster\ndiscussed why the underlying technology might have much wider applications and the challenges that needed to be overcome.\nHis first broadcast was on 29 June 2019.\nThe number of blockchain wallets quadrupled to 40 million between 2016 and 2020.\nA paper published in 2022 discussed the potential use of blockchain technology in\nsustainable management\nMost cryptocurrencies are designed to gradually decrease the production of that currency, placing a cap on the total amount of that currency that will ever be in circulation.\nCompared with ordinary currencies held by financial institutions or kept as cash on hand, cryptocurrencies can be more difficult for seizure by law enforcement.\nThe validity of each cryptocurrency's coins is provided by a blockchain. A blockchain is a continuously growing list of\n, called blocks, which are linked and secured using cryptography.\nEach block typically contains a\nas a link to a previous block,\na timestamp and transaction data.\nBy design, blockchains are inherently resistant to modification of the data. It is \"an open, distributed ledger that can record transactions between two parties efficiently and in a verifiable and permanent way\".\nFor use as a distributed ledger, a blockchain is typically managed by a\nnetwork collectively adhering to a protocol for validating new blocks. Once recorded, the data in any given block cannot be altered retroactively without the alteration of all subsequent blocks, which requires collusion of the network majority.\nBlockchains are secure by design and are an example of a distributed computing system with high\nByzantine fault tolerance\n. Decentralized consensus has therefore been achieved with a blockchain.\nIn the context of cryptocurrencies, the blockchain serves as a public ledger for all transactions. Cryptocurrencies use various\ntimestamping schemes\nto \"prove the validity of transactions added to the blockchain ledger without the need for a trusted third party\".\nThe first cryptocurrency was\n, which was first released as open-source software in 2009.\nAs cryptocurrencies have gained prominence, several countries have made advancements in their private and commercial law treatment to address legal uncertainties. In the United States, for example, the 2022 amendments to the Uniform Commercial Code (UCC) introduced Article 12, which establishes \"controllable electronic records\" (CERs) as a new category of personal property. This framework provides legal clarity for the ownership, transfer, and use of cryptocurrencies as CERs, with the concept of \"control\" serving as a functional equivalent to possession for digital assets. These reforms aim to align legal standards with market practices, reducing title disputes and supporting the integration of cryptocurrencies into commercial transactions.\nare contracts that can be partially or fully executed or enforced without human interaction.\nOne of the main objectives of a smart contract is\n. A key feature of smart contracts is that they do not need a trusted third party (such as a trustee) to act as an intermediary between contracting entities — the blockchain network executes the contract on its own. This may reduce friction between entities when transferring value and could subsequently open the door to a higher level of transaction automation.\nstaff discussion from 2018 reported that smart contracts based on blockchain technology might reduce\nand optimize the use of contracts in general, but \"no viable smart contract systems have yet emerged.\" Due to the lack of widespread use, their legal status was unclear.\n, many banks have expressed interest in implementing\nand are cooperating with companies creating private blockchains;\n[ 93 ] [ 94 ] [ 95 ]\naccording to a September 2016\nstudy, it is occurring faster than expected.\nIt has been estimated by the World Economic Forum that by 2025, 10% of the world's GDP will be stored on blockchain related technology.\nBanks are interested in this technology not least because it has the potential to speed up\nMoreover, as the blockchain industry has reached early maturity institutional appreciation has grown that it is, practically speaking, the infrastructure of a whole new financial industry, with all the implications which that entails.\nThis technology will transform financial transactions due to its ability to enhance data storage, process simultaneous transactions, lessen transaction costs, and improve capital market transparency for debt and equity capital administration.\nare opening new research labs dedicated to blockchain technology in order to explore how blockchain can be used in financial services to increase efficiency and reduce costs.\n[ 99 ] [ 100 ] Berenberg\n, a German bank, believes that blockchain is an \"overhyped technology\" that has had a large number of \"proofs of concept\", but still has major challenges, and very few success stories.\nThe blockchain has also given rise to\ninitial coin offerings\n(ICOs) as well as a new category of digital asset called security token offerings (STOs), also sometimes referred to as digital security offerings (DSOs).\nSTO/DSOs may be conducted privately or on public, regulated stock exchange and are used to tokenize traditional assets such as company shares as well as more innovative ones like intellectual property, real estate,\nart, or individual products. A number of companies are active in this space providing services for compliant\n, private STOs, and public STOs.\nBlockchain technology, such as cryptocurrencies and\n(NFTs), has been used in video games for\noffer in-game customization options, such as character skins or other in-game items, which the players can earn and trade with other players using in-game currency. Some games also allow for trading of virtual items using real-world currency, but this may be illegal in some countries where video games are seen as akin to gambling, and has led to\n, and thus publishers typically have shied away from allowing players to earn real-world funds from games.\nBlockchain games typically allow players to trade these in-game items for cryptocurrency, which can then be exchanged for money.\nThe first known game to use blockchain technologies was\n, launched in November 2017, where the player would purchase NFTs with Ethereum cryptocurrency, each NFT consisting of a\nthat the player could breed with others to create offspring with combined traits as new NFTs.\nThe game made headlines in December 2017 when one virtual pet sold for more than\n[ 107 ] CryptoKitties\nalso illustrated scalability problems for games on Ethereum when it created significant congestion on the Ethereum network in early 2018 with approximately 30% of all Ethereum transactions\n[ clarification needed ]\nBy the early 2020s, there had not been a breakout success in video games using blockchain, as these games tend to focus on using blockchain for speculation instead of more traditional forms of gameplay, which offers limited appeal to most players. Such games also represent a high risk to investors as their revenues can be difficult to predict.\nHowever, limited successes of some games, such as\n, and corporate plans towards\ncontent, refueled interest in the area of GameFi, a term describing the intersection of video games and financing typically backed by blockchain currency, in the second half of 2021.\nSeveral major publishers, including\nTake Two Interactive\n, have stated that blockchain and NFT-based games are under serious consideration for their companies in the future.\nbanned blockchain games, including those using cryptocurrency and\n, from being hosted on its\ndigital storefront service, which is widely used for personal computer gaming, claiming that this was an extension of their policy banning games that offered in-game items with real-world value. Valve's prior history with\n, was speculated to be a factor in the decision to ban blockchain games.\nJournalists and players responded positively to Valve's decision as blockchain and NFT games have a reputation for scams and fraud among most PC gamers,\nin competition to Steam, said that they would be open to accepted blockchain games in the wake of Valve's refusal.\nThis section needs to be\nPlease help update this article to reflect recent events or newly available information. ( August 2023 )\nThere have been several different efforts to employ blockchains in\nsupply chain management\nPrecious commodities mining\n— Blockchain technology has been used for tracking the origins of gemstones and other precious commodities. In 2016,\nThe Wall Street Journal\nreported that the blockchain technology company Everledger was partnering with\n's blockchain-based tracking service to trace the origin of diamonds to ensure that they were ethically mined.\nDiamond Trading Company\n(DTC) has been involved in building a diamond trading supply chain product called Tracer.\nwere running a trial to use a blockchain-backed system for\nmonitoring for lettuce and spinach – all nodes of the blockchain were administered by Walmart and located on the IBM\n[ 116 ] Fashion industry\n— There is an opaque relationship between brands, distributors, and customers in the fashion industry, which prevents the sustainable and stable development of the fashion industry. Blockchain could make this information transparent, assisting sustainable development of the industry.\n[ 117 ] Motor vehicles\ndeveloped a blockchain prototype used to facilitate consistent documentation of contracts along the supply chain so that the\nand contractual obligations required of its direct suppliers can be passed on to second tier suppliers and beyond.\nIn another project, the company uses blockchain technology to track the emissions of climate-relevant gases and the amount of secondary material along the supply chain for its\nThere are several different efforts to offer\nservices via the blockchain. These domain names can be controlled by the use of a private key, which purports to allow for uncensorable websites. This would also bypass a registrar's ability to suppress domains used for fraud, abuse, or illegal content.\nis a cryptocurrency that supports the \".bit\"\n(TLD). Namecoin was forked from bitcoin in 2011. The .bit TLD is not sanctioned by\n, instead requiring an\nalternative DNS root\nAs of 2015, .bit was used by 28 websites, out of 120,000 registered names.\nNamecoin was dropped by\nin 2019, due to malware and potential other legal issues.\nOther blockchain alternatives to ICANN include\nThe Handshake Network\nSpecific TLDs include \".eth\", \".luxe\", and \".kred\", which are associated with the Ethereum blockchain through the Ethereum Name Service (ENS). The .kred TLD also acts as an alternative to conventional\ncryptocurrency wallet\naddresses as a convenience for transferring cryptocurrency.\nBlockchain technology can be used to create a permanent, public, transparent ledger system for compiling data on sales, tracking digital use and payments to content creators, such as wireless users\nThe Gartner 2019 CIO Survey reported 2% of higher education respondents had launched blockchain projects and another 18% were planning academic projects in the next 24 months.\nto adopt blockchain technology in music distribution.\n's Mycelia service has also been proposed as a blockchain-based alternative \"that gives artists more control over how their songs and associated data circulate among fans and other musicians.\"\nNew distribution methods are available for the\npeer-to-peer insurance\nparametric insurance\nfollowing the adoption of blockchain.\nare also set to benefit from blockchains because they involve many collaborating peers.\nThe use of blockchain in libraries is being studied with a grant from the U.S. Institute of Museum and Library Services.\nOther blockchain designs include\n, a collaborative effort from the\nto support blockchain-based distributed ledgers, with projects under this initiative including Hyperledger Burrow (by Monax) and Hyperledger Fabric (spearheaded by IBM).\n[ 135 ] [ 136 ] [ 137 ]\nAnother is Quorum, a permissioned private blockchain by\nwith private storage, used for contract applications.\nintroduced a blockchain table feature in its\nBlockchain is also being used in\npeer-to-peer energy trading\n[ 139 ] [ 140 ] [ 141 ] Lightweight blockchains\n, or simplified blockchains, are more suitable for\n(IoT) applications than conventional blockchains.\nOne experiment suggested that a lightweight blockchain-based network could accommodate up to 1.34 million authentication processes every second, which could be sufficient for resource-constrained IoT networks.\nBlockchain could be used in detecting counterfeits by associating unique identifiers to products, documents and shipments, and storing records associated with transactions that cannot be forged or altered.\nIt is however argued that blockchain technology needs to be supplemented with technologies that provide a strong binding between physical objects and blockchain systems,\nas well as provisions for content creator verification\nestablished an Anti-Counterfeiting Blockathon Forum, with the objective of \"defining, piloting and implementing\" an anti-counterfeiting infrastructure at the European level.\nThe Dutch Standardisation organisation NEN uses blockchain together with\nto authenticate certificates.\nBeijing and Shanghai are among the cities designated by China to trial blockchain applications as January 30, 2022.\nIn Chinese legal proceedings, blockchain technology was first accepted as a method for authenticating internet evidence by the\nHangzhou Internet Court\nin 2019 and has since been accepted by other Chinese courts.\nBlockchain interoperability\nWith the increasing number of blockchain systems appearing, even only those that support cryptocurrencies, blockchain interoperability is becoming a topic of major importance. The objective is to support transferring assets from one blockchain system to another blockchain system. Wegner\nis the ability of two or more software components to cooperate despite differences in language, interface, and execution platform\". The objective of blockchain interoperability is therefore to support such cooperation among blockchain systems, despite those kinds of differences.\nThere are already several blockchain interoperability solutions available.\nThey can be classified into three categories: cryptocurrency interoperability approaches, blockchain engines, and blockchain connectors.\nSeveral individual IETF participants produced the draft of a blockchain interoperability architecture.\nEnergy consumption concerns\nSome cryptocurrencies use blockchain mining — the peer-to-peer computer computations by which transactions are validated and verified. This requires a large amount of energy. In June 2018, the\nBank for International Settlements\ncriticized the use of public\nblockchains for their high energy consumption.\n[ 156 ] [ 157 ] [ 158 ]\nEarly concern over the high energy consumption was a factor in later blockchains such as\n(2020) adopting the less energy-intensive\nmodel. Researchers have estimated that bitcoin consumes 100,000 times as much energy as proof-of-stake networks.\nCambridge University\ndetermined that bitcoin (at 121 terawatt-hours per year) used more electricity than Argentina (at 121TWh) and the Netherlands (109TWh).\nAccording to Digiconomist, one bitcoin transaction required 708 kilowatt-hours of electrical energy, the amount an average U.S. household consumed in 24 days.\nIn February 2021, U.S. Treasury secretary\ncalled bitcoin \"an extremely inefficient way to conduct transactions\", saying \"the amount of energy consumed in processing those transactions is staggering\".\nstated that \"Bitcoin uses more electricity per transaction than any other method known to mankind\", adding \"It's not a great climate thing.\"\nNicholas Weaver, of the\nInternational Computer Science Institute\nUniversity of California, Berkeley\n, examined blockchain's online security, and the\nof proof-of-work public blockchains, and in both cases found it grossly inadequate.\nThe 31TWh-45TWh of electricity used for bitcoin in 2018 produced 17–23 million tonnes of CO\nBy 2022, the University of Cambridge and Digiconomist estimated that the two largest proof-of-work blockchains, bitcoin and Ethereum, together used twice as much electricity in one year as the whole of Sweden, leading to the release of up to 120 million tonnes of CO\nSome cryptocurrency developers are considering moving from the proof-of-work model to the\nIn Sept, 2022, Ethereum converted from proof-of-work to proof-of-stake.\nBlockchain panel discussion at the first IEEE Computer Society TechIgnite conference\nIn October 2014, the MIT Bitcoin Club, with funding from MIT alumni, provided undergraduate students at the\nMassachusetts Institute of Technology\naccess to $100 of bitcoin. The adoption rates, as studied by\n(2016), revealed that when people who typically adopt technologies early are given delayed access, they tend to reject the technology.\nMany universities have founded departments focusing on crypto and blockchain, including\n, in 2017. In the same year,\nbecame \"one of the first big European universities to launch a blockchain course\", according to the\nMotivations for adopting blockchain technology (an aspect of\n) have been investigated by researchers. For example, Janssen, et al. provided a framework for analysis,\nand Koens \u0026 Poll pointed out that adoption could be heavily driven by non-technical factors.\nBased on behavioral models, Li\nhas discussed the differences between adoption at the individual level and organizational levels.\nScholars in business and management have started studying the role of blockchains to support collaboration.\nIt has been argued that blockchains can foster both cooperation (i.e., prevention of opportunistic behavior) and coordination (i.e., communication and information sharing). Thanks to reliability, transparency, traceability of records, and information immutability, blockchains facilitate collaboration in a way that differs both from the traditional use of contracts and from relational norms. Contrary to contracts, blockchains do not directly rely on the legal system to enforce agreements.\nIn addition, contrary to the use of relational norms, blockchains do not require a trust or direct connections between collaborators.\nBlockchain and internal audit\nBlockchain Basics \u0026 Cryptography\nMassachusetts Institute of Technology\nThe need for internal audits to provide effective oversight of organizational efficiency will require a change in the way that information is accessed in new formats.\nBlockchain adoption requires a framework to identify the risk of exposure associated with transactions using blockchain. The\nInstitute of Internal Auditors\nhas identified the need for internal auditors to address this transformational technology. New methods are required to develop audit plans that identify threats and risks. The Internal Audit Foundation study,\nBlockchain and Internal Audit,\nassesses these factors.\nAmerican Institute of Certified Public Accountants\nhas outlined new roles for auditors as a result of blockchain.\nIn blockchain technology, a\nis an instance of a blockchain powered by the same or a newer version of the underlying software, to be used for testing and experimentation without risk to real funds or the main chain.\nare separate and distinct from the official\ncoins, don't have value, and can be obtained freely from\nTestnets allow for the development of blockchain applications without the risk of losing funds.\nUsing testnets, a bug was discovered in the\nsoftware that gave miners the ability to take down essential parts of the Bitcoin infrastructure (nodes) by sending a 'bad' block to the blockchain.\n) is the fully operational version of a blockchain where real transactions occur, as opposed to a testnet. It is secured through consensus mechanisms like\ndecentralized applications\nmarks the transition from a testnet to a live blockchain, involving security audits, network deployment, and token migration.\nIn September 2015, the first peer-reviewed academic journal dedicated to cryptocurrency and blockchain technology research,\n, was announced. The inaugural issue was published in December 2016.\nThe journal covers aspects of\nthat relate to cryptocurrencies.\nThe journal encourages authors to\nof submitted papers, which are then\ninto the bitcoin blockchain. Authors are also asked to include a personal bitcoin address on the first page of their papers for non-repudiation purposes.\nEconomics portal Changelog\n– a record of all notable changes made to a project\n– an informational aid used to reduce failure\nEconomics of digitization List of blockchains Privacy and blockchain Version control\n– a record of all changes (mostly of software project) in a form of a graph\n– a version control system where the commit hash value depends on the previous commit hash value, also with\n^ Morris, David Z. (15 May 2016). \"Leaderless, Blockchain-Based Venture Capital Fund Raises $100 Million, And Counting\" . Fortune . Archived from the original on 21 May 2016 . Retrieved 23 May 2016 . ^ a b Popper, Nathan (21 May 2016). \"A Venture Fund With Plenty of Virtual Capital, but No Capitalist\" . The New York Times . Archived from the original on 22 May 2016 . Retrieved 23 May 2016 . ^ a b c d e f g h i \"Blockchains: The great chain of being sure about things\" . The Economist . 31 October 2015. Archived from the original on 3 July 2016 . Retrieved 18 June 2016 . The technology behind bitcoin lets people who do not know or trust each other build a dependable ledger. This has implications far beyond the crypto currency. ^ a b c d e Narayanan, Arvind; Bonneau, Joseph; Felten, Edward; Miller, Andrew; Goldfeder, Steven (2016). Bitcoin and cryptocurrency technologies: a comprehensive introduction . Princeton, New Jersey: Princeton University Press . ISBN 978-0-691-17169-2 . ^ Iansiti, Marco; Lakhani, Karim R. (January 2017). \"The Truth About Blockchain\" . Harvard Business Review . Cambridge, Massachusetts: Harvard University . Archived from the original on 18 January 2017 . Retrieved 17 January 2017 . The technology at the heart of bitcoin and other virtual currencies, blockchain is an open, distributed ledger that can record transactions between two parties efficiently and in a verifiable and permanent way. ^ Oberhaus, Daniel (27 August 2018). \"The World's Oldest Blockchain Has Been Hiding in the New York Times Since 1995\" . Vice . Retrieved 9 October 2021 . ^ Lunn, Bernard (10 February 2018). \"Blockchain may finally disrupt payments from Micropayments to credit cards to SWIFT\" . dailyfintech.com . Archived from the original on 27 September 2018 . Retrieved 18 November 2018 . ^ a b c d e Hampton, Nikolai (5 September 2016). \"Understanding the blockchain hype: Why much of it is nothing more than snake oil and spin\" . Computerworld . Archived from the original on 6 September 2016 . Retrieved 5 September 2016 . ^ a b Bakos, Yannis; Halaburda, Hanna; Mueller-Bloch, Christoph (February 2021). \"When Permissioned Blockchains Deliver More Decentralization Than Permissionless\" . Communications of the ACM . 64 (2): 20– 22. doi : 10.1145/3442371 . S2CID 231704491 . ^ Sherman, Alan T.; Javani, Farid; Zhang, Haibin; Golaszewski, Enis (January 2019). \"On the Origins and Variations of Blockchain Technologies\". IEEE Security \u0026 Privacy . 17 (1): 72– 77. arXiv : 1810.06130 . doi : 10.1109/MSEC.2019.2893730 . ISSN 1558-4046 . S2CID 53114747 . ^ Download PDF of dissertation from Dr. Chaum's website ^ Haber, Stuart; Stornetta, W. Scott (January 1991). \"How to time-stamp a digital document\". Journal of Cryptology . 3 (2): 99– 111. CiteSeerX 10.1.1.46.8740 . doi : 10.1007/bf00196791 . S2CID 14363020 . ^ Bayer, Dave; Haber, Stuart; Stornetta, W. Scott (March 1992). \"Improving the Efficiency and Reliability of Digital Time-Stamping\". Sequences . Vol. 2. pp. 329– 334. CiteSeerX 10.1.1.71.4891 . doi : 10.1007/978-1-4613-9323-8_24 . ISBN 978-1-4613-9325-2 . ^ Oberhaus, Daniel (27 August 2018). \"The World's Oldest Blockchain Has Been Hiding in the New York Times Since 1995\" . Vice.com . Retrieved 9 October 2021 . ^ Nian, Lam Pak; Chuen, David LEE Kuo (2015). \"A Light Touch of Regulation for Virtual Currencies\". In Chuen, David LEE Kuo (ed.). Handbook of Digital Currency: Bitcoin, Innovation, Financial Instruments, and Big Data . Academic Press. p. 319. ISBN 978-0-12-802351-8 . ^ \"Blockchain Size\" . Archived from the original on 19 May 2020 . Retrieved 25 February 2020 . ^ Johnsen, Maria (12 May 2020). Blockchain in Digital Marketing: A New Paradigm of Trust . Maria Johnsen. p. 6. ISBN 979-8-6448-7308-1 . ^ \"The future of blockchain in 8 charts\" . Raconteur . 27 June 2016. Archived from the original on 2 December 2016 . Retrieved 3 December 2016 . ^ \"Hype Killer - Only 1% of Companies Are Using Blockchain, Gartner Reports | Artificial Lawyer\" . Artificial Lawyer . 4 May 2018. Archived from the original on 22 May 2018 . Retrieved 22 May 2018 . ^ Kasey Panetta. (31 October 2018). \"Digital Business: CIO Agenda 2019: Exploit Transformational Technologies.\" Gartner website Archived 20 April 2021 at the Wayback Machine Retrieved 27 March 2021. ^ Armstrong, Stephen (7 November 2016). \"Move over Bitcoin, the blockchain is only just getting started\" . Wired . Archived from the original on 8 November 2016 . Retrieved 9 November 2016 . ^ Catalini, Christian; Gans, Joshua S. (23 November 2016). \"Some Simple Economics of the Blockchain\" (PDF) . SSRN . doi : 10.2139/ssrn.2874598 . hdl : 1721.1/130500 . S2CID 46904163 . SSRN 2874598 . Archived (PDF) from the original on 6 March 2020 . Retrieved 16 September 2019 . ^ Tapscott, Don ; Tapscott, Alex (8 May 2016). \"Here's Why Blockchains Will Change the World\" . Fortune . Archived from the original on 13 November 2016 . Retrieved 16 November 2016 . ^ Bheemaiah, Kariappa (January 2015). \"Block Chain 2.0: The Renaissance of Money\" . Wired . Archived from the original on 14 November 2016 . Retrieved 13 November 2016 . ^ Chen, Huashan; Pendleton, Marcus; Njilla, Laurent; Xu, Shouhuai (12 June 2020). \"A Survey on Ethereum Systems Security: Vulnerabilities, Attacks, and Defenses\". ACM Computing Surveys . 53 (3): 3– 4. arXiv : 1908.04507 . doi : 10.1145/3391195 . ISSN 0360-0300 . S2CID 199551841 . ^ Shishir, Bhatia (2 February 2006). Structured Information Flow (SIF) Framework for Automating End-to-End Information Flow for Large Organizations (Thesis). Virginia Tech. Archived from the original on 4 April 2022 . Retrieved 28 March 2022 . ^ a b c d Bhaskar, Nirupama Devi; Chuen, David LEE Kuo (2015). \"Bitcoin Mining Technology\". Handbook of Digital Currency . pp. 45– 65. doi : 10.1016/B978-0-12-802117-0.00003-5 . ISBN 978-0-12-802117-0 . ^ Knirsch, Unterweger \u0026 Engel 2019 , p. 2. ^ a b c Antonopoulos, Andreas (20 February 2014). \"Bitcoin security model: trust by computation\" . Radar . O'Reilly. Archived from the original on 31 October 2016 . Retrieved 19 November 2016 . ^ a b Antonopoulos, Andreas M. (2014). Mastering Bitcoin. Unlocking Digital Cryptocurrencies . Sebastopol, CA: O'Reilly Media. ISBN 978-1449374037 . Archived from the original on 1 December 2016 . Retrieved 3 November 2015 . ^ Nakamoto, Satoshi (October 2008). \"Bitcoin: A Peer-to-Peer Electronic Cash System\" (PDF) . bitcoin.org. Archived (PDF) from the original on 20 March 2014 . Retrieved 28 April 2014 . ^ \"Permissioned Blockchains\" . Explainer . Monax. Archived from the original on 20 November 2016 . Retrieved 20 November 2016 . ^ Kumar, Randhir; Tripathi, Rakesh (November 2019). \"Implementation of Distributed File Storage and Access Framework using IPFS and Blockchain\". 2019 Fifth International Conference on Image Information Processing (ICIIP) . IEEE. pp. 246– 251. doi : 10.1109/iciip47207.2019.8985677 . ISBN 978-1-7281-0899-5 . S2CID 211119043 . ^ Lee, Timothy (12 March 2013). \"Major glitch in Bitcoin network sparks sell-off; price temporarily falls 23%\" . Arstechnica. Archived from the original on 20 April 2013 . Retrieved 25 February 2018 . ^ Smith, Oli (21 January 2018). \"Bitcoin price RIVAL: Cryptocurrency 'faster than bitcoin' will CHALLENGE market leaders\" . Express . Archived from the original on 28 March 2019 . Retrieved 6 April 2021 . ^ \"Bitcoin split in two, here's what that means\" . CNN . 1 August 2017. Archived from the original on 27 February 2018 . Retrieved 7 April 2021 . ^ Hughes, Laurie; Dwivedi, Yogesh K.; Misra, Santosh K.; Rana, Nripendra P.; Raghavan, Vishnupriya; Akella, Viswanadh (December 2019). \"Blockchain research, practice and policy: Applications, benefits, limitations, emerging research themes and research agenda\" . International Journal of Information Management . 49 : 114– 129. doi : 10.1016/j.ijinfomgt.2019.02.005 . hdl : 10454/17473 . S2CID 116666889 . Archived from the original on 2 February 2023 . Retrieved 20 November 2022 . ^ Roberts, Jeff John (29 May 2018). \"Bitcoin Spinoff Hacked in Rare '51% Attack' \" . Fortune . Archived from the original on 22 December 2021 . Retrieved 27 December 2022 . ^ a b Brito, Jerry; Castillo, Andrea (2013). Bitcoin: A Primer for Policymakers (PDF) (Report). Fairfax, VA: Mercatus Center, George Mason University. Archived (PDF) from the original on 21 September 2013 . Retrieved 22 October 2013 . ^ Raval, Siraj (2016). Decentralized Applications: Harnessing Bitcoin's Blockchain Technology . O'Reilly Media, Inc. pp. 1 – 2 . ISBN 978-1-4919-2452-5 . ^ Kopfstein, Janus (12 December 2013). \"The Mission to Decentralize the Internet\" . The New Yorker . Archived from the original on 31 December 2014 . Retrieved 30 December 2014 . The network's 'nodes' — users running the bitcoin software on their computers — collectively check the integrity of other nodes to ensure that no one spends the same coins twice. All transactions are published on a shared public ledger, called the 'block chain.' ^ Gervais, Arthur; Karame, Ghassan O.; Capkun, Vedran; Capkun, Srdjan. \"Is Bitcoin a Decentralized Currency?\" . InfoQ . InfoQ \u0026 IEEE computer society. Archived from the original on 10 October 2016 . Retrieved 11 October 2016 . ^ a b c Deirmentzoglou, Evangelos; Papakyriakopoulos, Georgios; Patsakis, Constantinos (2019). \"A Survey on Long-Range Attacks for Proof of Stake Protocols\" . IEEE Access . 7 : 28712– 28725. Bibcode : 2019IEEEA...728712D . doi : 10.1109/ACCESS.2019.2901858 . eISSN 2169-3536 . S2CID 84185792 . ^ a b Voorhees, Erik (30 October 2015). \"It's All About the Blockchain\" . Money and State . Archived from the original on 1 November 2015 . Retrieved 2 November 2015 . ^ Reutzel, Bailey (13 July 2015). \"A Very Public Conflict Over Private Blockchains\" . PaymentsSource . New York, NY: SourceMedia, Inc. Archived from the original on 21 April 2016 . Retrieved 18 June 2016 . ^ a b Casey MJ (15 April 2015). \"Moneybeat/BitBeat: Blockchains Without Coins Stir Tensions in Bitcoin Community\" . The Wall Street Journal . Archived from the original on 10 June 2016 . Retrieved 18 June 2016 . ^ \"The 'Blockchain Technology' Bandwagon Has A Lesson Left To Learn\" . dinbits.com . 3 November 2015. Archived from the original on 29 June 2016 . Retrieved 18 June 2016 . ^ DeRose, Chris (26 June 2015). \"Why the Bitcoin Blockchain Beats Out Competitors\" . American Banker . Archived from the original on 30 March 2016 . Retrieved 18 June 2016 . ^ Greenspan, Gideon (19 July 2015). \"Ending the bitcoin vs blockchain debate\" . multichain.com . Archived from the original on 8 June 2016 . Retrieved 18 June 2016 . ^ Tapscott, Don ; Tapscott, Alex (May 2016). The Blockchain Revolution: How the Technology Behind Bitcoin is Changing Money, Business, and the World . Portfolio/Penguin. ISBN 978-0-670-06997-2 . ^ Barry, Levine (11 June 2018). \"A new report bursts the blockchain bubble\" . MarTech. Archived from the original on 13 July 2018 . Retrieved 13 July 2018 . ^ a b Ovenden, James. \"Blockchain Top Trends In 2017\" . The Innovation Enterprise. Archived from the original on 30 November 2016 . Retrieved 4 December 2016 . ^ a b Bob Marvin (30 August 2017). \"Blockchain: The Invisible Technology That's Changing the World\" . PC MAG Australia . ZiffDavis, LLC. Archived from the original on 25 September 2017 . Retrieved 25 September 2017 . ^ O'Keeffe, M.; Terzi, A. (7 July 2015). \"The political economy of financial crisis policy\" . Bruegel . Archived from the original on 19 May 2018 . Retrieved 8 May 2018 . ^ Dr Garrick Hileman \u0026 Michel Rauchs (2017). \"GLOBAL CRYPTOCURRENCY BENCHMARKING STUDY\" (PDF) . Cambridge Centre for Alternative Finance . University of Cambridge Judge Business School. Archived (PDF) from the original on 15 May 2019 . Retrieved 15 May 2019 – via crowdfundinsider. ^ Raymaekers, Wim (March 2015). \"Cryptocurrency Bitcoin: Disruption, challenges and opportunities\" . Journal of Payments Strategy \u0026 Systems . 9 (1): 30– 46. doi : 10.69554/FBUJ3107 . Archived from the original on 15 May 2019 . Retrieved 15 May 2019 . ^ \"Why Crypto Companies Still Can't Open Checking Accounts\" . 3 March 2019. Archived from the original on 4 June 2019 . Retrieved 4 June 2019 . ^ Christian Brenig, Rafael Accorsi \u0026 Günter Müller (Spring 2015). \"Economic Analysis of Cryptocurrency Backed Money Laundering\" . Association for Information Systems AIS Electronic Library (AISeL) . Archived from the original on 28 August 2019 . Retrieved 15 May 2019 . ^ Greenberg, Andy (25 January 2017). \"Monero, the Drug Dealer's Cryptocurrency of Choice, Is on Fire\" . Wired . ISSN 1059-1028 . Archived from the original on 10 December 2018 . Retrieved 15 May 2019 . ^ Orcutt, Mike. \"It's getting harder to hide money in Bitcoin\" . MIT Technology Review . Archived from the original on 5 May 2019 . Retrieved 15 May 2019 . ^ \"Explainer: 'Privacy coin' Monero offers near total anonymity\" . Reuters . 15 May 2019. Archived from the original on 15 May 2019 . Retrieved 15 May 2019 . ^ \"Blockchain\" . standards.org.au . Standards Australia. Archived from the original on 24 June 2021 . Retrieved 21 June 2021 . ^ a b \"ISO/TC 307 Blockchain and distributed ledger technologies\" . iso.org . ISO. 15 September 2020. Archived from the original on 18 June 2021 . Retrieved 21 June 2021 . ^ Deshmukh, Sumedha; Boulais, Océane; Koens, Tommy. \"Global Standards Mapping Initiative: An overview of blockchain technical standards\" (PDF) . weforum.org . World Economic Forum. Archived (PDF) from the original on 24 June 2021 . Retrieved 23 June 2021 . ^ \"Blockchain Overview\" . NIST . 25 September 2019. Archived from the original on 24 June 2021 . Retrieved 21 June 2021 . ^ \"CEN and CENELEC publish a White Paper on standards in Blockchain \u0026 Distributed Ledger Technologies\" . cencenelec.eu . CENELEC. Archived from the original on 24 June 2021 . Retrieved 21 June 2021 . ^ \"Standards\" . IEEE . IEEE Blockchain. Archived from the original on 24 June 2021 . Retrieved 21 June 2021 . ^ Hardjono, Thomas. \"An Interoperability Architecture for Blockchain/DLT Gateways\" . ietf.org . IETF. Archived from the original on 28 April 2021 . Retrieved 21 June 2021 . ^ a b \"Details: Oracle Blockchain Table\" . Archived from the original on 20 January 2021 . Retrieved 1 January 2022 . ^ a b \"Oracle Blockchain Table\" . Archived from the original on 16 May 2021 . Retrieved 1 January 2022 . ^ \"How Companies Can Leverage Private Blockchains to Improve Efficiency and Streamline Business Processes\" . Perfectial . ^ [Distributed Ledger Technology: Hybrid Approach, Front-to-Back Designing and Changing Trade Processing Infrastructure, By Martin Walker, First published:, 24 OCT 2018 ISBN 978-1-78272-389-9 ] ^ Siraj Raval (18 July 2016). Decentralized Applications: Harnessing Bitcoin's Blockchain Technology . \"O'Reilly Media, Inc.\". pp. 22–. ISBN 978-1-4919-2452-5 . ^ Niaz Chowdhury (16 August 2019). Inside Blockchain, Bitcoin, and Cryptocurrencies . CRC Press. pp. 22–. ISBN 978-1-00-050770-6 . ^ U.S. patent 10,438,290 ^ Dib, Omar; Brousmiche, Kei-Leo; Durand, Antoine; Thea, Eric; Hamida, Elyes Ben. \"Consortium Blockchains: Overview, Applications and Challenges\". International Journal on Advances in Telecommunications . 11 (1/2): 51– 64. ^ Katie Martin (27 September 2016). \"CLS dips into blockchain to net new currencies\" . Financial Times . Archived from the original on 9 November 2016 . Retrieved 7 November 2016 . ^ a b c Kimani, Danson; Adams, Kweku; Attah-Boakye, Rexford; Ullah, Subhan; Frecknall-Hughes, Jane; Kim, Ja (1 December 2020). \"Blockchain, business and the fourth industrial revolution: Whence, whither, wherefore and how?\" . Technological Forecasting and Social Change . 161 120254. doi : 10.1016/j.techfore.2020.120254 . ISSN 0040-1625 . ^ Castillo, Michael (16 April 2019). \"blockchain 50: Billion Dollar Babies\" . Financial Website . SourceMedia. Archived from the original on 31 January 2021 . Retrieved 1 February 2021 . ^ Davies, Steve (2018). \"PwC's Global Blockchain Survey\" . Financial Website . SourceMedia. Archived from the original on 25 January 2021 . Retrieved 1 February 2021 . ^ \"BBC Radio 4 - Things That Made the Modern Economy, Series 2, Blockchain\" . BBC . Archived from the original on 14 October 2022 . Retrieved 14 October 2022 . ^ Liu, Shanhong (13 March 2020). \"Blockchain - Statistics \u0026 Facts\" . Statistics Website . SourceMedia. Archived from the original on 15 February 2021 . Retrieved 17 February 2021 . ^ Du, Wenbo; Ma, Xiaozhi; Yuan, Hongping; Zhu, Yue (1 August 2022). \"Blockchain technology-based sustainable management research: the status quo and a general framework for future application\" . Environmental Science and Pollution Research . 29 (39): 58648– 58663. Bibcode : 2022ESPR...2958648D . doi : 10.1007/s11356-022-21761-2 . ISSN 1614-7499 . PMC 9261142 . PMID 35794327 . ^ a b c d Narayanan, Arvind; Bonner, Joseph; Clark, Andrew; Felten, Edward W.; Kroll, Joshua A.; Wallach, Daniel S. (2016). Bitcoin and Cryptocurrency Technologies: A Comprehensive Introduction . Princeton University Press. ISBN 978-0-691-17169-2 . ^ a b c Iansiti, Marco; Lakhani, Karim R. (January 2017). \"The Truth About Blockchain\" . Harvard Business Review . Retrieved 17 January 2017 . The technology at the heart of bitcoin and other virtual currencies, blockchain is an open, distributed ledger that can record transactions between two parties efficiently and in a verifiable and permanent way. ^ Raval, Siraj (2016). Decentralized Applications: Harnessing Bitcoin's Blockchain Technology . O'Reilly Media, Inc. ISBN 978-1-4919-2452-5 . OCLC 968277125 . ^ Catalini, Christian. \"How Blockchain Technology Will Impact the Digital Economy\" . Forbes . Retrieved 8 May 2017 . ^ Tosato, Andrea; Odinet, Christopher K. (2025). \"Digital Assets and the Property Question\" . Florida Law Review . forthcoming. SSRN 5151907 . ^ Franco, Pedro (2014). Understanding Bitcoin: Cryptography, Engineering and Economics . John Wiley \u0026 Sons. p. 9. ISBN 978-1-119-01916-9 . Archived from the original on 14 February 2017 . Retrieved 4 January 2017 – via Google Books. ^ Casey M (16 July 2018). The impact of blockchain technology on finance : a catalyst for change . London, UK. ISBN 978-1-912179-15-2 . OCLC 1059331326 . {{ cite book }} : CS1 maint: location missing publisher ( link ) ^ Governatori, Guido; Idelberger, Florian; Milosevic, Zoran; Riveret, Regis; Sartor, Giovanni; Xu, Xiwei (2018). \"On legal contracts, imperative and declarative smart contracts, and blockchain systems\". Artificial Intelligence and Law . 26 (4): 33. doi : 10.1007/s10506-018-9223-3 . S2CID 3663005 . ^ Virtual Currencies and Beyond: Initial Considerations (PDF) . IMF Discussion Note. International Monetary Fund. 2016. p. 23. ISBN 978-1-5135-5297-2 . Archived (PDF) from the original on 14 April 2018 . Retrieved 19 April 2018 . ^ Epstein, Jim (6 May 2016). \"Is Blockchain Technology a Trojan Horse Behind Wall Street's Walled Garden?\" . Reason . Archived from the original on 8 July 2016 . Retrieved 29 June 2016 . mainstream misgivings about working with a system that's open for anyone to use. Many banks are partnering with companies building so-called private blockchains that mimic some aspects of Bitcoin's architecture except they're designed to be closed off and accessible only to chosen parties. ... [but some believe] that open and permission-less blockchains will ultimately prevail even in the banking sector simply because they're more efficient. ^ Redrup, Yolanda (29 June 2016). \"ANZ backs private blockchain, but won't go public\" . Australia Financial Review . Archived from the original on 3 July 2016 . Retrieved 7 July 2016 . Blockchain networks can be either public or private. Public blockchains have many users and there are no controls over who can read, upload or delete the data and there are an unknown number of pseudonymous participants. In comparison, private blockchains also have multiple data sets, but there are controls in place over who can edit data and there are a known number of participants. ^ Shah, Rakesh (1 March 2018). \"How Can The Banking Sector Leverage Blockchain Technology?\" . PostBox Communications . PostBox Communications Blog. Archived from the original on 17 March 2018. Banks preferably have a notable interest in utilizing Blockchain Technology because it is a great source to avoid fraudulent transactions. Blockchain is considered hassle free, because of the extra level of security it offers. ^ Kelly, Jemima (28 September 2016). \"Banks adopting blockchain 'dramatically faster' than expected: IBM\" . Reuters . Archived from the original on 28 September 2016 . Retrieved 28 September 2016 . ^ Arnold, Martin (23 September 2013). \"IBM in blockchain project with China UnionPay\" . Financial Times . Archived from the original on 9 November 2016 . Retrieved 7 November 2016 . ^ Ravichandran, Arvind; Fargo, Christopher; Kappos, David; Portilla, David; Buretta, John; Ngo, Minh Van; Rosenthal-Larrea, Sasha (28 January 2022). \"Blockchain in the Banking Sector: A Review of the Landscape and Opportunities\" . Archived from the original on 16 February 2022 . Retrieved 16 February 2022 . ^ \"UBS leads team of banks working on blockchain settlement system\" . Reuters . 24 August 2016. Archived from the original on 19 May 2017 . Retrieved 13 May 2017 . ^ \"Cryptocurrency Blockchain\" . capgemini.com . Archived from the original on 5 December 2016 . Retrieved 13 May 2017 . ^ Kelly, Jemima (31 October 2017). \"Top banks and R3 build blockchain-based payments system\" . Reuters . Archived from the original on 10 July 2018 . Retrieved 9 July 2018 . ^ \"Are Token Assets the Securities of Tomorrow?\" (PDF) . Deloitte . February 2019. Archived (PDF) from the original on 23 June 2019 . Retrieved 26 September 2019 . ^ Hammerberg, Jeff (7 November 2021). \"Potential impact of blockchain on real estate\" . Washington Blade . Archived from the original on 3 December 2021 . Retrieved 3 December 2021 . ^ a b Clark, Mitchell (15 October 2021). \"Valve bans blockchain games and NFTs on Steam, Epic will try to make it work\" . The Verge . Archived from the original on 12 June 2022 . Retrieved 8 November 2021 . ^ a b c Mozuch, Mo (29 April 2021). \"Blockchain Games Twist The Fundamentals Of Online Gaming\" . Inverse . Archived from the original on 5 July 2022 . Retrieved 4 November 2021 . ^ \"Internet firms try their luck at blockchain games\" . Asia Times . 22 February 2018. Archived from the original on 18 November 2021 . Retrieved 28 February 2018 . ^ Evelyn Cheng (6 December 2017). \"Meet CryptoKitties, the $100,000 digital beanie babies epitomizing the cryptocurrency mania\" . CNBC . Archived from the original on 20 November 2018 . Retrieved 28 February 2018 . ^ Laignee Barron (13 February 2018). \"CryptoKitties is Going Mobile. Can Ethereum Handle the Traffic?\" . Fortune . Archived from the original on 28 October 2018 . Retrieved 30 September 2018 . ^ \"CryptoKitties craze slows down transactions on Ethereum\" . 12 May 2017. Archived from the original on 12 January 2018. ^ Wells, Charlie; Egkolfopoulou, Misrylena (30 October 2021). \"Into the Metaverse: Where Crypto, Gaming and Capitalism Collide\" . Bloomberg News . Archived from the original on 2 July 2022 . Retrieved 11 November 2021 . ^ Orland, Kyle (4 November 2021). \"Big-name publishers see NFTs as a big part of gaming's future\" . Ars Technica . Archived from the original on 18 June 2022 . Retrieved 4 November 2021 . ^ a b Knoop, Joseph (15 October 2021). \"Steam bans all games with NFTs or cryptocurrency\" . PC Gamer . Archived from the original on 17 July 2022 . Retrieved 8 November 2021 . ^ Clark, Mitchell (15 October 2021). \"Epic says it's 'open' to blockchain games after Steam bans them\" . The Verge . Archived from the original on 6 June 2022 . Retrieved 11 November 2021 . ^ Nash, Kim S. (14 July 2016). \"IBM Pushes Blockchain into the Supply Chain\" . The Wall Street Journal . Archived from the original on 18 July 2016 . Retrieved 24 July 2016 . ^ Gstettner, Stefan (30 July 2019). \"How Blockchain Will Redefine Supply Chain Management\" . Knowledge@Wharton . The Wharton School of the University of Pennsylvania . Retrieved 28 August 2020 . ^ Corkery, Michael; Popper, Nathaniel (24 September 2018). \"From Farm to Blockchain: Walmart Tracks Its Lettuce\" . The New York Times . Archived from the original on 5 December 2018 . Retrieved 5 December 2018 . ^ \"Blockchain basics: Utilizing blockchain to improve sustainable supply chains in fashion\" . Strategic Direction . 37 (5): 25– 27. 8 June 2021. doi : 10.1108/SD-03-2021-0028 . ISSN 0258-0543 . S2CID 241322151 . Archived from the original on 27 May 2022 . Retrieved 30 April 2022 . ^ Mercedes-Benz Group AG, Once round the block, please! Archived 24 August 2023 at the Wayback Machine , accessed 24 August 2023 ^ Green, W., Mercedes tests blockchain to improve supply chain transparency Archived 24 August 2023 at the Wayback Machine , Supply Management , published 25 February 2019, accessed 24 August 2023 ^ Mercedes-Benz AG, Mercedes-Benz Cars drives \"Ambition2039\" in the supply chain: blockchain pilot project provides transparency on CO₂ emissions Archived 24 August 2023 at the Wayback Machine , published 30 January 2020, accessed 24 August 2023 ^ a b c Sanders, James (28 August 2019). \"Blockchain-based Unstoppable Domains is a rehash of a failed idea\" . TechRepublic . Archived from the original on 19 November 2019 . Retrieved 16 April 2020 . ^ a b Orcutt, Mike (4 June 2019). \"The ambitious plan to reinvent how websites get their names\" . MIT Technology Review . Archived from the original on 25 May 2022 . Retrieved 17 May 2021 . ^ Cimpanu, Catalin (17 July 2019). \"OpenNIC drops support for .bit domain names after rampant malware abuse\" . ZDNet . Archived from the original on 13 April 2022 . Retrieved 17 May 2021 . ^ \".Kred launches as dual DNS and ENS domain\" . Domain Name Wire | Domain Name News . 6 March 2020. Archived from the original on 8 March 2020 . Retrieved 16 April 2020 . ^ K. Kotobi, and S. G. Bilen, \"Secure Blockchains for Dynamic Spectrum Access : A Decentralized Database in Moving Cognitive Radio Networks Enhances Security and User Access\" , IEEE Vehicular Technology Magazine, 2018. ^ \"Blockchain Could Be Music's Next Disruptor\" . 22 September 2016. Archived from the original on 23 September 2016. ^ Susan Moore. (16 October 2019). \"Digital Business: 4 Ways Blockchain Will Transform Higher Education\". Gartner website Archived 8 April 2021 at the Wayback Machine Retrieved 27 March 2021. ^ \"ASCAP, PRS and SACEM Join Forces for Blockchain Copyright System\" . Music Business Worldwide. 9 April 2017. Archived from the original on 10 April 2017. ^ Burchardi, K.; Harle, N. (20 January 2018). \"The blockchain will disrupt the music business and beyond\" . Wired UK . Archived from the original on 8 May 2018 . Retrieved 8 May 2018 . ^ Bartlett, Jamie (6 September 2015). \"Imogen Heap: saviour of the music industry?\" . The Guardian . Archived from the original on 22 April 2016 . Retrieved 18 June 2016 . ^ Wang, Kevin; Safavi, Ali (29 October 2016). \"Blockchain is empowering the future of insurance\" . Tech Crunch . AOL Inc. Archived from the original on 7 November 2016 . Retrieved 7 November 2016 . ^ Gatteschi, Valentina; Lamberti, Fabrizio; Demartini, Claudio; Pranteda, Chiara; Santamaría, Víctor (20 February 2018). \"Blockchain and Smart Contracts for Insurance: Is the Technology Mature Enough?\" . Future Internet . 10 (2): 20. doi : 10.3390/fi10020020 . ^ \"Blockchain reaction: Tech companies plan for critical mass\" (PDF) . Ernst \u0026 Young. p. 5. Archived (PDF) from the original on 14 November 2016 . Retrieved 13 November 2016 . ^ Carrie Smith. Blockchain Reaction: How library professionals are approaching blockchain technology and its potential impact. Archived 12 September 2019 at the Wayback Machine American Libraries March 2019. ^ \"IBM Blockchain based on Hyperledger Fabric from the Linux Foundation\" . IBM.com . 9 January 2018. Archived from the original on 7 December 2017 . Retrieved 18 January 2018 . ^ Hyperledger (22 January 2019). \"Announcing Hyperledger Grid, a new project to help build and deliver supply chain solutions!\" . Archived from the original on 4 February 2019 . Retrieved 8 March 2019 . ^ Mearian, Lucas (23 January 2019). \"Grid, a new project from the Linux Foundation, will offer developers tools to create supply chain-specific applications running atop distributed ledger technology\" . Computerworld . Archived from the original on 3 February 2019 . Retrieved 8 March 2019 . ^ \"Why J.P. Morgan Chase Is Building a Blockchain on Ethereum\" . Fortune . Archived from the original on 2 February 2017 . Retrieved 24 January 2017 . ^ Andoni, Merlinda; Robu, Valentin; Flynn, David; Abram, Simone; Geach, Dale; Jenkins, David; McCallum, Peter; Peacock, Andrew (2019). \"Blockchain technology in the energy sector: A systematic review of challenges and opportunities\" . Renewable and Sustainable Energy Reviews . 100 : 143– 174. Bibcode : 2019RSERv.100..143A . doi : 10.1016/j.rser.2018.10.014 . S2CID 116422191 . ^ \"This Blockchain-Based Energy Platform Is Building A Peer-To-Peer Grid\" . Fast Company . 16 October 2017. Archived from the original on 7 June 2020 . Retrieved 7 June 2020 . ^ \"Blockchain-based microgrid gives power to consumers in New York\" . Archived from the original on 22 March 2016 . Retrieved 7 June 2020 . ^ Alassaf, Norah; Gutub, Adnan; Parah, Shabir A.; Al Ghamdi, Manal (December 2019). \"Enhancing speed of SIMON: A light-weight-cryptographic algorithm for IoT applications\" . Multimedia Tools and Applications . 78 (23): 32633– 32657. doi : 10.1007/s11042-018-6801-z . ISSN 1380-7501 . S2CID 254858308 . ^ Khan, Safiullah; Lee, Wai-Kong; Hwang, Seong Oun (2021). \"AEchain: A lightweight blockchain for IoT applications\". IEEE Consumer Electronics Magazine . 11 (2): 64– 76. doi : 10.1109/MCE.2021.3060373 . ISSN 2162-2256 . S2CID 234325223 . ^ Ma, Jinhua; Lin, Shih-Ya; Chen, Xin; Sun, Hung-Min; Chen, Yeh-Cheng; Wang, Huaxiong (2020). \"A Blockchain-Based Application System for Product Anti-Counterfeiting\" . IEEE Access . 8 : 77642– 77652. Bibcode : 2020IEEEA...877642M . doi : 10.1109/ACCESS.2020.2972026 . hdl : 10356/145745 . ISSN 2169-3536 . S2CID 214205788 . ^ Alzahrani, Naif; Bulusu, Nirupama (15 June 2018). \"Block-Supply Chain\". Proceedings of the 1st Workshop on Cryptocurrencies and Blockchains for Distributed Systems . CryBlock'18. Munich, Germany: Association for Computing Machinery. pp. 30– 35. doi : 10.1145/3211933.3211939 . ISBN 978-1-4503-5838-5 . S2CID 169188795 . ^ Balagurusamy, V. S. K.; Cabral, C.; Coomaraswamy, S.; Delamarche, E.; Dillenberger, D. N.; Dittmann, G.; Friedman, D.; Gökçe, O.; Hinds, N.; Jelitto, J.; Kind, A. (1 March 2019). \"Crypto anchors\" . IBM Journal of Research and Development . 63 (2/3): 4:1–4:12. doi : 10.1147/JRD.2019.2900651 . ISSN 0018-8646 . S2CID 201109790 . Archived from the original on 11 January 2021 . Retrieved 9 January 2021 . ^ Jung, Seung Wook (23 June 2021). Lee, Robert (ed.). A Novel Authentication System for Artwork Based on Blockchain (eBook) . 20th IEEE/ACIS International Summer Semi-Virtual Conference on Computer and Information Science (ICIS 2021). Studies in Computational Intelligence. Vol. 985. Springer. p. 159. Archived from the original on 26 August 2023 . Retrieved 8 July 2023 – via Google Books . ^ Brett, Charles (18 April 2018). \"EUIPO Blockathon Challenge 2018 -\" . Enterprise Times . Archived from the original on 11 January 2021 . Retrieved 1 September 2020 . ^ \"EUIPO Anti-Counterfeiting Blockathon Forum\" . Archived from the original on 11 January 2021 . Retrieved 9 January 2021 . ^ \"PT Industrieel Management\" . PT Industrieel Management . Archived from the original on 24 May 2022 . Retrieved 1 September 2020 . ^ \"China selects pilot zones, application areas for blockchain project\" . Reuters . 31 January 2022. Archived from the original on 7 June 2022 . Retrieved 7 June 2022 . ^ Šimalčík, Matej (2023). \"Rule by Law\". In Kironska, Kristina; Turscanyi, Richard Q. (eds.). Contemporary China: a New Superpower? . Routledge . ISBN 978-1-03-239508-1 . ^ Wegner, Peter (March 1996). \"Interoperability\" . ACM Computing Surveys . 28 : 285– 287. doi : 10.1145/234313.234424 . ^ Belchior, Rafael; Vasconcelos, André; Guerreiro, Sérgio; Correia, Miguel (May 2020). \"A Survey on Blockchain Interoperability: Past, Present, and Future Trends\". arXiv : 2005.14282 [ cs.DC ]. ^ Hardjono, T.; Hargreaves, M.; Smith, N. (2 October 2020). An Interoperability Architecture for Blockchain Gateways (Technical report). IETF. draft-hardjono-blockchain-interop-arch-00. ^ Hyun Song Shin (June 2018). \"Chapter V. Cryptocurrencies: looking beyond the hype\" (PDF) . BIS 2018 Annual Economic Report . Bank for International Settlements. Archived (PDF) from the original on 18 June 2018 . Retrieved 19 June 2018 . Put in the simplest terms, the quest for decentralised trust has quickly become an environmental disaster. ^ Janda, Michael (18 June 2018). \"Cryptocurrencies like bitcoin cannot replace money, says Bank for International Settlements\" . ABC (Australia). Archived from the original on 18 June 2018 . Retrieved 18 June 2018 . ^ Hiltzik, Michael (18 June 2018). \"Is this scathing report the death knell for bitcoin?\" . Los Angeles Times . Archived from the original on 18 June 2018 . Retrieved 19 June 2018 . ^ Ossinger, Joanna (2 February 2022). \"Polkadot Has Least Carbon Footprint, Crypto Researcher Says\" . Bloomberg . Archived from the original on 12 April 2022 . Retrieved 1 June 2022 . ^ Jones, Jonathan Spencer (13 September 2021). \"Blockchain proof-of-stake – not all are equal\" . www.smart-energy.com . Archived from the original on 25 September 2021 . Retrieved 26 February 2022 . ^ Criddle, Christina (February 20, 2021) \"Bitcoin consumes 'more electricity than Argentina'.\" Archived 1 May 2021 at the Wayback Machine BBC News. (Retrieved April 26, 2021.) ^ Ponciano, Jonathan (March 9, 2021) \"Bill Gates Sounds Alarm On Bitcoin's Energy Consumption–Here's Why Crypto Is Bad For Climate Change.\" Archived 11 July 2021 at the Wayback Machine Forbes.com. (Retrieved April 26, 2021.) ^ Rowlatt, Justin (February 27, 2021) \"How Bitcoin's vast energy use could burst its bubble.\" Archived 30 April 2021 at the Wayback Machine BBC News. (Retrieved April 26, 2021.) ^ Sorkin, Andrew et al. (March 9, 2021) \"Why Bill Gates Is Worried About Bitcoin.\" Archived 30 April 2021 at the Wayback Machine New York Times . (Retrieved April 25, 2021.) ^ Illing, Sean (11 April 2018). \"Why Bitcoin is bullshit, explained by an expert\" . Vox . Archived from the original on 17 July 2018 . Retrieved 17 July 2018 . ^ Weaver, Nicholas (20 April 2018). \"Blockchains and Cryptocurrencies: Burn It With Fire\" . YouTube video . Berkeley School of Information. Archived from the original on 19 February 2019 . Retrieved 17 July 2018 . ^ Köhler, Susanne; Pizzol, Massimo (20 November 2019). \"Life Cycle Assessment of Bitcoin Mining\" . Environmental Science \u0026 Technology . 53 (23): 13598– 13606. Bibcode : 2019EnST...5313598K . doi : 10.1021/acs.est.9b05687 . PMID 31746188 . ^ Stoll, Christian; Klaaßen, Lena; Gallersdörfer, Ulrich (2019). \"The Carbon Footprint of Bitcoin\" . Joule . 3 (7): 1647– 1661. Bibcode : 2019Joule...3.1647S . doi : 10.1016/j.joule.2019.05.012 . ^ \"US lawmakers begin probe into Bitcoin miners' high energy use\" . Business Standard India . 29 January 2022. Archived from the original on 21 May 2022 . Retrieved 21 May 2022 – via Business Standard. ^ Cuen, Leigh (March 21, 2021) \"The debate about cryptocurrency and data consumption.\" Archived 10 November 2021 at the Wayback Machine TechCrunch. (Retrieved April 26, 2021.) ^ \"The Ethereum Merge: What It Is and Why It Is Important | Insights | Holland \u0026 Knight\" . www.hklaw.com . Retrieved 5 December 2024 . ^ Catalini, Christian; Tucker, Catherine E. (11 August 2016). \"Seeding the S-Curve? The Role of Early Adopters in Diffusion\" (PDF) . SSRN . doi : 10.2139/ssrn.2822729 . S2CID 157317501 . SSRN 2822729 . Archived (PDF) from the original on 30 December 2021 . Retrieved 22 December 2021 . ^ Arnold, Martin (18 June 2017). \"Arnold, M. (2017) \"Universities add blockchain to course list\", Financial Times: Masters in Finance, Retrieved 26 January 2022\" . Financial Times . Archived from the original on 26 January 2022 . Retrieved 26 January 2022 . ^ Janssen, Marijn; Weerakkody, Vishanth; Ismagilova, Elvira; Sivarajah, Uthayasankar; Irani, Zahir (2020). \"A framework for analysing blockchain technology adoption: Integrating institutional, market and technical factors\" . International Journal of Information Management . 50 . Elsevier: 302– 309. doi : 10.1016/j.ijinfomgt.2019.08.012 . hdl : 10454/17358 . S2CID 203129604 . ^ Koens, Tommy; Poll, Erik (2019), \"The Drivers Behind Blockchain Adoption: The Rationality of Irrational Choices\", Euro-Par 2018: Parallel Processing Workshops , Lecture Notes in Computer Science, vol. 11339, pp. 535– 546, doi : 10.1007/978-3-030-10549-5_42 , hdl : 2066/200787 , ISBN 978-3-030-10548-8 , S2CID 57662305 ^ Li, Jerry (7 April 2020). \"Blockchain Technology Adoption: Examining the Fundamental Drivers\" . Proceedings of the 2020 2nd International Conference on Management Science and Industrial Engineering . Association for Computing Machinery. pp. 253– 260. doi : 10.1145/3396743.3396750 . ISBN 9781450377065 . S2CID 218982506 . Archived from the original on 5 June 2020 – via ACM Digital Library. ^ Hsieh, Ying-Ying; Vergne, Jean-Philippe; Anderson, Philip; Lakhani, Karim; Reitzig, Markus (12 February 2019). \"Correction to: Bitcoin and the rise of decentralized autonomous organizations\" . Journal of Organization Design . 8 (1): 3. doi : 10.1186/s41469-019-0041-1 . hdl : 10044/1/88332 . ISSN 2245-408X . ^ Felin, Teppo; Lakhani, Karim (2018). \"What Problems Will You Solve With Blockchain?\". MIT Sloan Management Review . ^ Beck, Roman; Mueller-Bloch, Christoph; King, John Leslie (2018). \"Governance in the Blockchain Economy: A Framework and Research Agenda\" . Journal of the Association for Information Systems : 1020– 1034. doi : 10.17705/1jais.00518 . S2CID 69365923 . ^ Popper, Nathaniel (27 June 2018). \"What is the Blockchain? Explaining the Tech Behind Cryptocurrencies (Published 2018)\" . The New York Times . Archived from the original on 27 October 2020 . Retrieved 24 October 2020 . ^ Hugh Rooney, Brian Aiken, \u0026 Megan Rooney. (2017). Q\u0026A. Is Internal Audit Ready for Blockchain? Technology Innovation Management Review , (10), 41. ^ Richard C. Kloch, Jr Simon J. Little, Blockchain and Internal Audit Internal Audit Foundation, 2019 ISBN 978-1-63454-065-0 ^ Alexander, A. (2019). The audit, transformed: New advancements in technology are reshaping this core service. Accounting Today , 33(1) ^ Franco, Pedro (21 October 2014). Understanding Bitcoin: Cryptography, Engineering and Economics . John Wiley \u0026 Sons . p. 74. ISBN 978-1-119-01914-5 . ^ Badr, Bellaj; Horrocks, Richard; Wu, Xun (Brian) (30 November 2018). Blockchain By Example: A developer's guide to creating decentralized applications using Bitcoin, Ethereum, and Hyperledger . Packt Publishing Ltd. p. 19. ISBN 978-1-78847-391-0 . ^ Hu, Yao-Chieh; Lee, Ting-Ting; Chatzopoulos, Dimitris; Hui, Pan (15 June 2018). \"Hierarchical interactions between Ethereum smart contracts across Testnets\" . Proceedings of the 1st Workshop on Cryptocurrencies and Blockchains for Distributed Systems . Association for Computing Machinery. pp. 7– 13. doi : 10.1145/3211933.3211935 . ISBN 9781450358385 . S2CID 134234291 . ^ Canellis, David (27 September 2018). \"Bitcoin hard fork nightmare becomes reality after testnet splits\" . TNW | Hardfork . Retrieved 10 October 2022 . ^ \"What is a Mainnet?\" . Investopedia . Retrieved 2 February 2025 . ^ \"Mainnet Launch: What It Means\" . Decrypt . Retrieved 2 February 2025 . ^ Extance, Andy (30 September 2015). \"The future of cryptocurrencies: Bitcoin and beyond\" . Nature . 526 (7571): 21– 23. Bibcode : 2015Natur.526...21E . doi : 10.1038/526021a . ISSN 0028-0836 . OCLC 421716612 . PMID 26432223 . ^ Ledger (eJournal / eMagazine, 2015) . OCLC. OCLC 910895894 . ^ Hertig, Alyssa (15 September 2015). \"Introducing Ledger, the First Bitcoin-Only Academic Journal\" . Motherboard . Archived from the original on 10 January 2017 . Retrieved 10 January 2017 . ^ Rizun, Peter R.; Wilmer, Christopher E.; Burley, Richard Ford; Miller, Andrew (2015). \"How to Write and Format an Article for Ledger\" (PDF) . Ledger . 1 (1): 1– 12. doi : 10.5195/LEDGER.2015.1 (inactive 1 July 2025). ISSN 2379-5980 . OCLC 910895894 . Archived (PDF) from the original on 22 September 2015 . Retrieved 11 January 2017 . {{ cite journal }} : CS1 maint: DOI inactive as of July 2025 ( link ) ^ \"Why is Git not considered a \"block chain\"?\" . Stack Overflow . Retrieved 2 April 2025 .\nCrosby, Michael; Nachiappan; Pattanayak, Pradhan; Verma, Sanjeev; Kalyanaraman, Vignesh (16 October 2015). BlockChain Technology: Beyond Bitcoin (PDF) (Report). Sutardja Center for Entrepreneurship \u0026 Technology Technical Report. University of California, Berkeley. Archived (PDF) from the original on 20 March 2017 . Retrieved 19 March 2017 . Jaikaran, Chris (28 February 2018). Blockchain: Background and Policy Issues . Washington, DC: Congressional Research Service. Archived from the original on 2 December 2018 . Retrieved 2 December 2018 . Kakavand, Hossein; De Sevres, Nicolette Kost; Chilton, Bart (12 October 2016). The Blockchain Revolution: An Analysis of Regulation and Technology Related to Distributed Ledger Technologies (Report). Luther Systems \u0026 DLA Piper. SSRN 2849251 . Mazonka, Oleg (29 December 2016). \"Blockchain: Simple Explanation\" (PDF) . Journal of Reference . Archived from the original (PDF) on 7 August 2017 . Retrieved 29 December 2016 . Tapscott, Don; Tapscott, Alex (2016). Blockchain Revolution: How the Technology Behind Bitcoin Is Changing Money, Business and the World . London: Portfolio Penguin. ISBN 978-0-241-23785-4 . OCLC 971395169 . Saito, Kenji; Yamada, Hiroyuki (June 2016). What's So Different about Blockchain? Blockchain is a Probabilistic State Machine . IEEE 36th International Conference on Distributed Computing Systems Workshops. Nara, Nara, Japan: IEEE. pp. 168– 75. doi : 10.1109/ICDCSW.2016.28 . ISBN 978-1-5090-3686-8 . ISSN 2332-5666 . Raval, Siraj (2016). Decentralized Applications: Harnessing Bitcoin's Blockchain Technology . Oreilly. ISBN 9781491924549 . Archived from the original on 30 May 2017 . Retrieved 2 May 2017 . Bashir, Imran (2017). Mastering Blockchain . Packt Publishing, Ltd. ISBN 978-1-78712-544-5 . OCLC 967373845 . Knirsch, Fabian; Unterweger, Andread; Engel, Dominik (2019). \"Implementing a blockchain from scratch: why, how, and what we learned\" . EURASIP Journal on Information Security . 2019 2. doi : 10.1186/s13635-019-0085-3 . S2CID 84837476 .\nD. Puthal, N. Malik,\n, E. Kougianos, and G. Das, \"\nEverything you Wanted to Know about the Blockchain\nIEEE Consumer Electronics Magazine\n, Volume 7, Issue 4, July 2018, pp. 6–14.\nDavid L. Portilla, David J. Kappos, Minh Van Ngo, Sasha Rosenthal-Larrea, John D. Buretta and Christopher K. Fargo, Cravath, Swaine \u0026 Moore LLP, \"\nBlockchain in the Banking Sector: A Review of the Landscape and Opportunities\nHarvard Law School of Corporate Governance\n, posted on Friday, January 28, 2022\nWikiversity has learning resources about\nat Wikimedia Commons\nv t e Cryptocurrencies\nBlockchain Cryptocurrency tumbler Cryptocurrency wallet Cryptographic hash function Decentralized exchange Decentralized finance Distributed ledger Fork Lightning Network MetaMask Smart contract Web3 Consensus\nProof of authority Proof of space Proof of stake Proof of work Proof of work\nBitcoin Bitcoin Cash Counterparty LBRY MazaCoin Namecoin Peercoin Titcoin Ethash\nEthereum Classic Scrypt\nAuroracoin Bitconnect Coinye Dogecoin Litecoin Equihash\nAmbaCoin Firo IOTA Nervos Network Primecoin Verge Vertcoin Proof of stake\nAlgorand Avalanche Cardano EOS.IO Ethereum\nGridcoin ICON Injective Kin Nxt Peercoin Polkadot Solana Steem Tezos TON ERC-20\nAugur Aventus Basic Attention Token Chainlink Kin KodakCoin Minds Polygon Shiba Inu The DAO TRON Stablecoins Dai Diem Pax Terra Tether USD Coin Meme coins CAR Chill Guy Coinye Dogecoin LGBcoin Libra Melania Pawthereum Trump\nChia Filecoin HBAR (Hashgraph) Helium Luna MobileCoin Nano NEO SafeMoon Stellar WhopperCoin XRP Ledger Inactive currencies BitConnect Coinye KodakCoin OneCoin Petro\nCrypto service companies\nHyperledger IQ.Wiki Initiative Q\n$Libra cryptocurrency scandal 2023 United States banking crisis Airdrop Bitcoin in El Salvador BitLicense Blockchain game Complementary currency Crypto-anarchy Cryptocurrencies in Europe Cryptocurrencies in Puerto Rico Cryptocurrency bubble Cryptocurrency in Australia Cryptocurrency in Nigeria Cryptocurrency scams Digital currency Decentralized autonomous organization Decentralized application Distributed ledger technology law Double-spending Environmental impact Initial coin offering Initial exchange offering List of cryptocurrencies Non-fungible token Token money United States Strategic Bitcoin Reserve Virtual currency Category Commons List v t e Bitcoin History Economics Politics Legal status Environmental effects\nGavin Andresen Andreas Antonopoulos Brian Armstrong Adam Back Wences Casares Tim Draper Hal Finney Wei Dai Mark Karpelès Satoshi Nakamoto Charlie Shrem Nick Szabo Vitalik Buterin Ross Ulbricht Roger Ver Cody Wilson Cameron Winklevoss Tyler Winklevoss Craig Wright Dave Kleiman Jihan Wu Erik Voorhees\nList of bitcoin companies List of bitcoin forks List of bitcoin organizations List of people in blockchain technology\nBitcoin network Blockchain Cryptocurrency Cryptocurrency wallet Bitcoin ATM ECDSA Lightning Network P2P Proof of work SegWit SHA-2 Forks\nBitcoin Cash Bitcoin Gold\nBitcoin scalability problem History of bitcoin 2013 Bitcoin buried in Newport landfill 2018 cryptocurrency crash 2018 Bitcoin bomb threats 2020 Twitter account hijacking\nThe Rise and Rise of Bitcoin\n(not exchanges) Bitcoin Foundation Bitcoin Magazine BitGo Bitmain Canaan Creative CoinDesk GHash.io Nuri Strategy Bitcoin in El Salvador Bitcoin Law Bitcoin Beach Bitcoin City Category Commons v t e Database management systems\nObject-oriented comparison Relational list comparison Key–value Column-oriented list Document-oriented Wide-column store Graph NoSQL NewSQL In-memory list Multi-model comparison Cloud Blockchain-based database\nDatabase ACID Armstrong's axioms Codd's 12 rules CAP theorem CRUD Null Candidate key Foreign key PACELC design principle Superkey Surrogate key Unique key\nRelation table column row View Transaction Transaction log Trigger Index Stored procedure Cursor Partition\nConcurrency control Data dictionary JDBC XQJ ODBC Query language Query optimizer Query rewriting system Query plan\nAdministration Query optimization Replication Sharding\nDatabase models Database normalization\nDistributed database Federated database system Referential integrity Relational algebra Relational calculus Relational model Object–relational database Transaction processing Category Outline Authority control databases\nUnited States France BnF data Czech Republic Israel\nhttps://en.wikipedia.org/w/index.php?title=Blockchain\u0026oldid=1304401302\nBitcoin Blockchains Cryptocurrencies Database models Financial metadata Computer-related introductions in 2009 Information systems Decentralization 21st-century inventions Database management systems\nWebarchive template wayback links CS1: long volume value CS1 maint: location missing publisher CS1 maint: DOI inactive as of July 2025 Articles with short description Short description matches Wikidata Wikipedia indefinitely semi-protected pages Use dmy dates from May 2019 All articles with unsourced statements Articles with unsourced statements from June 2022 Articles with excerpts Articles containing potentially dated statements from April 2018 All articles containing potentially dated statements All pages needing factual verification Wikipedia articles needing factual verification from January 2022 All articles with self-published sources Articles with self-published sources from April 2018 All articles lacking reliable references Articles lacking reliable references from November 2021 Wikipedia articles needing clarification from October 2021 Wikipedia articles in need of updating from August 2023 All Wikipedia articles in need of updating Commons category link from Wikidata Search Search Blockchain 86 languages Add topic",
          "role": "teacher"
        },
        {
          "content": "Why is this important to understand?",
          "role": "student"
        },
        {
          "content": "Understanding Blockchain is crucial for building efficient, scalable, and maintainable software systems. It forms the foundation for solving complex computational problems.",
          "role": "teacher"
        },
        {
          "content": "Can you give me a concrete example?",
          "role": "student"
        },
        {
          "content": "For example, Blockchain can be applied in scenarios where you need to optimize performance, manage complexity, or implement scalable solutions.",
          "role": "teacher"
        }
      ],
      "format": "socratic_dialogue",
      "metadata": {
        "domain": "Technology",
        "source": "https://en.wikipedia.org/wiki/Blockchain",
        "words": 15597
      },
      "topic": "Blockchain"
    },
    {
      "content": {
        "applications": "applications\nand blockchains that are readable by the public and are widely used by\n. The blockchain may be considered a type of\nPrivate blockchains have been proposed for business use.\ncalled the marketing of such privatized blockchains without a proper security model \"\nhowever, others have argued that permissioned blockchains, if carefully designed, may be more decentralized and therefore more secure in practice than permissionless ones.\nCryptographer David Chaum\nfirst proposed a blockchain-li",
        "considerations": "considered\nand exemplify a distributed computing system with high\nByzantine fault tolerance\nA blockchain was created by a person (or group of people) using the name (or\nin 2008 to serve as the public\nbitcoin cryptocurrency\ntransactions, based on previous work by\nThe implementation of the blockchain within bitcoin made it the first digital currency to solve the\nproblem without the need for a truste",
        "key_concepts": [
          "based proof-of-stake protocols purport to provide so called \"absolute finality\": a randomly chosen",
          "crypto exchanges. The development, some argue, has led criminals to prioritise the use of new cryptos such as"
        ],
        "overview": "A comprehensive technical concept with wide applications."
      },
      "format": "conceptual_breakdown",
      "metadata": {
        "domain": "Technology",
        "structure": "hierarchical"
      },
      "topic": "Blockchain"
    },
    {
      "format": "problem_solving",
      "scenario": {
        "approach": "solve the\nproblem without the need for a trusted authority or central\ndesign has inspired other applications\nand blockchains that are readable by the public and are widely used by\n. The blockchain may be considered a type of\nPrivate blockchains have been proposed for business use.\ncalled the marketing of such privatized blockchains without a proper security model \"\nhowever, others have argued that permissioned blockchains, if carefully designed, may be more decentralized and therefore more secur",
        "benefits": "benefit from blockchains because they involve many collaborating peers.\nThe use of blockchain in libraries is being studied with a grant from the U.S. Institute of Museum and Library Services.\nOther blockchain designs include\n, a collaborative effort from the\nto support blockchain-based distributed ",
        "problem": "How can Blockchain solve real-world problems?",
        "tradeoffs": "Disadvantages of permissioned blockchain\nNikolai Hampton argued in\nthat \"There is also no need for a '51 percent' attack on a private blockchain, as the private blockchain (most likely) already controls 100 percent of all block creation resources. If you could attack or damage the blockchain creatio"
      },
      "topic": "Blockchain"
    },
    {
      "comparison": "Compared to decentralized blockchains, centralized blockchains normally can provide a higher throughput and lower latency of transactions than consensus-based distributed blockchains.\nCurrently, there are at least four types of blockchain networks — public blockchains, private blockchains,\nblockchains and hybrid blockchains.\nA public blockchain has absolutely no access restrictions. Anyone with an\nto it as well as become a validator (i.e., participate in the execution of a\n[ 71 ] [ self-publis",
      "format": "comparative_analysis",
      "topic": "Blockchain"
    },
    {
      "dialogue": [
        {
          "content": "What is Quantum Computing really about at its core?",
          "role": "student"
        },
        {
          "content": "Computer hardware technology that uses quantum mechanics\nBloch sphere representation of a qubit. The state | ψ ⟩ = α | 0 ⟩ + β | 1 ⟩ {\\displaystyle |\\psi \\rangle =\\alpha |0\\rangle +\\beta |1\\rangle } is a point on the surface of the sphere, partway between the poles, | 0 ⟩ {\\displaystyle |0\\rangle } and | 1 ⟩ {\\displaystyle |1\\rangle } .\nis a (real or theoretical)\nphenomena in an essential way: a quantum computer exploits\nand the (non-deterministic) outcomes of\nquantum measurements\nas features of its computation. Ordinary (\"classical\") computers operate, by contrast, using deterministic rules. Any classical computer can, in principle, be replicated using a (\n) mechanical device such as a\n, with at most a constant-factor slowdown in time—unlike quantum computers, which are believed to require\nmore resources to simulate classically. It is widely believed that a scalable quantum computer could perform\ncalculations exponentially faster than any classical computer. Theoretically, a large-scale quantum computer could\nbreak some widely used encryption schemes\nand aid physicists in performing\nphysical simulations\n. However, current hardware implementations of quantum computation are largely experimental and only suitable for specialized tasks.\nin quantum computing, the\n(or \"quantum bit\"), serves the same function as the\nin ordinary or \"classical\" computing.\nHowever, unlike a classical bit, which can be in one of two states (a\n), a qubit can exist in a\nof its two \"basis\" states, a state that is in an abstract sense \"between\" the two basis states. When\na qubit, the result is a\nprobabilistic output\nof a classical bit. If a quantum computer manipulates the qubit in a particular way,\neffects can amplify the desired measurement results. The design of\ninvolves creating procedures that allow a quantum computer to perform calculations efficiently and quickly.\nQuantum computers are not yet practical for real-world applications. Physically engineering high-quality qubits has proven to be challenging. If a physical qubit is not sufficiently\nfrom its environment, it suffers from\ninto calculations. National governments have invested heavily in experimental research aimed at developing scalable qubits with longer coherence times and lower error rates. Example implementations include\nelectrical resistance\n(which confine a single\nelectromagnetic fields\n). Researchers have demonstrated that certain quantum devices can outperform classical computers on narrowly defined tasks, a milestone referred to as quantum advantage (previously\n), though these tasks are not necessarily useful for real-world applications.\nFor a chronological guide, see\nTimeline of quantum computing and communication\nFor many years, the fields of\nformed distinct academic communities.\n[ 2 ] Modern quantum theory\ndeveloped in the 1920s to explain perplexing physical phenomena observed at atomic scales,\nemerged in the following decades to replace\nfor tedious calculations.\nBoth disciplines had practical applications during\n; computers played a major role in\nwartime cryptography\nand quantum physics was essential for\napplied quantum mechanical models to computational problems and swapped digital\n, the fields of quantum mechanics and computer science began to converge. In 1980,\nquantum Turing machine\n, which uses quantum theory to describe a simplified computer.\nWhen digital computers became faster, physicists faced an\nincrease in overhead when\nsimulating quantum dynamics\nto independently suggest that hardware based on quantum phenomena might be more efficient for computer simulation.\n[ 10 ] [ 11 ] [ 12 ]\napplied quantum theory to\nprotocols and demonstrated that quantum key distribution could enhance\ninformation security\n[ 13 ] [ 14 ] Quantum algorithms\nthen emerged for solving\nBernstein–Vazirani algorithm\nThese algorithms did not solve practical problems, but demonstrated mathematically that one could gain more information by querying a\nwith a quantum state in\n, sometimes referred to as\n[ 18 ] Peter Shor (pictured here in 2017) showed in 1994 that a scalable quantum computer would be able to break RSA encryption . Peter Shor\nbuilt on these results with\nfor breaking the widely used\nencryption protocols,\nwhich drew significant attention to the field of quantum computing. In 1996,\nestablished a quantum speedup for the widely applicable\nproved that quantum computers could simulate quantum systems without the exponential overhead present in classical simulations,\nvalidating Feynman's 1982 conjecture.\nhave constructed small-scale quantum computers using\nand superconductors.\nIn 1998, a two-qubit quantum computer demonstrated the feasibility of the technology,\nand subsequent experiments have increased the number of qubits and reduced error rates.\nannounced that they had achieved\nwith a 54-qubit machine, performing a computation that is impossible for any classical computer.\n[ 27 ] [ 28 ] [ 29 ] [ 30 ]\nThis announcement was met with a rebuttal from Google's direct competitor, IBM. IBM contended that the calculation Google claimed would take 10,000 years could be performed in just 2.5 days on its own Summit supercomputer if its architecture were optimized, sparking a debate over the precise threshold for \"quantum supremacy\".\nQuantum information processing\nIntroduction to quantum mechanics Computer engineers\ntypically describe a\n's operation in terms of\nclassical electrodynamics\nWithin these \"classical\" computers, some components (such as\nrandom number generators\n) may rely on quantum behavior, but these components are not\nfrom their environment, so any\nrandomized algorithm\n, quantum mechanical notions like superposition and\nare largely irrelevant for\n, in contrast, rely on precise control of\nquantum systems. Physicists\ndescribe these systems mathematically\nprobability amplitudes\nmodel the operations that can be performed on these states. Programming a quantum computer is then a matter of\noperations in such a way that the resulting program computes a useful result in theory and is implementable in practice.\ndescribes the relationship between quantum and classical computers,\nA classical computer is a quantum computer ... so we shouldn't be asking about \"where do quantum speedups come from?\" We should say, \"well, all computers are quantum. ... Where do classical slowdowns come from?\"\nJust as the bit is the basic concept of classical information theory, the\nis the fundamental unit of\nis used to refer to an abstract mathematical model and to any physical system that is represented by that model. A classical bit, by definition, exists in either of two physical states, which can be denoted 0 and 1. A qubit is also described by a state, and two states often written\n| 0 ⟩ {\\displaystyle |0\\rangle }\n| 1 ⟩ {\\displaystyle |1\\rangle }\nserve as the quantum counterparts of the classical states 0 and 1. However, the quantum states\n| 0 ⟩ {\\displaystyle |0\\rangle }\n| 1 ⟩ {\\displaystyle |1\\rangle }\n, meaning that they can be multiplied by constants and added together, and the result is again a valid quantum state. Such a combination is known as a\n| 0 ⟩ {\\displaystyle |0\\rangle }\n| 1 ⟩ {\\displaystyle |1\\rangle }\nmathematically represents a qubit state. Physicists typically use\nfor quantum mechanical\n| ψ ⟩ {\\displaystyle |\\psi \\rangle } ' ket psi '\nfor a vector labeled\nψ {\\displaystyle \\psi }\n. Because a qubit is a two-state system, any qubit state takes the form\nα | 0 ⟩ + β | 1 ⟩ {\\displaystyle \\alpha |0\\rangle +\\beta |1\\rangle }\n| 0 ⟩ {\\displaystyle |0\\rangle }\n| 1 ⟩ {\\displaystyle |1\\rangle }\nα {\\displaystyle \\alpha }\nβ {\\displaystyle \\beta }\nprobability amplitudes ,\nwhich are in general\nα {\\displaystyle \\alpha }\nβ {\\displaystyle \\beta }\nis zero, the qubit is effectively a classical bit; when both are nonzero, the qubit is in superposition. Such a\nquantum state vector\nacts similarly to a (classical)\n, with one key difference: unlike probabilities, probability\nare not necessarily positive numbers.\nNegative amplitudes allow for destructive wave interference.\n, the result is a classical bit. The\ncorrespondence between amplitudes and probabilities—when measuring a qubit\nα | 0 ⟩ + β | 1 ⟩ {\\displaystyle \\alpha |0\\rangle +\\beta |1\\rangle }\n| 0 ⟩ {\\displaystyle |0\\rangle }\n| α | 2 {\\displaystyle |\\alpha |^{2}}\n| 1 ⟩ {\\displaystyle |1\\rangle }\n| β | 2 {\\displaystyle |\\beta |^{2}}\nAny valid qubit state has coefficients\nα {\\displaystyle \\alpha }\nβ {\\displaystyle \\beta }\n| α | 2 + | β | 2 = 1 {\\displaystyle |\\alpha |^{2}+|\\beta |^{2}=1}\nAs an example, measuring the qubit\n1 / 2 | 0 ⟩ + 1 / 2 | 1 ⟩ {\\displaystyle 1/{\\sqrt {2}}|0\\rangle +1/{\\sqrt {2}}|1\\rangle }\nwould produce either\n| 0 ⟩ {\\displaystyle |0\\rangle }\n| 1 ⟩ {\\displaystyle |1\\rangle }\nwith equal probability.\nEach additional qubit doubles the\nAs an example, the vector\n⁠ 1 / √2 ⁠ |00⟩ + ⁠ 1 / √2 ⁠ |01⟩\nrepresents a two-qubit state, a\n⁠ 1 / √2 ⁠ |0⟩ + ⁠ 1 / √2 ⁠ |1⟩\nThis vector inhabits a four-dimensional\nspanned by the basis vectors\nBell state ⁠ 1 / √2 ⁠ |00⟩ + ⁠ 1 / √2 ⁠ |11⟩\nis impossible to decompose into the tensor product of two individual qubits—the two qubits are\nbecause neither qubit has a state vector of its own.\nIn general, the vector space for an\n-dimensional, and this makes it challenging for a classical computer to simulate a quantum one: representing a 100-qubit system requires storing 2\nThe state of this one-qubit\ncan be manipulated by applying\n, analogous to how classical memory can be manipulated with\nclassical logic gates\n. One important gate for both classical and quantum computation is the NOT gate, which can be represented by a\nmatrix X := ( 0 1 1 0 ) . {\\displaystyle X:={\\begin{pmatrix}0\u00261\\\\1\u00260\\end{pmatrix}}.}\nMathematically, the application of such a logic gate to a quantum state vector is modelled with\nmatrix multiplication\nX | 0 ⟩ = | 1 ⟩ {\\displaystyle X|0\\rangle =|1\\rangle }\nX | 1 ⟩ = | 0 ⟩ {\\displaystyle X|1\\rangle =|0\\rangle }\nThe mathematics of single qubit gates can be extended to operate on multi-qubit quantum memories in two important ways. One way is simply to select a qubit and apply that gate to the target qubit while leaving the remainder of the memory unaffected. Another way is to apply the gate to its target only if another part of the memory is in a desired state. These two choices can be illustrated using another example. The possible states of a two-qubit quantum memory are\n| 00 ⟩ := ( 1 0 0 0 ) ; | 01 ⟩ := ( 0 1 0 0 ) ; | 10 ⟩ := ( 0 0 1 0 ) ; | 11 ⟩ := ( 0 0 0 1 ) . {\\displaystyle |00\\rangle :={\\begin{pmatrix}1\\\\0\\\\0\\\\0\\end{pmatrix}};\\quad |01\\rangle :={\\begin{pmatrix}0\\\\1\\\\0\\\\0\\end{pmatrix}};\\quad |10\\rangle :={\\begin{pmatrix}0\\\\0\\\\1\\\\0\\end{pmatrix}};\\quad |11\\rangle :={\\begin{pmatrix}0\\\\0\\\\0\\\\1\\end{pmatrix}}.}\ncontrolled NOT (CNOT)\ngate can then be represented using the following matrix:\nCNOT := ( 1 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 ) . {\\displaystyle \\operatorname {CNOT} :={\\begin{pmatrix}1\u00260\u00260\u00260\\\\0\u00261\u00260\u00260\\\\0\u00260\u00260\u00261\\\\0\u00260\u00261\u00260\\end{pmatrix}}.}\nAs a mathematical consequence of this definition,\nCNOT ⁡ | 00 ⟩ = | 00 ⟩ {\\textstyle \\operatorname {CNOT} |00\\rangle =|00\\rangle }\nCNOT ⁡ | 01 ⟩ = | 01 ⟩ {\\textstyle \\operatorname {CNOT} |01\\rangle =|01\\rangle }\nCNOT ⁡ | 10 ⟩ = | 11 ⟩ {\\textstyle \\operatorname {CNOT} |10\\rangle =|11\\rangle }\nCNOT ⁡ | 11 ⟩ = | 10 ⟩ {\\textstyle \\operatorname {CNOT} |11\\rangle =|10\\rangle }\n. In other words, the CNOT applies a NOT gate (\nfrom before) to the second qubit if and only if the first qubit is in the state\n| 1 ⟩ {\\textstyle |1\\rangle }\n. If the first qubit is\n| 0 ⟩ {\\textstyle |0\\rangle }\n, nothing is done to either qubit.\nIn summary, quantum computation can be described as a network of quantum logic gates and measurements. However, any\nmeasurement can be deferred\nto the end of quantum computation, though this deferment may come at a computational cost, so most\ndepict a network consisting only of quantum logic gates and no measurements.\n[ edit ] Quantum parallelism\nis the heuristic that quantum computers can be thought of as evaluating a function for multiple input values simultaneously. This can be achieved by preparing a quantum system in a superposition of input states and applying a unitary transformation that encodes the function to be evaluated. The resulting state encodes the function's output values for all input values in the superposition, allowing for the computation of multiple outputs simultaneously. This property is key to the speedup of many quantum algorithms. However, \"parallelism\" in this sense is insufficient to speed up a computation, because the measurement at the end of the computation gives only one value. To be useful, a quantum algorithm must also incorporate some other conceptual ingredient.\nFurther information:\nThere are a number of\nmodels of computation\nfor quantum computing, distinguished by the basic elements in which the computation is decomposed.\n[ edit ] A quantum circuit diagram implementing a Toffoli gate from more primitive gates\ndecomposes computation into a sequence of few-qubit\n. A quantum computation can be described as a network of quantum logic gates and measurements. However, any measurement can be deferred to the end of quantum computation, though this deferment may come at a computational cost, so most quantum circuits depict a network consisting only of quantum logic gates and no measurements.\nAny quantum computation (which is, in the above formalism, any\n2 n × 2 n {\\displaystyle 2^{n}\\times 2^{n}}\nqubits) can be represented as a network of quantum logic gates from a fairly small family of gates. A choice of gate family that enables this construction is known as a\n, since a computer that can run such circuits is a\nuniversal quantum computer\n. One common such set includes all single-qubit gates as well as the CNOT gate from above. This means any quantum computation can be performed by executing a sequence of single-qubit gates together with CNOT gates. Though this gate set is infinite, it can be replaced with a finite gate set by appealing to the\nSolovay-Kitaev theorem\n. Implementation of Boolean functions using the few-qubit quantum gates is presented here.\nMeasurement-based quantum computing\nmeasurement-based quantum computer\ndecomposes computation into a sequence of\nBell state measurements\napplied to a highly entangled initial state (a\n), using a technique called\nquantum gate teleportation\nAdiabatic quantum computing\nadiabatic quantum computer\n, decomposes computation into a slow continuous transformation of an initial\ninto a final Hamiltonian, whose ground states contain the solution.\nNeuromorphic quantum computing\nNeuromorphic quantum computing (abbreviated as 'n.quantum computing') is an unconventional type of computing that uses\nneuromorphic computing\nto perform quantum operations. It was suggested that quantum algorithms, which are algorithms that run on a realistic model of quantum computation, can be computed equally efficiently with neuromorphic quantum computing. Both traditional quantum computing and neuromorphic quantum computing are physics-based unconventional computing approaches to computations and do not follow the\nvon Neumann architecture\n. They both construct a system (a circuit) that represents the physical problem at hand and then leverage their respective physics properties of the system to seek the \"minimum\". Neuromorphic quantum computing and quantum computing share similar physical properties during computation.\nTopological quantum computing\ntopological quantum computer\ndecomposes computation into the braiding of\nQuantum Turing machine\nquantum Turing machine\nis the quantum analog of a\nAll of these models of computation—quantum circuits,\n[ 42 ] one-way quantum computation\nadiabatic quantum computation,\nand topological quantum computation\n—have been shown to be equivalent to the quantum Turing machine; given a perfect implementation of one such quantum computer, it can simulate all the others with no more than polynomial overhead. This equivalence need not hold for practical quantum computers, since the overhead of simulation may be too large to be practical.\nNoisy intermediate-scale quantum computing\nshows how increasing the number of qubits can mitigate errors,\nyet fully fault-tolerant quantum computing remains \"a rather distant dream\".\nAccording to some researchers,\nnoisy intermediate-scale quantum\n) machines may have specialized uses in the near future, but\nin quantum gates limits their reliability.\nUniversity successfully created \"quantum circuits\" that correct errors more efficiently than alternative methods, which may potentially remove a major obstacle to practical quantum computers.\nThe Harvard research team was supported by\nUniversity and funded by\n's Optimization with Noisy Intermediate-Scale Quantum devices (ONISQ) program.\nQuantum cryptography and cybersecurity\nQuantum computing has significant potential applications in the fields of cryptography and cybersecurity. Quantum cryptography, which leverages the principles of quantum mechanics, offers the possibility of secure communication channels that are fundamentally resistant to eavesdropping. Quantum key distribution (QKD) protocols, such as BB84, enable the secure exchange of cryptographic keys between parties, ensuring the confidentiality and integrity of communication. Additionally, quantum random number generators (QRNGs) can produce high-quality randomness, which is essential for secure encryption.\nAt the same time, quantum computing poses substantial challenges to traditional cryptographic systems. Shor's algorithm, a quantum algorithm for integer factorization, could potentially break widely used public-key encryption schemes like RSA, which rely on the intractability of factoring large numbers. This has prompted a global effort to develop post-quantum cryptography—algorithms designed to resist both classical and quantum attacks. This field remains an active area of research and standardization, aiming to future-proof critical infrastructure against quantum-enabled threats.\nOngoing research in quantum and post-quantum cryptography will be critical for maintaining the integrity of digital infrastructure. Advances such as new QKD protocols, improved QRNGs, and the international standardization of quantum-resistant algorithms will play a key role in ensuring the security of communication and data in the emerging quantum era.\nQuantum computing also presents broader systemic and geopolitical risks. These include the potential to break current encryption protocols, disrupt financial systems, and accelerate the development of dual-use technologies such as advanced military systems or engineered pathogens. As a result, nations and corporations are actively investing in post-quantum safeguards, and the race for quantum supremacy is increasingly shaping global power dynamics.\nFurther information:\nQuantum information science Quantum cryptography\nenables new ways to transmit data securely; for example,\nquantum key distribution\nuses entangled quantum states to establish secure\nWhen a sender and receiver exchange quantum states, they can guarantee that an\ndoes not intercept the message, as any unauthorized eavesdropper would disturb the delicate quantum system and introduce a detectable change.\ncryptographic protocols\n, the sender and receiver can thus establish shared private information resistant to eavesdropping.\ncan transmit quantum information over relatively short distances. Ongoing experimental research aims to develop more reliable hardware (such as quantum repeaters), hoping to scale this technology to long-distance\nwith end-to-end entanglement. Theoretically, this could enable novel technological applications, such as distributed quantum computing and enhanced\ntypically focuses on this quantum circuit model, though exceptions like the\nquantum adiabatic algorithm\nexist. Quantum algorithms can be roughly categorized by the type of speedup achieved over corresponding classical algorithms.\nQuantum algorithms that offer more than a polynomial speedup over the best-known classical algorithm include Shor's algorithm for factoring and the related quantum algorithms for computing\n, and more generally solving the\nhidden subgroup problem\nThese algorithms depend on the primitive of the\nquantum Fourier transform\n. No mathematical proof has been found that shows that an equally fast classical algorithm cannot be discovered, but evidence suggests that this is unlikely.\nCertain oracle problems like\nBernstein–Vazirani problem\ndo give provable speedups, though this is in the\n, which is a restricted model where lower bounds are much easier to prove and doesn't necessarily translate to speedups for practical problems.\nOther problems, including the simulation of quantum physical processes from chemistry and solid-state physics, the approximation of certain\nquantum algorithm for linear systems of equations\n, have quantum algorithms appearing to give super-polynomial speedups and are\n-complete. Because these problems are BQP-complete, an equally fast classical algorithm for them would imply that\nno quantum algorithm\ngives a super-polynomial speedup, which is believed to be unlikely.\nSome quantum algorithms, like\namplitude amplification\n, give polynomial speedups over corresponding classical algorithms.\nThough these algorithms give comparably modest quadratic speedup, they are widely applicable and thus give speedups for a wide range of problems.\nSimulation of quantum systems\nSince chemistry and nanotechnology rely on understanding quantum systems, and such systems are impossible to simulate in an efficient manner classically,\nmay be an important application of quantum computing.\nQuantum simulation could also be used to simulate the behavior of atoms and particles at unusual conditions such as the reactions inside a\nIn June 2023, IBM computer scientists reported that a quantum computer produced better results for a physics problem than a conventional supercomputer.\nAbout 2% of the annual global energy output is used for\nin the agricultural fertilizer industry (even though naturally occurring organisms also produce ammonia). Quantum simulations might be used to understand this process and increase the energy efficiency of production.\nIt is expected that an early use of quantum computing will be modeling that improves the efficiency of the Haber–Bosch process\nalthough some have predicted it will take longer.\nPost-quantum cryptography\nPost-quantum cryptography\nA notable application of quantum computation is for\non cryptographic systems that are currently in use.\nInteger factorization\n, which underpins the security of\npublic key cryptographic\nsystems, is believed to be computationally infeasible with an ordinary computer for large integers if they are the product of few\n(e.g., products of two 300-digit primes).\nBy comparison, a quantum computer could solve this problem exponentially faster using Shor's algorithm to find its factors.\nThis ability would allow a quantum computer to break many of the\nsystems in use today, in the sense that there would be a\n(in the number of digits of the integer) algorithm for solving the problem. In particular, most of the popular\nare based on the difficulty of factoring integers or the\nproblem, both of which can be solved by Shor's algorithm. In particular, the\nelliptic curve Diffie–Hellman\nalgorithms could be broken. These are used to protect secure Web pages, encrypted email, and many other types of data. Breaking these would have significant ramifications for electronic privacy and security.\nIdentifying cryptographic systems that may be secure against quantum algorithms is an actively researched topic under the field of\npost-quantum cryptography\nSome public-key algorithms are based on problems other than the integer factorization and discrete logarithm problems to which Shor's algorithm applies, like the\nMcEliece cryptosystem\nbased on a problem in\n[ 71 ] [ 73 ] Lattice-based cryptosystems\nare also not known to be broken by quantum computers, and finding a polynomial time algorithm for solving the\ndihedral hidden subgroup problem\n, which would break many lattice based cryptosystems, is a well-studied open problem.\nIt has been proven that applying Grover's algorithm to break a\nsymmetric (secret key) algorithm\nby brute force requires time equal to roughly 2\ninvocations of the underlying cryptographic algorithm, compared with roughly 2\nin the classical case,\nmeaning that symmetric key lengths are effectively halved: AES-256 would have the same security against an attack using Grover's algorithm that AES-128 has against classical brute-force search (see\nThe most well-known example of a problem that allows for a polynomial quantum speedup is\n, which involves finding a marked item out of a list of\nitems in a database. This can be solved by Grover's algorithm using\nO ( n ) {\\displaystyle O({\\sqrt {n}})}\nqueries to the database, quadratically fewer than the\nΩ ( n ) {\\displaystyle \\Omega (n)}\nqueries required for classical algorithms. In this case, the advantage is not only provable but also optimal: it has been shown that Grover's algorithm gives the maximal possible probability of finding the desired element for any number of oracle lookups. Many examples of provable quantum speedups for query problems are based on Grover's algorithm, including\nBrassard, Høyer, and Tapp's algorithm\nfor finding collisions in two-to-one functions,\nand Farhi, Goldstone, and Gutmann's algorithm for evaluating NAND trees.\nProblems that can be efficiently addressed with Grover's algorithm have the following properties:\nThere is no searchable structure in the collection of possible answers,\nThe number of possible answers to check is the same as the number of inputs to the algorithm, and\nThere exists a Boolean function that evaluates each input and determines whether it is the correct answer.\nFor problems with all these properties, the running time of Grover's algorithm on a quantum computer scales as the square root of the number of inputs (or elements in the database), as opposed to the linear scaling of classical algorithms. A general class of problems to which Grover's algorithm can be applied\nBoolean satisfiability problem\nthrough which the algorithm iterates is that of all possible answers. An example and possible application of this is a\nthat attempts to guess a password. Breaking\nwith this algorithm is of interest to government agencies.\n[ edit ] Quantum annealing\nrelies on the adiabatic theorem to undertake calculations. A system is placed in the ground state for a simple Hamiltonian, which slowly evolves to a more complicated Hamiltonian whose ground state represents the solution to the problem in question. The adiabatic theorem states that if the evolution is slow enough the system will stay in its ground state at all times through the process. Quantum annealing can solve\nand the (computationally equivalent)\nproblem, which in turn can be used to encode a wide range of\ncombinatorial optimization\nAdiabatic optimization may be helpful for solving\ncomputational biology\nQuantum machine learning\nSince quantum computers can produce outputs that classical computers cannot produce efficiently, and since quantum computation is fundamentally linear algebraic, some express hope in developing quantum algorithms that can speed up\n, named after its discoverers Harrow, Hassidim, and Lloyd, is believed to provide speedup over classical counterparts.\nSome research groups have recently explored the use of quantum annealing hardware for training\ndeep neural networks\n[ 86 ] [ 87 ] [ 88 ]\nDeep generative chemistry models emerge as powerful tools to expedite\n. However, the immense size and complexity of the structural space of all possible drug-like molecules pose significant obstacles, which could be overcome in the future by quantum computers. Quantum computers are naturally good for solving complex quantum many-body problems\nand thus may be instrumental in applications involving quantum chemistry. Therefore, one can expect that quantum-enhanced generative models\nincluding quantum GANs\nmay eventually be developed into ultimate generative chemistry algorithms.\n[ edit ] A wafer of adiabatic quantum computers\nclassical computers outperform quantum computers for all real-world applications. While current quantum computers may speed up solutions to particular mathematical problems, they give no computational advantage for practical tasks. Scientists and engineers are exploring multiple technologies for quantum computing hardware and hope to develop scalable quantum architectures, but serious obstacles remain.\nThere are a number of technical challenges in building a large-scale quantum computer.\nfor a practical quantum computer:\nPhysically scalable to increase the number of qubits\nQubits that can be initialized to arbitrary values\nQuantum gates that are faster than\nQubits that can be read easily.\nSourcing parts for quantum computers is also very difficult.\nSuperconducting quantum computers\n, like those constructed by\nresearch byproduct, and special\ncables made only by the Japanese company Coax Co.\nThe control of multi-qubit systems requires the generation and coordination of a large number of electrical signals with tight and deterministic timing resolution. This has led to the development of\nthat enable interfacing with the qubits. Scaling these systems to support a growing number of qubits is an additional challenge.\nOne of the greatest challenges involved in constructing quantum computers is controlling or removing quantum decoherence. This usually means isolating the system from its environment as interactions with the external world cause the system to decohere. However, other sources of decoherence also exist. Examples include the quantum gates and the lattice vibrations and background thermonuclear spin of the physical system used to implement the qubits. Decoherence is irreversible, as it is effectively non-unitary, and is usually something that should be highly controlled, if not avoided. Decoherence times for candidate systems in particular, the transverse relaxation time\ntechnology, also called the\n), typically range between nanoseconds and seconds at low temperatures.\nCurrently, some quantum computers require their qubits to be cooled to 20 millikelvin (usually using a\ndilution refrigerator [ 98 ]\n) in order to prevent significant decoherence.\nA 2020 study argues that\ncan nevertheless cause certain systems to decohere within milliseconds.\nAs a result, time-consuming tasks may render some quantum algorithms inoperable, as attempting to maintain the state of qubits for a long enough duration will eventually corrupt the superpositions.\nThese issues are more difficult for optical approaches as the timescales are orders of magnitude shorter and an often-cited approach to overcoming them is optical\n. Error rates are typically proportional to the ratio of operating time to decoherence time; hence any operation must be completed much more quickly than the decoherence time.\n, if the error rate is small enough, it is thought to be possible to use\nquantum error correction\nto suppress errors and decoherence. This allows the total calculation time to be longer than the decoherence time if the error correction scheme can correct errors faster than decoherence introduces them. An often-cited figure for the required error rate in each gate for fault-tolerant computation is 10\n, assuming the noise is depolarizing.\nMeeting this scalability condition is possible for a wide range of systems. However, the use of error correction brings with it the cost of a greatly increased number of required qubits. The number required to factor integers using Shor's algorithm is still polynomial, and thought to be between\nis the number of binary digits in the number to be factored; error correction algorithms would inflate this figure by an additional factor of\n. For a 1000-bit number, this implies a need for about 10\nbits without error correction.\nWith error correction, the figure would rise to about 10\nbits. Computation time is about\nMHz, about 10 seconds. However, the encoding and error-correction overheads increase the size of a real fault-tolerant quantum computer by several orders of magnitude. Careful estimates\nshow that at least 3\nmillion physical qubits would factor 2,048-bit integer in 5 months on a fully error-corrected trapped-ion quantum computer. In terms of the number of physical qubits, to date, this remains the lowest estimate\nfor practically useful integer factorization problem sizing 1,024-bit or larger.\nAnother approach to the stability-decoherence problem is to create a\ntopological quantum computer\nused as threads, and relying on\nto form stable logic gates.\nNon-Abelian anyons can, in effect, remember how they have been manipulated, making the potentially useful in quantum computing.\nAs of 2025, Microsoft and other organizations are investing in quasi-particle research.\nto describe the engineering feat of demonstrating that a programmable quantum device can solve a problem beyond the capabilities of state-of-the-art classical computers.\n[ 109 ] [ 110 ] [ 111 ]\nThe problem need not be useful, so some view the quantum supremacy test only as a potential future benchmark.\nIn October 2019, Google AI Quantum, with the help of NASA, became the first to claim to have achieved quantum supremacy by performing calculations on the\nSycamore quantum computer\nmore than 3,000,000 times faster than they could be done on\n, generally considered the world's fastest computer.\n[ 28 ] [ 113 ] [ 114 ]\nThis claim has been subsequently challenged: IBM has stated that Summit can perform samples much faster than claimed,\nand researchers have since developed better algorithms for the sampling problem used to claim quantum supremacy, giving substantial reductions to the gap between Sycamore and classical supercomputers\n[ 117 ] [ 118 ] [ 119 ]\nand even beating it.\n[ 120 ] [ 121 ] [ 122 ]\nIn December 2020, a group at\nimplemented a type of\non 76 photons with a\nphotonic quantum computer\n, to demonstrate quantum supremacy.\n[ 123 ] [ 124 ] [ 125 ]\nThe authors claim that a classical contemporary supercomputer would require a computational time of 600 million years to generate the number of samples their quantum processor can generate in 20 seconds.\nClaims of quantum supremacy have generated hype around quantum computing,\nbut they are based on contrived benchmark tasks that do not directly imply useful real-world applications.\nIn January 2024, a study published in\nPhysical Review Letters\nprovided direct verification of quantum supremacy experiments by computing exact amplitudes for experimentally generated bitstrings using a new-generation Sunway supercomputer, demonstrating a significant leap in simulation capability built on a multiple-amplitude tensor network contraction algorithm. This development underscores the evolving landscape of quantum computing, highlighting both the progress and the complexities involved in validating quantum supremacy claims.\nDespite high hopes for quantum computing, significant progress in hardware, and optimism about future applications, a 2023\nspotlight article summarized current quantum computers as being \"For now, [good for] absolutely nothing\".\nThe article elaborated that quantum computers are yet to be more useful or efficient than conventional computers in any case, though it also argued that in the long term such computers are likely to be useful. A 2023\nCommunications of the ACM\nfound that current quantum computing algorithms are \"insufficient for practical quantum advantage without significant improvements across the software/hardware stack\". It argues that the most promising candidates for achieving speedup with quantum computers are \"small-data problems\", for example in chemistry and materials science. However, the article also concludes that a large range of the potential applications it considered, such as machine learning, \"will not achieve quantum advantage with current quantum algorithms in the foreseeable future\", and it identified I/O constraints that make speedup unlikely for \"big data problems, unstructured linear systems, and database search based on Grover's algorithm\".\nThis state of affairs can be traced to several current and long-term considerations.\nConventional computer hardware and algorithms are not only optimized for practical tasks, but are still improving rapidly, particularly\nCurrent quantum computing hardware generates only a limited amount of\nbefore getting overwhelmed by noise.\nQuantum algorithms provide speedup over conventional algorithms only for some tasks, and matching these tasks with practical applications proved challenging. Some promising tasks and applications require resources far beyond those available today.\nIn particular, processing large amounts of non-quantum data is a challenge for quantum computers.\nSome promising algorithms have been \"dequantized\", i.e., their non-quantum analogues with similar complexity have been found.\nquantum error correction\nis used to scale quantum computers to practical applications, its overhead may undermine speedup offered by many quantum algorithms.\nComplexity analysis of algorithms sometimes makes abstract assumptions that do not hold in applications. For example, input data may not already be available encoded in quantum states, and \"oracle functions\" used in Grover's algorithm often have internal structure that can be exploited for faster algorithms.\nIn particular, building computers with large numbers of qubits may be futile if those qubits are not connected well enough and cannot maintain sufficiently high degree of entanglement for a long time. When trying to outperform conventional computers, quantum computing researchers often look for new tasks that can be solved on quantum computers, but this leaves the possibility that efficient non-quantum techniques will be developed in response, as seen for Quantum supremacy demonstrations. Therefore, it is desirable to prove lower bounds on the complexity of best possible non-quantum algorithms (which may be unknown) and show that some quantum algorithms asymptomatically improve upon those bounds.\ndoubted the practicality of quantum computers in a paper published in 1994.\nargued that a 400-qubit computer would even come into conflict with the cosmological information bound implied by the\nholographic principle\ndoubt that quantum supremacy will ever be achieved.\n[ 134 ] [ 135 ] [ 136 ]\nhas expressed skepticism of quantum computing as follows:\n\"So the number of continuous parameters describing the state of such a useful quantum computer at any given moment must be... about 10\n... Could we ever learn to control the more than 10\ncontinuously variable parameters defining the quantum state of such a system? My answer is simple.\nPhysical realizations\nFurther information:\nList of proposed quantum registers Quantum System One , a quantum computer by IBM from 2019 with 20 superconducting qubits [ 138 ]\nA practical quantum computer must use a physical system as a programmable quantum register.\nResearchers are exploring several technologies as candidates for reliable qubit implementations.\n[ 140 ] Superconductors\nare some of the most developed proposals, but experimentalists are considering other hardware possibilities as well.\ntopological quantum computer\napproaches are being explored for more fault-tolerance computing systems.\nThe first quantum logic gates were implemented with\nand prototype general purpose machines with up to 20 qubits have been realized. However, the technology behind these devices combines complex vacuum equipment, lasers, microwave and radio frequency equipment making full scale processors difficult to integrate with standard computing equipment. Moreover, the trapped ion system itself has engineering challenges to overcome.\nThe largest commercial systems are based on\ndevices and have scaled to 2000 qubits. However, the error rates for larger machines have been on the order of 5%. Technologically these devices are all cryogenic and scaling to large numbers of qubits requires wafer-scale integration, a serious engineering challenge by itself.\nPotential applications\nWith focus on business management's point of view, the potential applications of quantum computing into four major categories are cybersecurity, data analytics and artificial intelligence, optimization and simulation, and data management and searching.\nOther applications include healthcare (ie. drug discovery), financial modeling, and natural language processing.\nFurther information:\nComputability theory\ncomputational problem\nsolvable by a classical computer is also solvable by a quantum computer.\nIntuitively, this is because it is believed that all physical phenomena, including the operation of classical computers, can be described using\n, which underlies the operation of quantum computers.\nConversely, any problem solvable by a quantum computer is also solvable by a classical computer. It is possible to simulate both quantum and classical computers manually with just some paper and a pen, if given enough time. More formally, any quantum computer can be simulated by a\n. In other words, quantum computers provide no additional power over classical computers in terms of\n. This means that quantum computers cannot solve\nundecidable problems\n, and the existence of quantum computers does not disprove the\nChurch–Turing thesis\nQuantum complexity theory\nWhile quantum computers cannot solve any problems that classical computers cannot already solve, it is suspected that they can solve certain problems faster than classical computers. For instance, it is known that quantum computers can efficiently\n, while this is not believed to be the case for classical computers.\nthat can be efficiently solved by a quantum computer with bounded error is called\n, for \"bounded error, quantum, polynomial time\". More formally, BQP is the class of problems that can be solved by a polynomial-time quantum Turing machine with an error probability of at most 1/3. As a class of probabilistic problems, BQP is the quantum counterpart to\n(\"bounded error, probabilistic, polynomial time\"), the class of problems that can be solved by polynomial-time\nprobabilistic Turing machines\nB P P ⊆ B Q P {\\displaystyle {\\mathsf {BPP\\subseteq BQP}}}\nand is widely suspected that\nB Q P ⊊ B P P {\\displaystyle {\\mathsf {BQP\\subsetneq BPP}}}\n, which intuitively would mean that quantum computers are more powerful than classical computers in terms of\n[ 150 ] The suspected relationship of BQP to several classical complexity classes [ 60 ]\nThe exact relationship of BQP to\nis not known. However, it is known that\nP ⊆ B Q P ⊆ P S P A C E {\\displaystyle {\\mathsf {P\\subseteq BQP\\subseteq PSPACE}}}\n; that is, all problems that can be efficiently solved by a deterministic classical computer can also be efficiently solved by a quantum computer, and all problems that can be efficiently solved by a quantum computer can also be solved by a deterministic classical computer with polynomial space resources. It is further suspected that BQP is a strict superset of P, meaning there are problems that are efficiently solvable by quantum computers that are not efficiently solvable by deterministic classical computers. For instance, integer factorization and the\ndiscrete logarithm problem\nare known to be in BQP and are suspected to be outside of P. On the relationship of BQP to NP, little is known beyond the fact that some NP problems that are believed not to be in P are also in BQP (integer factorization and the discrete logarithm problem are both in NP, for example). It is suspected that\nN P ⊈ B Q P {\\displaystyle {\\mathsf {NP\\nsubseteq BQP}}}\n; that is, it is believed that there are efficiently checkable problems that are not efficiently solvable by a quantum computer. As a direct consequence of this belief, it is also suspected that BQP is disjoint from the class of\nproblems (if an NP-complete problem were in BQP, then it would follow from\nthat all problems in NP are in BQP).\nWikimedia Commons has media related to\n– Quantum computing company\nElectronic quantum holography\n– Information storage technology\nGlossary of quantum computing IARPA\n– American government agency\nPages displaying short descriptions of redirect targets India's quantum computer\n– Indian proposed quantum computer\n– US information technology company\nList of emerging technologies\n– New technologies actively in development\nList of quantum computing journals List of quantum processors Magic state distillation\n– Quantum computing algorithm\n– Computing for the purpose of computing\n– Computer that uses photons or light waves\nQuantum bus Quantum cognition\n– Application of quantum theory mathematics to cognitive phenomena\n– Device measuring quantum mechanical effects\n– Metric for a quantum computer's capabilities\n– Unintuitive aspects of quantum mechanics\n– American quantum computing company\n– Type of extremely powerful computer\nTheoretical computer science\n– Subfield of computer science and mathematics\nUnconventional computing\n– Computing by new or unusual methods\n– Experimental area in semiconductors\n[ edit ] ^ The standard basis is also the computational basis . [ 35 ]\nQuantum computers are now being used in production environments like telecom and pharmaceuticals.\n[ edit ] Aaronson, Scott (2013). Quantum Computing Since Democritus . Cambridge University Press. doi : 10.1017/CBO9780511979309 . ISBN 978-0-521-19956-8 . OCLC 829706638 . Grumbling, Emily; Horowitz, Mark, eds. (2019). Quantum Computing: Progress and Prospects . Washington, DC: The National Academies Press. doi : 10.17226/25196 . ISBN 978-0-309-47970-7 . OCLC 1091904777 . S2CID 125635007 . Mermin, N. David (2007). Quantum Computer Science: An Introduction . doi : 10.1017/CBO9780511813870 . ISBN 978-0-511-34258-5 . OCLC 422727925 . Nielsen, Michael ; Chuang, Isaac (2010). Quantum Computation and Quantum Information (10th anniversary ed.). doi : 10.1017/CBO9780511976667 . ISBN 978-0-511-99277-3 . OCLC 700706156 . S2CID 59717455 . Shor, Peter W. (1994). Algorithms for Quantum Computation: Discrete Logarithms and Factoring . Symposium on Foundations of Computer Science . Santa Fe, New Mexico : IEEE . pp. 124– 134. doi : 10.1109/SFCS.1994.365700 . ISBN 978-0-8186-6580-6 .\n[ edit ] Benenti, Giuliano; Casati, Giulio; Rossini, Davide; Strini, Giuliano (2019). Principles of Quantum Computation and Information: A Comprehensive Textbook (2nd ed.). doi : 10.1142/10909 . ISBN 978-981-3237-23-0 . OCLC 1084428655 . S2CID 62280636 . Bernhardt, Chris (2019). Quantum Computing for Everyone . MIT Press. ISBN 978-0-262-35091-4 . OCLC 1082867954 . Exman, Iaakov; Pérez-Castillo, Ricardo; Piattini, Mario; Felderer, Michael, eds. (2024). Quantum Software: Aspects of Theory and System Design . Springer Nature . doi : 10.1007/978-3-031-64136-7 . ISBN 978-3-031-64136-7 . Hidary, Jack D. (2021). Quantum Computing: An Applied Approach (2nd ed.). doi : 10.1007/978-3-030-83274-2 . ISBN 978-3-03-083274-2 . OCLC 1272953643 . S2CID 238223274 . Hiroshi, Imai; Masahito, Hayashi, eds. (2006). Quantum Computation and Information: From Theory to Experiment . Topics in Applied Physics. Vol. 102. doi : 10.1007/3-540-33133-6 . ISBN 978-3-540-33133-9 . Hughes, Ciaran; Isaacson, Joshua; Perry, Anastasia; Sun, Ranbel F.; Turner, Jessica (2021). Quantum Computing for the Quantum Curious . doi : 10.1007/978-3-030-61601-4 . ISBN 978-3-03-061601-4 . OCLC 1244536372 . S2CID 242566636 . Jaeger, Gregg (2007). Quantum Information: An Overview . doi : 10.1007/978-0-387-36944-0 . ISBN 978-0-387-36944-0 . OCLC 186509710 . Johnston, Eric R.; Harrigan, Nic; Gimeno-Segovia, Mercedes (2019). Programming Quantum Computers: Essential Algorithms and Code Samples . O'Reilly Media, Incorporated. ISBN 978-1-4920-3968-6 . OCLC 1111634190 . Kaye, Phillip; Laflamme, Raymond ; Mosca, Michele (2007). An Introduction to Quantum Computing . OUP Oxford. ISBN 978-0-19-857000-4 . OCLC 85896383 . Kitaev, Alexei Yu. ; Shen, Alexander H.; Vyalyi, Mikhail N. (2002). Classical and Quantum Computation . American Mathematical Soc. ISBN 978-0-8218-3229-5 . OCLC 907358694 . Kurgalin, Sergei; Borzunov, Sergei (2021). Concise Guide to Quantum Computing: Algorithms, Exercises, and Implementations . Springer. doi : 10.1007/978-3-030-65052-0 . ISBN 978-3-030-65052-0 . Stolze, Joachim; Suter, Dieter (2004). Quantum Computing: A Short Course from Theory to Experiment . doi : 10.1002/9783527617760 . ISBN 978-3-527-61776-0 . OCLC 212140089 . Susskind, Leonard ; Friedman, Art (2014). Quantum Mechanics: The Theoretical Minimum . New York : Basic Books . ISBN 978-0-465-08061-8 . Wichert, Andreas (2020). Principles of Quantum Artificial Intelligence: Quantum Problem Solving and Machine Learning (2nd ed.). doi : 10.1142/11938 . ISBN 978-981-12-2431-7 . OCLC 1178715016 . S2CID 225498497 . Wong, Thomas (2022). Introduction to Classical and Quantum Computing (PDF) . Rooted Grove. ISBN 979-8-9855931-0-5 . OCLC 1308951401 . Archived from the original (PDF) on 29 January 2022 . Retrieved 6 February 2022 . Zeng, Bei; Chen, Xie; Zhou, Duan-Lu; Wen, Xiao-Gang (2019). Quantum Information Meets Quantum Matter . arXiv : 1508.02595 . doi : 10.1007/978-1-4939-9084-9 . ISBN 978-1-4939-9084-9 . OCLC 1091358969 . S2CID 118528258 .\n[ edit ] Abbot, Derek ; Doering, Charles R. ; Caves, Carlton M. ; Lidar, Daniel M. ; Brandt, Howard E. ; et al. (2003). \"Dreams versus Reality: Plenary Debate Session on Quantum Computing\". Quantum Information Processing . 2 (6): 449– 472. arXiv : quant-ph/0310130 . Bibcode : 2003QuIP....2..449A . doi : 10.1023/B:QINP.0000042203.24782.9a . hdl : 2027.42/45526 . S2CID 34885835 . Berthiaume, Andre (1 December 1998). \"Quantum Computation\". Solution Manual for Quantum Mechanics . pp. 233– 234. doi : 10.1142/9789814541893_0016 . ISBN 978-981-4541-88-6 . S2CID 128255429 – via Semantic Scholar. DiVincenzo, David P. (2000). \"The Physical Implementation of Quantum Computation\". Fortschritte der Physik . 48 ( 9– 11): 771– 783. arXiv : quant-ph/0002077 . Bibcode : 2000ForPh..48..771D . doi : 10.1002/1521-3978(200009)48:9/11\u003c771::AID-PROP771\u003e3.0.CO;2-E . S2CID 15439711 . DiVincenzo, David P. (1995). \"Quantum Computation\". Science . 270 (5234): 255– 261. Bibcode : 1995Sci...270..255D . CiteSeerX 10.1.1.242.2165 . doi : 10.1126/science.270.5234.255 . S2CID 220110562 .\nTable 1 lists switching and dephasing times for various systems.\nJeutner, Valentin (2021). \"The Quantum Imperative: Addressing the Legal Dimension of Quantum Computers\" . Morals \u0026 Machines . 1 (1): 52– 59. doi : 10.5771/2747-5174-2021-1-52 . S2CID 236664155 . Krantz, P.; Kjaergaard, M.; Yan, F.; Orlando, T. P.; Gustavsson, S.; Oliver, W. D. (17 June 2019). \"A Quantum Engineer's Guide to Superconducting Qubits\". Applied Physics Reviews . 6 (2): 021318. arXiv : 1904.06560 . Bibcode : 2019ApPRv...6b1318K . doi : 10.1063/1.5089550 . ISSN 1931-9401 . S2CID 119104251 . Mitchell, Ian (1998). \"Computing Power into the 21st Century: Moore's Law and Beyond\" . Simon, Daniel R. (1994). \"On the Power of Quantum Computation\" . Institute of Electrical and Electronics Engineers Computer Society Press.\nat Wikimedia Commons\nLearning materials related to\nStanford Encyclopedia of Philosophy\n\" by Amit Hagar and Michael E. Cuffaro\n\"Quantum computation, theory of\" , Encyclopedia of Mathematics , EMS Press , 2001 [1994] Introduction to Quantum Computing for Business by Koen Groenland\nQuantum computing for the determined\n– 22 video lectures by\nMichael Nielsen Video Lectures\nFour Lectures on Quantum Computing given at Oxford University in July 2006 v t e Processor technologies Models Abstract machine Stored-program computer Finite-state machine with datapath Hierarchical Deterministic finite automaton Queue automaton Cellular automaton Quantum cellular automaton Turing machine Alternating Turing machine Universal Post–Turing Quantum Nondeterministic Turing machine Probabilistic Turing machine Hypercomputation Zeno machine Belt machine Stack machine Register machines Counter Pointer Random-access Random-access stored program Architecture Microarchitecture Von Neumann Harvard modified Dataflow Transport-triggered Cellular Endianness Memory access NUMA HUMA Load–store Register/memory Cache hierarchy Memory hierarchy Virtual memory Secondary storage Heterogeneous Fabric Multiprocessing Cognitive Neuromorphic Instruction set architectures\nOrthogonal instruction set CISC RISC Application-specific EDGE TRIPS VLIW EPIC MISC OISC NISC ZISC VISC architecture Quantum computing Comparison Addressing modes\nMotorola 68000 series VAX PDP-11 x86 ARM Stanford MIPS MIPS MIPS-X\nPOWER PowerPC Power ISA Clipper architecture SPARC SuperH DEC Alpha ETRAX CRIS M32R Unicore Itanium OpenRISC RISC-V MicroBlaze LMC\nS/360 S/370 S/390 z/Architecture\nVISC architecture Epiphany architecture Others Execution Instruction pipelining Pipeline stall Operand forwarding Classic RISC pipeline Hazards Data dependency Structural Control False sharing Out-of-order Scoreboarding Tomasulo's algorithm Reservation station Re-order buffer Register renaming Wide-issue Speculative Branch prediction Memory dependence prediction Parallelism\nBit Bit-serial Word Instruction Pipelining Scalar Superscalar Task Thread Process Data Vector Memory Distributed Multithreading Temporal Simultaneous Hyperthreading Simultaneous and heterogenous Speculative Preemptive Cooperative Flynn's taxonomy SISD SIMD Array processing (SIMT) Pipelined processing Associative processing SWAR MISD MIMD SPMD Processor performance Transistor count Instructions per cycle\nCycles per instruction\nInstructions per second\nFloating-point operations per second\nTransactions per second\nSynaptic updates per second\nPerformance per watt\nCache performance metrics Computer performance by orders of magnitude Types Central processing unit\nGraphics processing unit\nGPGPU Vector Barrel Stream Tile processor Coprocessor PAL ASIC FPGA FPOA CPLD Multi-chip module\nPackage on a package\nEmbedded system Microprocessor Microcontroller Mobile Ultra-low-voltage ASIP Soft microprocessor\nCypress PSoC Network on a chip\nHardware accelerators Coprocessor AI accelerator Graphics processing unit\nImage processor Vision processing unit\nPhysics processing unit\nDigital signal processor\nTensor Processing Unit\nSecure cryptoprocessor Network processor Baseband processor Word size 1-bit 4-bit 8-bit 12-bit 15-bit 16-bit 24-bit 32-bit 48-bit 64-bit 128-bit 256-bit 512-bit bit slicing others variable\nSingle-core Multi-core Manycore Heterogeneous architecture\nCore Cache CPU cache Scratchpad memory Data cache Instruction cache replacement policies coherence Bus Clock rate Clock signal FIFO Functional units Arithmetic logic unit\nAddress generation unit\nMemory management unit\nLoad–store unit Translation lookaside buffer\nBranch predictor Branch target predictor Integrated memory controller\nMemory management unit Instruction decoder Logic Combinational Sequential Glue Logic gate Quantum Array Registers Processor register Status register Stack register Register file Memory buffer Memory address register Program counter Control unit Hardwired control unit Instruction unit Data buffer Write buffer Microcode ROM Counter Datapath Multiplexer Demultiplexer Adder Multiplier CPU Binary decoder Address decoder Sum-addressed decoder Barrel shifter Circuitry Integrated circuit 3D Mixed-signal Power management Boolean Digital Analog Quantum Switch Power management PMU APM ACPI Dynamic frequency scaling Dynamic voltage scaling Clock gating Performance per watt\nHistory of general-purpose CPUs Microprocessor chronology Processor design Digital electronics Hardware security module Semiconductor device fabrication Tick–tock model Pin grid array Chip carrier v t e Quantum information science\nDiVincenzo's criteria NISQ era Quantum computing timeline Quantum information Quantum programming Quantum simulation Qubit physical vs. logical Quantum processors cloud-based\nBell's Eastin–Knill Gleason's Gottesman–Knill Holevo's No-broadcasting No-cloning No-communication No-deleting No-hiding No-teleportation PBR Quantum speed limit Threshold Solovay–Kitaev Schrödinger-HJW\nClassical capacity entanglement-assisted quantum capacity Entanglement distillation Entanglement swapping Monogamy of entanglement LOCC Quantum channel quantum network State purification Quantum teleportation quantum energy teleportation quantum gate teleportation Superdense coding Quantum cryptography Post-quantum cryptography Quantum coin flipping Quantum money Quantum key distribution BB84 SARG04 other protocols Quantum secret sharing Quantum algorithms Algorithmic cooling Amplitude amplification Bernstein–Vazirani BHT Boson sampling Deutsch–Jozsa Grover's HHL Hidden subgroup Magic state distillation Quantum annealing Quantum counting Quantum Fourier transform Quantum optimization Quantum phase estimation Shor's Simon's VQE Quantum complexity theory BQP DQC1 EQP QIP QMA PostBQP\nprocessor benchmarks\nQuantum supremacy Quantum volume Randomized benchmarking XEB Relaxation times T 1 T 2\ncomputing models Adiabatic quantum computation Continuous-variable quantum information One-way quantum computer cluster state Quantum circuit quantum logic gate Quantum machine learning quantum neural network Quantum Turing machine Topological quantum computer Hamiltonian quantum computation Quantum error correction\n5 qubit CSS GKP quantum convolutional stabilizer Shor Bacon–Shor Steane Toric gnu Entanglement-assisted\nQuantum optics Cavity QED Circuit QED Linear optical QC KLM protocol Ultracold atoms Neutral atom QC Trapped-ion QC Spin\nKane QC Spin qubit QC NV center NMR QC Superconducting Charge qubit Flux qubit Phase qubit Transmon Quantum programming OpenQASM\nForest/Rigetti QCS Cirq Q# libquantum many others... Quantum information science Quantum mechanics topics v t e Emerging technologies\nQuantum algorithms amplifier bus cellular automata channel circuit complexity theory computing cryptography post-quantum dynamics electronics error correction finite automata image processing imaging information key distribution logic logic clock logic gate machine machine learning metamaterial network neural network optics programming sensing simulator teleportation\nAcoustic levitation Anti-gravity Cloak of invisibility Digital scent technology Force field Plasma window Immersive virtual reality Magnetic refrigeration Phased-array optics Thermoacoustic heat engine List v t e Quantum mechanics\nIntroduction History Timeline Classical mechanics Old quantum theory Glossary\nBorn rule Bra–ket notation Complementarity Density matrix Energy level Ground state Excited state Degenerate levels Zero-point energy Entanglement Hamiltonian Interference Decoherence Measurement Nonlocality Quantum state Superposition Tunnelling Scattering theory Symmetry in quantum mechanics Uncertainty Wave function Collapse Wave–particle duality\nFormulations Heisenberg Interaction Matrix mechanics Schrödinger Path integral formulation Phase space\nKlein–Gordon Dirac Weyl Majorana Rarita–Schwinger Pauli Rydberg Schrödinger Interpretations Bayesian Consciousness causes collapse Consistent histories Copenhagen de Broglie–Bohm Ensemble Hidden-variable Local Superdeterminism Many-worlds Objective collapse Quantum logic Relational Transactional\nBell test Davisson–Germer Delayed-choice quantum eraser Double-slit Franck–Hertz Mach–Zehnder interferometer Elitzur–Vaidman Popper Quantum eraser Stern–Gerlach Wheeler's delayed choice Science Quantum biology Quantum chemistry Quantum chaos Quantum cosmology Quantum differential calculus Quantum dynamics Quantum geometry Quantum measurement problem Quantum mind Quantum stochastic calculus Quantum spacetime Technology Quantum algorithms Quantum amplifier Quantum bus Quantum cellular automata Quantum finite automata Quantum channel Quantum circuit Quantum complexity theory Quantum computing Timeline Quantum cryptography Quantum electronics Quantum error correction Quantum imaging Quantum image processing Quantum information Quantum key distribution Quantum logic Quantum logic gates Quantum machine Quantum machine learning Quantum metamaterial Quantum metrology Quantum network Quantum neural network Quantum optics Quantum programming Quantum sensing Quantum simulator Quantum teleportation\nQuantum fluctuation Casimir effect Quantum statistical mechanics Quantum field theory History Quantum gravity Relativistic quantum mechanics\nSchrödinger's cat in popular culture Wigner's friend EPR paradox Quantum mysticism Category Authority control databases\nUnited States Czech Republic Israel\nYale LUX ^ Mermin 2007 , p. 1. ^ Aaronson 2013 , p. 132. ^ Zwiebach, Barton (2022). Mastering Quantum Mechanics: Essentials, Theory, and Applications . MIT Press. §1. ISBN 978-0-262-04613-8 . Quantum physics has replaced classical physics as the correct fundamental description of our physical universe. It is used routinely to describe most phenomena that occur at short distances. [...] The era of quantum physics began in earnest in 1925 with the discoveries of Erwin Schrödinger and Werner Heisenberg. The seeds for these discoveries were planted by Max Planck, Albert Einstein, Niels Bohr, Louis de Broglie, and others. ^ Weinberg, Steven (2015). \"Historical Introduction\". Lectures on Quantum Mechanics (2nd ed.). Cambridge University Press. pp. 1– 30. ISBN 978-1-107-11166-0 . ^ Ceruzzi, Paul E. (2012). Computing: A Concise History . Cambridge, Massachusetts : MIT Press. pp. 3, 46. ISBN 978-0-262-31038-3 . OCLC 796812982 . ^ Hodges, Andrew (2014). Alan Turing: The Enigma . Princeton, New Jersey: Princeton University Press . p. xviii. ISBN 9780691164724 . ^ Mårtensson-Pendrill, Ann-Marie (1 November 2006). \"The Manhattan project—a part of physics history\". Physics Education . 41 (6): 493– 501. Bibcode : 2006PhyEd..41..493M . doi : 10.1088/0031-9120/41/6/001 . ISSN 0031-9120 . S2CID 120294023 . ^ a b Benioff, Paul (1980). \"The computer as a physical system: A microscopic quantum mechanical Hamiltonian model of computers as represented by Turing machines\". Journal of Statistical Physics . 22 (5): 563– 591. Bibcode : 1980JSP....22..563B . doi : 10.1007/bf01011339 . S2CID 122949592 . ^ Buluta, Iulia; Nori, Franco (2 October 2009). \"Quantum Simulators\". Science . 326 (5949): 108– 111. Bibcode : 2009Sci...326..108B . doi : 10.1126/science.1177838 . ISSN 0036-8075 . PMID 19797653 . S2CID 17187000 . ^ Manin, Yu. I. (1980). Vychislimoe i nevychislimoe [ Computable and Noncomputable ] (in Russian). Soviet Radio. pp. 13– 15. Archived from the original on 10 May 2013 . Retrieved 4 March 2013 . ^ Feynman, Richard (June 1982). \"Simulating Physics with Computers\" (PDF) . International Journal of Theoretical Physics . 21 (6/7): 467– 488. Bibcode : 1982IJTP...21..467F . doi : 10.1007/BF02650179 . S2CID 124545445 . Archived from the original (PDF) on 8 January 2019 . Retrieved 28 February 2019 . ^ Nielsen \u0026 Chuang 2010 , p. 214. ^ a b Bennett, C. H.; Brassard, G. (1984). \"Quantum cryptography: Public key distribution and coin tossing\". Proceedings of the International Conference on Computers, Systems \u0026 Signal Processing, Bangalore, India . Vol. 1. New York: IEEE. pp. 175– 179. Reprinted as Bennett, C. H.; Brassard, G. (4 December 2014). \"Quantum cryptography: Public key distribution and coin tossing\" . Theoretical Computer Science . Theoretical Aspects of Quantum Cryptography – celebrating 30 years of BB84. 560 (1): 7– 11. arXiv : 2003.06557 . doi : 10.1016/j.tcs.2014.05.025 . ^ Brassard, G. (2005). \"Brief history of quantum cryptography: A personal perspective\". IEEE Information Theory Workshop on Theory and Practice in Information-Theoretic Security, 2005 . Awaji Island, Japan: IEEE. pp. 19– 23. arXiv : quant-ph/0604072 . doi : 10.1109/ITWTPI.2005.1543949 . ISBN 978-0-7803-9491-9 . S2CID 16118245 . ^ Deutsch, D. (8 July 1985). \"Quantum theory, the Church–Turing principle and the universal quantum computer\". Proceedings of the Royal Society of London. A. Mathematical and Physical Sciences . 400 (1818): 97– 117. Bibcode : 1985RSPSA.400...97D . doi : 10.1098/rspa.1985.0070 . ISSN 0080-4630 . S2CID 1438116 . ^ Bernstein, Ethan; Vazirani, Umesh (1993). \"Quantum complexity theory\" . Proceedings of the twenty-fifth annual ACM symposium on Theory of computing – STOC '93 . San Diego, California, United States: ACM Press. pp. 11– 20. doi : 10.1145/167088.167097 . ISBN 978-0-89791-591-5 . S2CID 676378 . ^ Simon, D. R. (1994). \"On the power of quantum computation\". Proceedings 35th Annual Symposium on Foundations of Computer Science . Santa Fe, New Mexico, USA: IEEE Comput. Soc. Press. pp. 116– 123. doi : 10.1109/SFCS.1994.365701 . ISBN 978-0-8186-6580-6 . S2CID 7457814 . ^ Nielsen \u0026 Chuang 2010 , p. 30-32. ^ Shor 1994 . ^ Grover, Lov K. (1996). A fast quantum mechanical algorithm for database search . ACM symposium on Theory of computing. Philadelphia : ACM Press. pp. 212– 219. arXiv : quant-ph/9605043 . doi : 10.1145/237814.237866 . ISBN 978-0-89791-785-8 . ^ a b Nielsen \u0026 Chuang 2010 , p. 7. ^ a b Lloyd, Seth (23 August 1996). \"Universal Quantum Simulators\". Science . 273 (5278): 1073– 1078. Bibcode : 1996Sci...273.1073L . doi : 10.1126/science.273.5278.1073 . ISSN 0036-8075 . PMID 8688088 . S2CID 43496899 . ^ Cao, Yudong; Romero, Jonathan; Olson, Jonathan P.; Degroote, Matthias; Johnson, Peter D.; et al. (9 October 2019). \"Quantum Chemistry in the Age of Quantum Computing\". Chemical Reviews . 119 (19): 10856– 10915. arXiv : 1812.09976 . doi : 10.1021/acs.chemrev.8b00803 . ISSN 0009-2665 . PMID 31469277 . S2CID 119417908 . ^ a b Grumbling \u0026 Horowitz 2019 , pp. 164–169. ^ Chuang, Isaac L.; Gershenfeld, Neil; Kubinec, Markdoi (April 1998). \"Experimental Implementation of Fast Quantum Searching\". Physical Review Letters . 80 (15). American Physical Society : 3408– 3411. Bibcode : 1998PhRvL..80.3408C . doi : 10.1103/PhysRevLett.80.3408 . ^ Holton, William Coffeen. \"quantum computer\" . Encyclopedia Britannica . Encyclopædia Britannica . Retrieved 4 December 2021 . ^ Gibney, Elizabeth (23 October 2019). \"Hello quantum world! Google publishes landmark quantum supremacy claim\" . Nature . 574 (7779): 461– 462. Bibcode : 2019Natur.574..461G . doi : 10.1038/d41586-019-03213-z . PMID 31645740 . ^ a b Lay summary: Martinis, John; Boixo, Sergio (23 October 2019). \"Quantum Supremacy Using a Programmable Superconducting Processor\" . Nature . 574 (7779). Google AI : 505– 510. arXiv : 1910.11333 . Bibcode : 2019Natur.574..505A . doi : 10.1038/s41586-019-1666-5 . PMID 31645734 . S2CID 204836822 . Retrieved 27 April 2022 . • Journal article: Arute, Frank; Arya, Kunal; Babbush, Ryan; Bacon, Dave; Bardin, Joseph C.; et al. (23 October 2019). \"Quantum supremacy using a programmable superconducting processor\". Nature . 574 (7779): 505– 510. arXiv : 1910.11333 . Bibcode : 2019Natur.574..505A . doi : 10.1038/s41586-019-1666-5 . PMID 31645734 . S2CID 204836822 . ^ Aaronson, Scott (30 October 2019). \"Opinion | Why Google's Quantum Supremacy Milestone Matters\" . The New York Times . ISSN 0362-4331 . Retrieved 25 September 2021 . ^ Pan, Feng; Zhang, Pan (4 March 2021). \"Simulating the Sycamore quantum supremacy circuits\". arXiv : 2103.03074 [ quant-ph ]. ^ Sample, Ian; editor, Ian Sample Science (23 October 2019). \"Google claims it has achieved 'quantum supremacy' – but IBM disagrees\" . The Guardian . ISSN 0261-3077 . Retrieved 1 August 2025 . {{ cite news }} : |last2= has generic name ( help ) ^ Bennett, Charlie (31 July 2020). Information Is Quantum: How Physics Helped Explain the Nature of Information and What Can Be Done With It (Videotape). Event occurs at 1:08:22 – via YouTube. ^ Nielsen \u0026 Chuang 2010 , p. 13. ^ a b Mermin 2007 , p. 17. ^ a b Mermin 2007 , p. 18. ^ Aaronson 2013 , p. 110. ^ Nielsen \u0026 Chuang 2010 , p. 30–32. ^ Mermin 2007 , pp. 38–39. ^ Kurgalin, Sergei; Borzunov, Sergei (2021). Concise guide to quantum computing: algorithms, exercises, and implementations . Texts in computer science. Cham: Springer. ISBN 978-3-030-65054-4 . ^ Das, A.; Chakrabarti, B. K. (2008). \"Quantum Annealing and Analog Quantum Computation\". Rev. Mod. Phys. 80 (3): 1061– 1081. arXiv : 0801.2193 . Bibcode : 2008RvMP...80.1061D . CiteSeerX 10.1.1.563.9990 . doi : 10.1103/RevModPhys.80.1061 . S2CID 14255125 . ^ Nayak, Chetan; Simon, Steven; Stern, Ady; Das Sarma, Sankar (2008). \"Nonabelian Anyons and Quantum Computation\". Reviews of Modern Physics . 80 (3): 1083– 1159. arXiv : 0707.1889 . Bibcode : 2008RvMP...80.1083N . doi : 10.1103/RevModPhys.80.1083 . S2CID 119628297 . ^ Chi-Chih Yao, A. (1993). \"Quantum circuit complexity\". Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science . pp. 352– 361. doi : 10.1109/SFCS.1993.366852 . ISBN 0-8186-4370-6 . S2CID 195866146 . ^ Raussendorf, Robert; Browne, Daniel E.; Briegel, Hans J. (25 August 2003). \"Measurement-based quantum computation on cluster states\". Physical Review A . 68 (2): 022312. arXiv : quant-ph/0301052 . Bibcode : 2003PhRvA..68b2312R . doi : 10.1103/PhysRevA.68.022312 . S2CID 6197709 . ^ Aharonov, Dorit; van Dam, Wim; Kempe, Julia; Landau, Zeph; Lloyd, Seth; Regev, Oded (1 January 2008). \"Adiabatic Quantum Computation Is Equivalent to Standard Quantum Computation\". SIAM Review . 50 (4): 755– 787. arXiv : quant-ph/0405098 . Bibcode : 2008SIAMR..50..755A . doi : 10.1137/080734479 . ISSN 0036-1445 . S2CID 1503123 . ^ Freedman, Michael H.; Larsen, Michael; Wang, Zhenghan (1 June 2002). \"A Modular Functor Which is Universal for Quantum Computation\". Communications in Mathematical Physics . 227 (3): 605– 622. arXiv : quant-ph/0001108 . Bibcode : 2002CMaPh.227..605F . doi : 10.1007/s002200200645 . ISSN 0010-3616 . S2CID 8990600 . ^ Nielsen \u0026 Chuang 2010 , p. 481. ^ a b c d Preskill, John (6 August 2018). \"Quantum Computing in the NISQ era and beyond\" . Quantum . 2 79. arXiv : 1801.00862 . Bibcode : 2018Quant...2...79P . doi : 10.22331/q-2018-08-06-79 . S2CID 44098998 . ^ Bluvstein, Dolev; Evered, Simon J.; Geim, Alexandra A.; Li, Sophie H.; Zhou, Hengyun; Manovitz, Tom; Ebadi, Sepehr; Cain, Madelyn; Kalinowski, Marcin; Hangleiter, Dominik; Ataides, J. Pablo Bonilla; Maskara, Nishad; Cong, Iris; Gao, Xun; Rodriguez, Pedro Sales (6 December 2023). \"Logical quantum processor based on reconfigurable atom arrays\" . Nature . 626 (7997): 58– 65. arXiv : 2312.03982 . doi : 10.1038/s41586-023-06927-3 . ISSN 1476-4687 . PMC 10830422 . PMID 38056497 . S2CID 266052773 . ^ \"DARPA-Funded Research Leads to Quantum Computing Breakthrough\" . darpa.mil . 6 December 2023 . Retrieved 5 January 2024 . ^ Choudhury, Rizwan (30 December 2023). \"Top 7 innovation stories of 2023 – Interesting Engineering\" . interestingengineering.com . Retrieved 6 January 2024 . ^ Pirandola, S.; Andersen, U. L.; Banchi, L.; Berta, M.; Bunandar, D.; Colbeck, R.; Englund, D.; Gehring, T.; Lupo, C.; Ottaviani, C.; Pereira, J.; Razavi, M.; Shamsul Shaari, J.; Tomamichel, M.; Usenko, V. C.; Vallone, G.; Villoresi, P.; Wallden, P. (2020). \"Advances in quantum cryptography\". Advances in Optics and Photonics . 12 (4): 1012– 1236. arXiv : 1906.01645 . Bibcode : 2020AdOP...12.1012P . doi : 10.1364/AOP.361502 . ^ Inderwildi, Oliver (14 April 2025). \"The Quantum Computing Revolution: From Technological Opportunity to Geopolitical Power Shift\" . The Geopolitical Economist . Retrieved 14 April 2025 . ^ Pirandola, S.; Andersen, U. L.; Banchi, L.; Berta, M.; Bunandar, D.; Colbeck, R.; Englund, D.; Gehring, T.; Lupo, C.; Ottaviani, C.; Pereira, J. L.; Razavi, M.; Shamsul Shaari, J.; Tomamichel, M.; Usenko, V. C. (14 December 2020). \"Advances in quantum cryptography\". Advances in Optics and Photonics . 12 (4): 1017. arXiv : 1906.01645 . Bibcode : 2020AdOP...12.1012P . doi : 10.1364/AOP.361502 . ISSN 1943-8206 . S2CID 174799187 . ^ Xu, Feihu; Ma, Xiongfeng; Zhang, Qiang; Lo, Hoi-Kwong; Pan, Jian-Wei (26 May 2020). \"Secure quantum key distribution with realistic devices\". Reviews of Modern Physics . 92 (2): 025002-3. arXiv : 1903.09051 . Bibcode : 2020RvMP...92b5002X . doi : 10.1103/RevModPhys.92.025002 . S2CID 210942877 . ^ Xu, Guobin; Mao, Jianzhou; Sakk, Eric; Wang, Shuangbao Paul (22 March 2023). \"An Overview of Quantum-Safe Approaches: Quantum Key Distribution and Post-Quantum Cryptography\". 2023 57th Annual Conference on Information Sciences and Systems (CISS) . IEEE . p. 3. doi : 10.1109/CISS56502.2023.10089619 . ISBN 978-1-6654-5181-9 . ^ Kozlowski, Wojciech; Wehner, Stephanie (25 September 2019). \"Towards Large-Scale Quantum Networks\". Proceedings of the Sixth Annual ACM International Conference on Nanoscale Computing and Communication . ACM. pp. 1– 7. arXiv : 1909.08396 . doi : 10.1145/3345312.3345497 . ISBN 978-1-4503-6897-1 . ^ Guo, Xueshi; Breum, Casper R.; Borregaard, Johannes; Izumi, Shuro; Larsen, Mikkel V.; Gehring, Tobias; Christandl, Matthias; Neergaard-Nielsen, Jonas S.; Andersen, Ulrik L. (23 December 2019). \"Distributed quantum sensing in a continuous-variable entangled network\". Nature Physics . 16 (3): 281– 284. arXiv : 1905.09408 . doi : 10.1038/s41567-019-0743-x . ISSN 1745-2473 . S2CID 256703226 . ^ a b c Jordan, Stephen (14 October 2022) [22 April 2011]. \"Quantum Algorithm Zoo\" . Archived from the original on 29 April 2018. ^ Aaronson, Scott ; Arkhipov, Alex (6 June 2011). \"The computational complexity of linear optics\". Proceedings of the forty-third annual ACM symposium on Theory of computing . San Jose, California : Association for Computing Machinery . pp. 333– 342. arXiv : 1011.3245 . doi : 10.1145/1993636.1993682 . ISBN 978-1-4503-0691-1 . ^ a b Nielsen \u0026 Chuang 2010 , p. 42. ^ Norton, Quinn (15 February 2007). \"The Father of Quantum Computing\" . Wired . ^ Ambainis, Andris (Spring 2014). \"What Can We Do with a Quantum Computer?\" . Institute for Advanced Study. ^ Chang, Kenneth (14 June 2023). \"Quantum Computing Advance Begins New Era, IBM Says – A quantum computer came up with better answers to a physics problem than a conventional supercomputer\" . The New York Times . Archived from the original on 14 June 2023 . Retrieved 15 June 2023 . ^ Kim, Youngseok; et al. (14 June 2023). \"Evidence for the utility of quantum computing before fault tolerance\" . Nature . 618 (7965): 500– 505. Bibcode : 2023Natur.618..500K . doi : 10.1038/s41586-023-06096-3 . PMC 10266970 . PMID 37316724 . ^ Morello, Andrea (21 November 2018). Lunch \u0026 Learn: Quantum Computing . Sibos TV . Archived from the original on 15 February 2021 . Retrieved 4 February 2021 – via YouTube. {{ cite AV media }} : CS1 maint: bot: original URL status unknown ( link ) ^ Ruane, Jonathan; McAfee, Andrew; Oliver, William D. (1 January 2022). \"Quantum Computing for Business Leaders\" . Harvard Business Review . ISSN 0017-8012 . Retrieved 12 April 2023 . ^ Budde, Florian; Volz, Daniel (12 July 2019). \"Quantum computing and the chemical industry | McKinsey\" . www.mckinsey.com . McKinsey and Company . Retrieved 12 April 2023 . ^ Bourzac, Katherine (30 October 2017). \"Chemistry is quantum computing's killer app\" . cen.acs.org . American Chemical Society . Retrieved 12 April 2023 . ^ Lenstra, Arjen K. (2000). \"Integer Factoring\" (PDF) . Designs, Codes and Cryptography . 19 (2/3): 101– 128. doi : 10.1023/A:1008397921377 . S2CID 9816153 . Archived from the original (PDF) on 10 April 2015. ^ Nielsen \u0026 Chuang 2010 , p. 216. ^ a b Bernstein, Daniel J. (2009). \"Introduction to post-quantum cryptography\". Post-Quantum Cryptography . Berlin, Heidelberg: Springer. pp. 1– 14. doi : 10.1007/978-3-540-88702-7_1 . ISBN 978-3-540-88701-0 . S2CID 61401925 . ^ See also pqcrypto.org , a bibliography maintained by Daniel J. Bernstein and Tanja Lange on cryptography not known to be broken by quantum computing. ^ McEliece, R. J. (January 1978). \"A Public-Key Cryptosystem Based On Algebraic Coding Theory\" (PDF) . DSNPR . 44 : 114– 116. Bibcode : 1978DSNPR..44..114M . ^ Kobayashi, H.; Gall, F. L. (2006). \"Dihedral Hidden Subgroup Problem: A Survey\" . Information and Media Technologies . 1 (1): 178– 185. doi : 10.2197/ipsjdc.1.470 . ^ Bennett, Charles H.; Bernstein, Ethan; Brassard, Gilles; Vazirani, Umesh (October 1997). \"Strengths and Weaknesses of Quantum Computing\". SIAM Journal on Computing . 26 (5): 1510– 1523. arXiv : quant-ph/9701001 . Bibcode : 1997quant.ph..1001B . doi : 10.1137/s0097539796300933 . S2CID 13403194 . ^ Brassard, Gilles; Høyer, Peter; Tapp, Alain (2016). \"Quantum Algorithm for the Collision Problem\". In Kao, Ming-Yang (ed.). Encyclopedia of Algorithms . New York, New York: Springer. pp. 1662– 1664. arXiv : quant-ph/9705002 . doi : 10.1007/978-1-4939-2864-4_304 . ISBN 978-1-4939-2864-4 . S2CID 3116149 . ^ Farhi, Edward; Goldstone, Jeffrey; Gutmann, Sam (23 December 2008). \"A Quantum Algorithm for the Hamiltonian NAND Tree\" . Theory of Computing . 4 (1): 169– 190. doi : 10.4086/toc.2008.v004a008 . ISSN 1557-2862 . S2CID 8258191 . ^ Williams, Colin P. (2011). Explorations in Quantum Computing . Springer . pp. 242– 244. ISBN 978-1-84628-887-6 . ^ Grover, Lov (29 May 1996). \"A fast quantum mechanical algorithm for database search\". arXiv : quant-ph/9605043 . ^ Ambainis, Ambainis (June 2004). \"Quantum search algorithms\". ACM SIGACT News . 35 (2): 22– 35. arXiv : quant-ph/0504012 . Bibcode : 2005quant.ph..4012A . doi : 10.1145/992287.992296 . S2CID 11326499 . ^ Rich, Steven; Gellman, Barton (1 February 2014). \"NSA seeks to build quantum computer that could crack most types of encryption\" . The Washington Post . ^ Lucas, Andrew (2014). \"Ising formulations of many NP problems\" . Frontiers in Physics . 2 : 5. arXiv : 1302.5843 . Bibcode : 2014FrP.....2....5L . doi : 10.3389/fphy.2014.00005 . ^ Outeiral, Carlos; Strahm, Martin; Morris, Garrett; Benjamin, Simon; Deane, Charlotte; Shi, Jiye (2021). \"The prospects of quantum computing in computational molecular biology\" . WIREs Computational Molecular Science . 11 . arXiv : 2005.12792 . doi : 10.1002/wcms.1481 . S2CID 218889377 . ^ Biamonte, Jacob; Wittek, Peter; Pancotti, Nicola; Rebentrost, Patrick; Wiebe, Nathan; Lloyd, Seth (September 2017). \"Quantum machine learning\". Nature . 549 (7671): 195– 202. arXiv : 1611.09347 . Bibcode : 2017Natur.549..195B . doi : 10.1038/nature23474 . ISSN 0028-0836 . PMID 28905917 . S2CID 64536201 . ^ Harrow, Aram; Hassidim, Avinatan; Lloyd, Seth (2009). \"Quantum algorithm for solving linear systems of equations\". Physical Review Letters . 103 (15): 150502. arXiv : 0811.3171 . Bibcode : 2009PhRvL.103o0502H . doi : 10.1103/PhysRevLett.103.150502 . PMID 19905613 . S2CID 5187993 . ^ Benedetti, Marcello; Realpe-Gómez, John; Biswas, Rupak; Perdomo-Ortiz, Alejandro (9 August 2016). \"Estimation of effective temperatures in quantum annealers for sampling applications: A case study with possible applications in deep learning\" . Physical Review A . 94 (2): 022308. arXiv : 1510.07611 . Bibcode : 2016PhRvA..94b2308B . doi : 10.1103/PhysRevA.94.022308 . ^ Ajagekar, Akshay; You, Fengqi (5 December 2020). \"Quantum computing assisted deep learning for fault detection and diagnosis in industrial process systems\". Computers \u0026 Chemical Engineering . 143 107119. arXiv : 2003.00264 . doi : 10.1016/j.compchemeng.2020.107119 . ISSN 0098-1354 . S2CID 211678230 . ^ Ajagekar, Akshay; You, Fengqi (1 December 2021). \"Quantum computing based hybrid deep learning for fault diagnosis in electrical power systems\" . Applied Energy . 303 117628. Bibcode : 2021ApEn..30317628A . doi : 10.1016/j.apenergy.2021.117628 . ISSN 0306-2619 . ^ Gao, Xun; Anschuetz, Eric R.; Wang, Sheng-Tao; Cirac, J. Ignacio; Lukin, Mikhail D. (2022). \"Enhancing Generative Models via Quantum Correlations\". Physical Review X . 12 (2): 021037. arXiv : 2101.08354 . Bibcode : 2022PhRvX..12b1037G . doi : 10.1103/PhysRevX.12.021037 . S2CID 231662294 . ^ Li, Junde; Topaloglu, Rasit; Ghosh, Swaroop (9 January 2021). \"Quantum Generative Models for Small Molecule Drug Discovery\". arXiv : 2101.03438 [ cs.ET ]. ^ a b c Brooks, Michael (24 May 2023). \"Quantum computers: what are they good for?\" . Nature . 617 (7962): S1 – S3 . Bibcode : 2023Natur.617S...1B . doi : 10.1038/d41586-023-01692-9 . PMID 37225885 . S2CID 258847001 . ^ a b c d Torsten Hoefler; Thomas Häner; Matthias Troyer (May 2023). \"Disentangling Hype from Practicality: On Realistically Achieving Quantum Advantage\" . Communications of the ACM. ^ Dyakonov, Mikhail (15 November 2018). \"The Case Against Quantum Computing\" . IEEE Spectrum . ^ DiVincenzo, David P. (13 April 2000). \"The Physical Implementation of Quantum Computation\". Fortschritte der Physik . 48 ( 9– 11): 771– 783. arXiv : quant-ph/0002077 . Bibcode : 2000ForPh..48..771D . doi : 10.1002/1521-3978(200009)48:9/11\u003c771::AID-PROP771\u003e3.0.CO;2-E . S2CID 15439711 . ^ Giles, Martin (17 January 2019). \"We'd have more quantum computers if it weren't so hard to find the damn cables\" . MIT Technology Review . Retrieved 17 May 2021 . ^ Pauka SJ, Das K, Kalra B, Moini A, Yang Y, Trainer M, Bousquet A, Cantaloube C, Dick N, Gardner GC, Manfra MJ, Reilly DJ (2021). \"A cryogenic CMOS chip for generating control signals for multiple qubits\" . Nature Electronics . 4 (4): 64– 70. arXiv : 1912.01299 . doi : 10.1038/s41928-020-00528-y . S2CID 231715555 . ^ DiVincenzo, David P. (1995). \"Quantum Computation\". Science . 270 (5234): 255– 261. Bibcode : 1995Sci...270..255D . CiteSeerX 10.1.1.242.2165 . doi : 10.1126/science.270.5234.255 . S2CID 220110562 . ^ Zu, H.; Dai, W.; de Waele, A.T.A.M. (2022). \"Development of Dilution refrigerators – A review\". Cryogenics . 121 . doi : 10.1016/j.cryogenics.2021.103390 . ISSN 0011-2275 . S2CID 244005391 . ^ Jones, Nicola (19 June 2013). \"Computing: The quantum company\" . Nature . 498 (7454): 286– 288. Bibcode : 2013Natur.498..286J . doi : 10.1038/498286a . PMID 23783610 . ^ Vepsäläinen, Antti P.; Karamlou, Amir H.; Orrell, John L.; Dogra, Akshunna S.; Loer, Ben; et al. (August 2020). \"Impact of ionizing radiation on superconducting qubit coherence\" . Nature . 584 (7822): 551– 556. arXiv : 2001.09190 . Bibcode : 2020Natur.584..551V . doi : 10.1038/s41586-020-2619-8 . ISSN 1476-4687 . PMID 32848227 . S2CID 210920566 . ^ Amy, Matthew; Matteo, Olivia; Gheorghiu, Vlad; Mosca, Michele; Parent, Alex; Schanck, John (30 November 2016). \"Estimating the cost of generic quantum pre-image attacks on SHA-2 and SHA-3\". arXiv : 1603.09383 [ quant-ph ]. ^ Dyakonov, M. I. (14 October 2006). S. Luryi; Xu, J.; Zaslavsky, A. (eds.). \"Is Fault-Tolerant Quantum Computation Really Possible?\". Future Trends in Microelectronics. Up the Nano Creek : 4– 18. arXiv : quant-ph/0610117 . Bibcode : 2006quant.ph.10117D . ^ Ahsan, Muhammad (2015). Architecture Framework for Trapped-ion Quantum Computer based on Performance Simulation Tool . Bibcode : 2015PhDT........56A . OCLC 923881411 . ^ Ahsan, Muhammad; Meter, Rodney Van; Kim, Jungsang (28 December 2016). \"Designing a Million-Qubit Quantum Computer Using a Resource Performance Simulator\" . ACM Journal on Emerging Technologies in Computing Systems . 12 (4): 39:1–39:25. arXiv : 1512.00796 . doi : 10.1145/2830570 . ISSN 1550-4832 . S2CID 1258374 . ^ Gidney, Craig; Ekerå, Martin (15 April 2021). \"How to factor 2048 bit RSA integers in 8 hours using 20 million noisy qubits\". Quantum . 5 433. arXiv : 1905.09749 . Bibcode : 2021Quant...5..433G . doi : 10.22331/q-2021-04-15-433 . ISSN 2521-327X . S2CID 162183806 . ^ Freedman, Michael H. ; Kitaev, Alexei ; Larsen, Michael J. ; Wang, Zhenghan (2003). \"Topological quantum computation\". Bulletin of the American Mathematical Society . 40 (1): 31– 38. arXiv : quant-ph/0101025 . doi : 10.1090/S0273-0979-02-00964-3 . MR 1943131 . ^ Monroe, Don (1 October 2008). \"Anyons: The breakthrough quantum computing needs?\" . New Scientist . ^ a b Cossins, Daniel (28 June 2025). \"How to think about...Quasiparticles\". New Scientist . 266 (3549): 34. ^ Preskill, John (26 March 2012). \"Quantum computing and the entanglement frontier\". arXiv : 1203.5813 [ quant-ph ]. ^ Preskill, John (6 August 2018). \"Quantum Computing in the NISQ era and beyond\" . Quantum . 2 79. arXiv : 1801.00862 . Bibcode : 2018Quant...2...79P . doi : 10.22331/q-2018-08-06-79 . ^ Boixo, Sergio; Isakov, Sergei V.; Smelyanskiy, Vadim N.; Babbush, Ryan; Ding, Nan; et al. (2018). \"Characterizing Quantum Supremacy in Near-Term Devices\". Nature Physics . 14 (6): 595– 600. arXiv : 1608.00263 . Bibcode : 2018NatPh..14..595B . doi : 10.1038/s41567-018-0124-x . S2CID 4167494 . ^ Savage, Neil (5 July 2017). \"Quantum Computers Compete for \"Supremacy\" \" . Scientific American . ^ Giles, Martin (20 September 2019). \"Google researchers have reportedly achieved 'quantum supremacy' \" . MIT Technology Review . Retrieved 15 May 2020 . ^ Tavares, Frank (23 October 2019). \"Google and NASA Achieve Quantum Supremacy\" . NASA . Retrieved 16 November 2021 . ^ Pednault, Edwin; Gunnels, John A.; Nannicini, Giacomo; Horesh, Lior; Wisnieff, Robert (22 October 2019). \"Leveraging Secondary Storage to Simulate Deep 54-qubit Sycamore Circuits\". arXiv : 1910.09534 [ quant-ph ]. ^ Cho, Adrian (23 October 2019). \"IBM casts doubt on Google's claims of quantum supremacy\" . Science . doi : 10.1126/science.aaz6080 . ISSN 0036-8075 . S2CID 211982610 . ^ Liu, Yong (Alexander); Liu, Xin (Lucy); Li, Fang (Nancy); Fu, Haohuan; Yang, Yuling; et al. (14 November 2021). \"Closing the \"quantum supremacy\" gap\". Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis . SC '21. New York, New York: Association for Computing Machinery. pp. 1– 12. arXiv : 2110.14502 . doi : 10.1145/3458817.3487399 . ISBN 978-1-4503-8442-1 . S2CID 239036985 . ^ Bulmer, Jacob F. F.; Bell, Bryn A.; Chadwick, Rachel S.; Jones, Alex E.; Moise, Diana; et al. (28 January 2022). \"The boundary for quantum advantage in Gaussian boson sampling\" . Science Advances . 8 (4): eabl9236. arXiv : 2108.01622 . Bibcode : 2022SciA....8.9236B . doi : 10.1126/sciadv.abl9236 . ISSN 2375-2548 . PMC 8791606 . PMID 35080972 . ^ McCormick, Katie (10 February 2022). \"Race Not Over Between Classical and Quantum Computers\" . Physics . 15 19. Bibcode : 2022PhyOJ..15...19M . doi : 10.1103/Physics.15.19 . S2CID 246910085 . ^ Pan, Feng; Chen, Keyang; Zhang, Pan (2022). \"Solving the Sampling Problem of the Sycamore Quantum Circuits\". Physical Review Letters . 129 (9): 090502. arXiv : 2111.03011 . Bibcode : 2022PhRvL.129i0502P . doi : 10.1103/PhysRevLett.129.090502 . PMID 36083655 . S2CID 251755796 . ^ Cho, Adrian (2 August 2022). \"Ordinary computers can beat Google's quantum computer after all\" . Science . 377 . doi : 10.1126/science.ade2364 . ^ \"Google's 'quantum supremacy' usurped by researchers using ordinary supercomputer\" . TechCrunch . 5 August 2022 . Retrieved 7 August 2022 . ^ Ball, Philip (3 December 2020). \"Physicists in China challenge Google's 'quantum advantage' \". Nature . 588 (7838): 380. Bibcode : 2020Natur.588..380B . doi : 10.1038/d41586-020-03434-7 . PMID 33273711 . S2CID 227282052 . ^ Garisto, Daniel. \"Light-based Quantum Computer Exceeds Fastest Classical Supercomputers\" . Scientific American . Retrieved 7 December 2020 . ^ Conover, Emily (3 December 2020). \"The new light-based quantum computer Jiuzhang has achieved quantum supremacy\" . Science News . Retrieved 7 December 2020 . ^ Zhong, Han-Sen; Wang, Hui; Deng, Yu-Hao; Chen, Ming-Cheng; Peng, Li-Chao; et al. (3 December 2020). \"Quantum computational advantage using photons\". Science . 370 (6523): 1460– 1463. arXiv : 2012.01625 . Bibcode : 2020Sci...370.1460Z . doi : 10.1126/science.abe8770 . ISSN 0036-8075 . PMID 33273064 . S2CID 227254333 . ^ Roberson, Tara M. (21 May 2020). \"Can Hype Be a Force for Good?\" . Public Understanding of Science . 29 (5): 544– 552. doi : 10.1177/0963662520923109 . ISSN 0963-6625 . PMID 32438851 . S2CID 218831653 . ^ Cavaliere, Fabio; Mattsson, John; Smeets, Ben (September 2020). \"The security implications of quantum cryptography and quantum computing\" . Network Security . 2020 (9): 9– 15. doi : 10.1016/S1353-4858(20)30105-7 . ISSN 1353-4858 . S2CID 222349414 . ^ Liu, Yong; Chen, Yaojian; Guo, Chu; Song, Jiawei; Shi, Xinmin; Gan, Lin; Wu, Wenzhao; Wu, Wei; Fu, Haohuan; Liu, Xin; Chen, Dexun; Zhao, Zhifeng; Yang, Guangwen; Gao, Jiangang (16 January 2024). \"Verifying Quantum Advantage Experiments with Multiple Amplitude Tensor Network Contraction\" . Physical Review Letters . 132 (3): 030601. arXiv : 2212.04749 . Bibcode : 2024PhRvL.132c0601L . doi : 10.1103/PhysRevLett.132.030601 . ISSN 0031-9007 . PMID 38307065 . ^ Monroe, Don (December 2022). \"Quantum Computers and the Universe\" . Communications of the ACM. ^ Swayne, Matt (20 June 2023). \"PsiQuantum Sees 700x Reduction in Computational Resource Requirements to Break Elliptic Curve Cryptography With a Fault Tolerant Quantum Computer\" . The Quanrum Insider . ^ Unruh, Bill (1995). \"Maintaining coherence in Quantum Computers\". Physical Review A . 51 (2): 992– 997. arXiv : hep-th/9406058 . Bibcode : 1995PhRvA..51..992U . doi : 10.1103/PhysRevA.51.992 . PMID 9911677 . S2CID 13980886 . ^ Davies, Paul (6 March 2007). \"The implications of a holographic universe for quantum information science and the nature of physical law\". arXiv : quant-ph/0703041 . ^ Regan, K. W. (23 April 2016). \"Quantum Supremacy and Complexity\" . Gödel's Lost Letter and P=NP . ^ Kalai, Gil (May 2016). \"The Quantum Computer Puzzle\" (PDF) . Notices of the AMS . 63 (5): 508– 516. ^ Rinott, Yosef; Shoham, Tomer; Kalai, Gil (13 July 2021). \"Statistical Aspects of the Quantum Supremacy Demonstration\". arXiv : 2008.05177 [ quant-ph ]. ^ Dyakonov, Mikhail (15 November 2018). \"The Case Against Quantum Computing\" . IEEE Spectrum . Retrieved 3 December 2019 . ^ Russell, John (10 January 2019). \"IBM Quantum Update: Q System One Launch, New Collaborators, and QC Center Plans\" . HPCwire . Retrieved 9 January 2023 . ^ Tacchino, Francesco; Chiesa, Alessandro; Carretta, Stefano; Gerace, Dario (19 December 2019). \"Quantum Computers as Universal Quantum Simulators: State-of-the-Art and Perspectives\" . Advanced Quantum Technologies . 3 (3): 1900052. arXiv : 1907.03505 . doi : 10.1002/qute.201900052 . ISSN 2511-9044 . S2CID 195833616 . ^ Grumbling \u0026 Horowitz 2019 , p. 127. ^ Grumbling \u0026 Horowitz 2019 , p. 114. ^ Nayak, Chetan; Simon, Steven H.; Stern, Ady; Freedman, Michael; Das Sarma, Sankar (12 September 2008). \"Non-Abelian anyons and topological quantum computation\" . Reviews of Modern Physics . 80 (3): 1083– 1159. arXiv : 0707.1889 . Bibcode : 2008RvMP...80.1083N . doi : 10.1103/RevModPhys.80.1083 . ^ Grumbling \u0026 Horowitz 2019 , p. 119. ^ Grumbling \u0026 Horowitz 2019 , p. 126. ^ Leong, Kelvin; Sung, Anna (November 2022). \"What Business Managers Should Know About Quantum Computing?\" (PDF) . Journal of Interdisciplinary Sciences . Retrieved 13 August 2023 . ^ \"10 Quantum Computing Applications \u0026 Examples to Know\" . Built In . Retrieved 21 June 2025 . ^ Nielsen \u0026 Chuang 2010 , p. 29. ^ Nielsen \u0026 Chuang 2010 , p. 126. ^ Nielsen \u0026 Chuang 2010 , p. 41. ^ Nielsen \u0026 Chuang 2010 , p. 201. ^ Bernstein, Ethan; Vazirani, Umesh (1997). \"Quantum Complexity Theory\" . SIAM Journal on Computing . 26 (5): 1411– 1473. CiteSeerX 10.1.1.144.7852 . doi : 10.1137/S0097539796300921 . ^ Techi Editorial Team (6 August 2025). \"Quantum Computing in 2025: Industry Applications and Breakthroughs\" . Techi . Retrieved 6 August 2025 .\nhttps://en.wikipedia.org/w/index.php?title=Quantum_computing\u0026oldid=1304469779\nQuantum computing Models of computation Quantum cryptography Information theory Computational complexity theory Classes of computers Theoretical computer science Open problems Computer-related introductions in 1980 Supercomputers\nArticles with short description Short description is different from Wikidata Use American English from February 2023 All Wikipedia articles written in American English Use dmy dates from February 2021 Articles containing potentially dated statements from 2023 All articles containing potentially dated statements Commons category link is on Wikidata Pages displaying short descriptions of redirect targets via Module:Annotated link Commons link is locally defined CS1 Russian-language sources (ru) CS1 errors: generic name CS1 maint: bot: original URL status unknown Search Search Quantum computing 39 languages Add topic",
          "role": "teacher"
        },
        {
          "content": "Why is this important to understand?",
          "role": "student"
        },
        {
          "content": "Understanding Quantum Computing is crucial for building efficient, scalable, and maintainable software systems. It forms the foundation for solving complex computational problems.",
          "role": "teacher"
        },
        {
          "content": "Can you give me a concrete example?",
          "role": "student"
        },
        {
          "content": "For example, Quantum Computing can be applied in scenarios where you need to optimize performance, manage complexity, or implement scalable solutions.",
          "role": "teacher"
        }
      ],
      "format": "socratic_dialogue",
      "metadata": {
        "domain": "Technology",
        "source": "https://en.wikipedia.org/wiki/Quantum_computing",
        "words": 15138
      },
      "topic": "Quantum Computing"
    },
    {
      "content": {
        "applications": "applications. Physically engineering high-quality qubits has proven to be challenging. If a physical qubit is not sufficiently\nfrom its environment, it suffers from\ninto calculations. National governments have invested heavily in experimental research aimed at developing scalable qubits with longer coherence times and lower error rates. Example implementations include\nelectrical resistance\n(which confine a single\nelectromagnetic fields\n). Researchers have demonstrated that certain quantum device",
        "considerations": "considered the world's fastest computer.\n[ 28 ] [ 113 ] [ 114 ]\nThis claim has been subsequently challenged: IBM has stated that Summit can perform samples much faster than claimed,\nand researchers have since developed better algorithms for the sampling problem used to claim quantum supremacy, giving substantial reductions to the gap between Sycamore and classical supercomputers\n[ 117 ] [ 118 ] [ ",
        "key_concepts": [
          "dimensional, and this makes it challenging for a classical computer to simulate a quantum one: representing a 100-qubit system requires storing 2",
          "complete. Because these problems are BQP-complete, an equally fast classical algorithm for them would imply that"
        ],
        "overview": "A comprehensive technical concept with wide applications."
      },
      "format": "conceptual_breakdown",
      "metadata": {
        "domain": "Technology",
        "structure": "hierarchical"
      },
      "topic": "Quantum Computing"
    },
    {
      "format": "problem_solving",
      "scenario": {
        "approach": "solving\nBernstein–Vazirani algorithm\nThese algorithms did not solve practical problems, but demonstrated mathematically that one could gain more information by querying a\nwith a quantum state in\n, sometimes referred to as\n[ 18 ] Peter Shor (pictured here in 2017) showed in 1994 that a scalable quantum computer would be able to break RSA encryption . Peter Shor\nbuilt on these results with\nfor breaking the widely used\nencryption protocols,\nwhich drew significant attention to the field of quantum",
        "benefits": "advantage (previously\n), though these tasks are not necessarily useful for real-world applications.\nFor a chronological guide, see\nTimeline of quantum computing and communication\nFor many years, the fields of\nformed distinct academic communities.\n[ 2 ] Modern quantum theory\ndeveloped in the 1920s to",
        "problem": "How can Quantum Computing solve real-world problems?",
        "tradeoffs": "cost, so most\ndepict a network consisting only of quantum logic gates and no measurements.\n[ edit ] Quantum parallelism\nis the heuristic that quantum computers can be thought of as evaluating a function for multiple input values simultaneously. This can be achieved by preparing a quantum system in a"
      },
      "topic": "Quantum Computing"
    },
    {
      "comparison": "comparably modest quadratic speedup, they are widely applicable and thus give speedups for a wide range of problems.\nSimulation of quantum systems\nSince chemistry and nanotechnology rely on understanding quantum systems, and such systems are impossible to simulate in an efficient manner classically,\nmay be an important application of quantum computing.\nQuantum simulation could also be used to simulate the behavior of atoms and particles at unusual conditions such as the reactions inside a\nIn Jun",
      "format": "comparative_analysis",
      "topic": "Quantum Computing"
    }
  ],
  "created_at": "2025-08-10T21:15:34Z",
  "metadata": {
    "formats": [
      "socratic_dialogue",
      "expert_explanation",
      "practical_guide",
      "conceptual_breakdown",
      "implementation_tutorial",
      "comparative_analysis",
      "problem_solving",
      "real_world_application"
    ],
    "name": "Premium Technical Knowledge",
    "quality": "exceptional",
    "total_conversations": 43,
    "total_words": 82079
  }
}
